#!/usr/bin/env python3
"""
è¨˜æ†¶åº«åƒæ•¸æ™ºèƒ½åˆ†æå™¨
æ ¹æ“šGPSæ•¸æ“šè‡ªå‹•æ¨è–¦æœ€ä½³çš„ spatial_radius å’Œ memory_size åƒæ•¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import argparse
from pathlib import Path
import json


class MemoryBankParameterAnalyzer:
    """è¨˜æ†¶åº«åƒæ•¸åˆ†æå™¨"""
    
    def __init__(self, train_gps_csv: str, val_gps_csv: str = None):
        """
        Args:
            train_gps_csv: è¨“ç·´é›†GPS CSVæ–‡ä»¶è·¯å¾‘
            val_gps_csv: é©—è­‰é›†GPS CSVæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        """
        self.train_gps_csv = train_gps_csv
        self.val_gps_csv = val_gps_csv
        
        # è¼‰å…¥GPSæ•¸æ“š
        print("ğŸ“‚ è¼‰å…¥GPSæ•¸æ“š...")
        self.train_gps = pd.read_csv(train_gps_csv)
        print(f"âœ… è¨“ç·´é›†: {len(self.train_gps)} å€‹GPSé»")
        
        if val_gps_csv:
            self.val_gps = pd.read_csv(val_gps_csv)
            print(f"âœ… é©—è­‰é›†: {len(self.val_gps)} å€‹GPSé»")
            self.all_gps = pd.concat([self.train_gps, self.val_gps], ignore_index=True)
        else:
            self.all_gps = self.train_gps
        
        print(f"ğŸ“Š ç¸½å…±: {len(self.all_gps)} å€‹GPSé»")
        
        # åŸºæœ¬çµ±è¨ˆ
        self.gps_stats = self._compute_basic_stats()
        self._print_basic_stats()
    
    def _compute_basic_stats(self) -> Dict[str, float]:
        """è¨ˆç®—åŸºæœ¬GPSçµ±è¨ˆä¿¡æ¯"""
        lats = self.all_gps['lat'].values
        lons = self.all_gps['long'].values
        
        return {
            'lat_min': lats.min(),
            'lat_max': lats.max(),
            'lat_range': lats.max() - lats.min(),
            'lat_std': lats.std(),
            'lon_min': lons.min(),
            'lon_max': lons.max(),
            'lon_range': lons.max() - lons.min(),
            'lon_std': lons.std(),
            'total_points': len(lats),
            'unique_coords': len(set(zip(lats, lons)))
        }
    
    def _print_basic_stats(self):
        """æ‰“å°åŸºæœ¬çµ±è¨ˆä¿¡æ¯"""
        stats = self.gps_stats
        print(f"\nğŸ“ˆ GPSæ•¸æ“šåŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç·¯åº¦ç¯„åœ: [{stats['lat_min']:.6f}, {stats['lat_max']:.6f}] (è·¨åº¦: {stats['lat_range']:.6f})")
        print(f"  ç¶“åº¦ç¯„åœ: [{stats['lon_min']:.6f}, {stats['lon_max']:.6f}] (è·¨åº¦: {stats['lon_range']:.6f})")
        print(f"  ç·¯åº¦æ¨™æº–å·®: {stats['lat_std']:.6f}")
        print(f"  ç¶“åº¦æ¨™æº–å·®: {stats['lon_std']:.6f}")
        print(f"  ç¸½GPSé»æ•¸: {stats['total_points']}")
        print(f"  å”¯ä¸€ä½ç½®æ•¸: {stats['unique_coords']}")
        print(f"  é‡è¤‡ç‡: {(1 - stats['unique_coords']/stats['total_points'])*100:.1f}%")
    
    def analyze_spatial_clustering(self, radius_candidates: List[float] = None) -> Dict[float, Dict]:
        """åˆ†æä¸åŒç©ºé–“åŠå¾‘ä¸‹çš„èšé¡æ•ˆæœ"""
        
        if radius_candidates is None:
            # è‡ªå‹•ç”Ÿæˆå€™é¸åŠå¾‘ï¼šå¾GPSç¯„åœçš„0.5%åˆ°20%
            base_range = max(self.gps_stats['lat_range'], self.gps_stats['lon_range'])
            radius_candidates = [
                base_range * factor for factor in 
                [0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.15, 0.2]
            ]
        
        print(f"\nğŸ” åˆ†æç©ºé–“èšé¡æ•ˆæœ...")
        print(f"æ¸¬è©¦åŠå¾‘å€™é¸: {[f'{r:.6f}' for r in radius_candidates]}")
        
        clustering_results = {}
        
        for radius in radius_candidates:
            result = self._analyze_single_radius(radius)
            clustering_results[radius] = result
            
            print(f"  åŠå¾‘ {radius:.6f}: "
                  f"{result['clusters']} å€‹èšé¡, "
                  f"å¹³å‡ {result['avg_points_per_cluster']:.1f} é»/èšé¡, "
                  f"å‘½ä¸­ç‡ {result['hit_rate']:.3f}")
        
        return clustering_results
    
    def _analyze_single_radius(self, radius: float) -> Dict:
        """åˆ†æå–®å€‹åŠå¾‘çš„èšé¡æ•ˆæœ"""
        
        # æ¨¡æ“¬GPSé‡åŒ–éç¨‹
        def gps_to_key(lat, lon):
            lat_grid = round(lat / radius) * radius
            lon_grid = round(lon / radius) * radius
            return f"{lat_grid:.8f},{lon_grid:.8f}"
        
        # æ§‹å»ºèšé¡
        clusters = defaultdict(list)
        for _, row in self.all_gps.iterrows():
            key = gps_to_key(row['lat'], row['long'])
            clusters[key].append((row['lat'], row['long']))
        
        # è¨ˆç®—çµ±è¨ˆ
        cluster_sizes = [len(points) for points in clusters.values()]
        
        # æ¨¡æ“¬è¨˜æ†¶åº«å‘½ä¸­ç‡
        hit_count = sum(1 for size in cluster_sizes if size > 1)
        hit_rate = hit_count / len(cluster_sizes) if cluster_sizes else 0
        
        return {
            'radius': radius,
            'clusters': len(clusters),
            'total_points': sum(cluster_sizes),
            'avg_points_per_cluster': np.mean(cluster_sizes) if cluster_sizes else 0,
            'max_points_per_cluster': max(cluster_sizes) if cluster_sizes else 0,
            'min_points_per_cluster': min(cluster_sizes) if cluster_sizes else 0,
            'cluster_sizes': cluster_sizes,
            'hit_rate': hit_rate,
            'clusters_with_multiple_points': hit_count,
            'compression_ratio': len(clusters) / self.gps_stats['total_points']
        }
    
    def compute_distance_statistics(self) -> Dict[str, Any]:
        """è¨ˆç®—GPSé»ä¹‹é–“çš„è·é›¢çµ±è¨ˆ"""
        print(f"\nğŸ“ è¨ˆç®—GPSé»è·é›¢çµ±è¨ˆ...")
        
        # éš¨æ©Ÿæ¡æ¨£ä»¥åŠ é€Ÿè¨ˆç®—ï¼ˆå¦‚æœæ•¸æ“šå¤ªå¤§ï¼‰
        sample_size = min(2000, len(self.all_gps))
        if len(self.all_gps) > sample_size:
            sampled_gps = self.all_gps.sample(n=sample_size, random_state=42)
            print(f"  æ¡æ¨£ {sample_size} å€‹é»é€²è¡Œè·é›¢åˆ†æ")
        else:
            sampled_gps = self.all_gps
        
        distances = []
        coords = [(row['lat'], row['long']) for _, row in sampled_gps.iterrows()]
        
        # è¨ˆç®—æ‰€æœ‰é»å°ä¹‹é–“çš„è·é›¢
        for i in range(len(coords)):
            for j in range(i+1, min(i+50, len(coords))):  # é™åˆ¶æ¯å€‹é»æœ€å¤šæ¯”è¼ƒ50å€‹é„°å±…
                lat1, lon1 = coords[i]
                lat2, lon2 = coords[j]
                dist = ((lat1-lat2)**2 + (lon1-lon2)**2)**0.5
                distances.append(dist)
        
        distances = np.array(distances)
        
        distance_stats = {
            'min_distance': distances.min(),
            'max_distance': distances.max(),
            'mean_distance': distances.mean(),
            'median_distance': np.median(distances),
            'std_distance': distances.std(),
            'percentiles': {
                '5%': np.percentile(distances, 5),
                '10%': np.percentile(distances, 10),
                '25%': np.percentile(distances, 25),
                '75%': np.percentile(distances, 75),
                '90%': np.percentile(distances, 90),
                '95%': np.percentile(distances, 95),
            }
        }
        
        print(f"  æœ€å°è·é›¢: {distance_stats['min_distance']:.6f}")
        print(f"  å¹³å‡è·é›¢: {distance_stats['mean_distance']:.6f}")
        print(f"  ä¸­ä½æ•¸è·é›¢: {distance_stats['median_distance']:.6f}")
        print(f"  10%åˆ†ä½æ•¸: {distance_stats['percentiles']['10%']:.6f}")
        print(f"  90%åˆ†ä½æ•¸: {distance_stats['percentiles']['90%']:.6f}")
        
        return distance_stats
    
    def recommend_optimal_parameters(self) -> Dict[str, Any]:
        """æ¨è–¦æœ€ä½³è¨˜æ†¶åº«åƒæ•¸"""
        print(f"\nğŸ¯ ç”Ÿæˆè¨˜æ†¶åº«åƒæ•¸æ¨è–¦...")
        
        # 1. åˆ†æç©ºé–“èšé¡
        clustering_results = self.analyze_spatial_clustering()
        
        # 2. è¨ˆç®—è·é›¢çµ±è¨ˆ
        distance_stats = self.compute_distance_statistics()
        
        # 3. åŸºæ–¼å¤šå€‹æŒ‡æ¨™è©•ä¼°æœ€ä½³åƒæ•¸
        recommendations = self._evaluate_parameter_combinations(clustering_results, distance_stats)
        
        return recommendations
    
    def _evaluate_parameter_combinations(self, clustering_results: Dict, distance_stats: Dict) -> Dict:
        """è©•ä¼°åƒæ•¸çµ„åˆä¸¦ç”Ÿæˆæ¨è–¦"""
        
        # è©•ä¼°æ¨™æº–
        def score_radius(result):
            """ç‚ºæ¯å€‹åŠå¾‘è¨ˆç®—åˆ†æ•¸"""
            # ç›®æ¨™ï¼š
            # 1. åˆç†çš„èšé¡æ•¸é‡ (ä¸è¦å¤ªå¤šä¹Ÿä¸è¦å¤ªå°‘)
            # 2. æ¯å€‹èšé¡æœ‰è¶³å¤ çš„é» (è¨˜æ†¶åº«æ‰æœ‰ç”¨)
            # 3. å‘½ä¸­ç‡è¦é«˜ (èƒ½ç¶“å¸¸åŒ¹é…åˆ°è¨˜æ†¶)
            
            clusters = result['clusters']
            avg_points = result['avg_points_per_cluster']
            hit_rate = result['hit_rate']
            compression = result['compression_ratio']
            
            # ç†æƒ³çš„èšé¡æ•¸é‡ï¼šç¸½é»æ•¸çš„10%-50%
            ideal_clusters_min = self.gps_stats['total_points'] * 0.1
            ideal_clusters_max = self.gps_stats['total_points'] * 0.5
            
            if clusters < ideal_clusters_min:
                cluster_score = clusters / ideal_clusters_min  # å¤ªå°‘èšé¡ï¼Œæ‡²ç½°
            elif clusters > ideal_clusters_max:
                cluster_score = ideal_clusters_max / clusters  # å¤ªå¤šèšé¡ï¼Œæ‡²ç½°
            else:
                cluster_score = 1.0  # ç†æƒ³ç¯„åœ
            
            # æ¯å€‹èšé¡çš„å¹³å‡é»æ•¸åˆ†æ•¸ (2-10å€‹é»æ¯”è¼ƒç†æƒ³)
            if avg_points < 2:
                avg_points_score = avg_points / 2
            elif avg_points > 10:
                avg_points_score = 10 / avg_points
            else:
                avg_points_score = 1.0
            
            # å‘½ä¸­ç‡åˆ†æ•¸ (è¶Šé«˜è¶Šå¥½)
            hit_rate_score = hit_rate
            
            # ç¶œåˆåˆ†æ•¸
            total_score = (cluster_score * 0.3 + 
                          avg_points_score * 0.3 + 
                          hit_rate_score * 0.4)
            
            return total_score
        
        # è©•ä¼°æ‰€æœ‰åŠå¾‘
        scored_results = []
        for radius, result in clustering_results.items():
            score = score_radius(result)
            scored_results.append((score, radius, result))
        
        # æ’åºå¾—åˆ°æœ€ä½³çµæœ
        scored_results.sort(reverse=True)
        
        # ç”Ÿæˆæ¨è–¦
        best_score, best_radius, best_result = scored_results[0]
        
        # æ ¹æ“šæœ€ä½³èšé¡çµæœæ¨è–¦è¨˜æ†¶åº«å¤§å°
        avg_cluster_size = best_result['avg_points_per_cluster']
        max_cluster_size = best_result['max_points_per_cluster']
        
        # è¨˜æ†¶åº«å¤§å°å»ºè­°ï¼šèƒ½å®¹ç´å¤§éƒ¨åˆ†èšé¡çš„é»
        recommended_memory_sizes = {
            'conservative': max(10, int(avg_cluster_size * 1.5)),
            'moderate': max(20, int(avg_cluster_size * 2.5)),
            'aggressive': max(30, int(max_cluster_size * 0.8))
        }
        
        recommendations = {
            'optimal_spatial_radius': best_radius,
            'spatial_radius_score': best_score,
            'recommended_memory_sizes': recommended_memory_sizes,
            'clustering_analysis': best_result,
            'all_radius_scores': [(score, radius) for score, radius, _ in scored_results],
            'distance_based_suggestions': {
                'min_useful_radius': distance_stats['percentiles']['10%'],
                'max_useful_radius': distance_stats['percentiles']['90%'],
                'sweet_spot_radius': distance_stats['percentiles']['25%']
            },
            'performance_prediction': self._predict_memory_performance(best_result, recommended_memory_sizes)
        }
        
        return recommendations
    
    def _predict_memory_performance(self, clustering_result: Dict, memory_sizes: Dict) -> Dict:
        """é æ¸¬è¨˜æ†¶åº«æ€§èƒ½"""
        
        cluster_sizes = clustering_result['cluster_sizes']
        
        performance = {}
        for size_name, memory_size in memory_sizes.items():
            # è¨ˆç®—èƒ½è¢«å®Œå…¨å­˜å„²çš„èšé¡æ¯”ä¾‹
            fully_stored = sum(1 for size in cluster_sizes if size <= memory_size)
            fully_stored_ratio = fully_stored / len(cluster_sizes)
            
            # è¨ˆç®—å¹³å‡å­˜å„²åˆ©ç”¨ç‡
            avg_utilization = np.mean([min(size, memory_size) / memory_size for size in cluster_sizes])
            
            performance[size_name] = {
                'memory_size': memory_size,
                'fully_stored_clusters_ratio': fully_stored_ratio,
                'average_utilization': avg_utilization,
                'expected_hit_rate': clustering_result['hit_rate'] * fully_stored_ratio
            }
        
        return performance
    
    def print_recommendations(self, recommendations: Dict):
        """æ‰“å°æ¨è–¦çµæœ"""
        print(f"\nğŸ¯ è¨˜æ†¶åº«åƒæ•¸æ¨è–¦çµæœ")
        print(f"=" * 60)
        
        print(f"\nğŸ“ æœ€ä½³ç©ºé–“åŠå¾‘:")
        print(f"  æ¨è–¦å€¼: {recommendations['optimal_spatial_radius']:.6f}")
        print(f"  è©•åˆ†: {recommendations['spatial_radius_score']:.3f}")
        
        clustering = recommendations['clustering_analysis']
        print(f"\nğŸ“Š èšé¡æ•ˆæœ:")
        print(f"  èšé¡æ•¸é‡: {clustering['clusters']}")
        print(f"  å¹³å‡æ¯èšé¡é»æ•¸: {clustering['avg_points_per_cluster']:.1f}")
        print(f"  æœ€å¤§èšé¡é»æ•¸: {clustering['max_points_per_cluster']}")
        print(f"  è¨˜æ†¶å‘½ä¸­ç‡: {clustering['hit_rate']:.3f}")
        print(f"  å£“ç¸®æ¯”: {clustering['compression_ratio']:.3f}")
        
        print(f"\nğŸ§  æ¨è–¦è¨˜æ†¶åº«å¤§å°:")
        memory_sizes = recommendations['recommended_memory_sizes']
        performance = recommendations['performance_prediction']
        
        for strategy, size in memory_sizes.items():
            perf = performance[strategy]
            print(f"  {strategy.capitalize():12s}: {size:3d} "
                  f"(é æœŸå‘½ä¸­ç‡: {perf['expected_hit_rate']:.3f}, "
                  f"åˆ©ç”¨ç‡: {perf['average_utilization']:.3f})")
        
        print(f"\nğŸ² è·é›¢åˆ†æå»ºè­°:")
        dist_suggestions = recommendations['distance_based_suggestions']
        print(f"  æœ€å°æœ‰ç”¨åŠå¾‘: {dist_suggestions['min_useful_radius']:.6f}")
        print(f"  æœ€å¤§æœ‰ç”¨åŠå¾‘: {dist_suggestions['max_useful_radius']:.6f}")
        print(f"  ç”œèœœé»åŠå¾‘: {dist_suggestions['sweet_spot_radius']:.6f}")
        
        print(f"\nâœ¨ æœ€çµ‚å»ºè­°:")
        best_memory = memory_sizes['moderate']
        best_radius = recommendations['optimal_spatial_radius']
        
        print(f"  --spatial-radius {best_radius:.6f}")
        print(f"  --memory-size {best_memory}")
        
        print(f"\nğŸ“‹ è¨“ç·´å‘½ä»¤ç¯„ä¾‹:")
        print(f"  python geotrain_v2_early_v1.py \\")
        print(f"    --spatial-radius {best_radius:.6f} \\")
        print(f"    --memory-size {best_memory} \\")
        print(f"    å…¶ä»–åƒæ•¸...")
    
    def save_analysis_report(self, recommendations: Dict, output_path: str):
        """ä¿å­˜åˆ†æå ±å‘Š"""
        report = {
            'gps_data_info': {
                'train_csv': self.train_gps_csv,
                'val_csv': self.val_gps_csv,
                'total_points': self.gps_stats['total_points'],
                'unique_coords': self.gps_stats['unique_coords']
            },
            'gps_statistics': self.gps_stats,
            'recommendations': recommendations,
            'suggested_commands': {
                'conservative': f"--spatial-radius {recommendations['optimal_spatial_radius']:.6f} --memory-size {recommendations['recommended_memory_sizes']['conservative']}",
                'moderate': f"--spatial-radius {recommendations['optimal_spatial_radius']:.6f} --memory-size {recommendations['recommended_memory_sizes']['moderate']}",
                'aggressive': f"--spatial-radius {recommendations['optimal_spatial_radius']:.6f} --memory-size {recommendations['recommended_memory_sizes']['aggressive']}"
            }
        }
        
        # ä¿å­˜ç‚ºJSON
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"ğŸ“„ åˆ†æå ±å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def visualize_clustering_effects(self, clustering_results: Dict, save_path: str = None):
        """å¯è¦–åŒ–èšé¡æ•ˆæœ"""
        try:
            import matplotlib.pyplot as plt
            
            # æå–æ•¸æ“šç”¨æ–¼ç¹ªåœ–
            radii = list(clustering_results.keys())
            clusters = [result['clusters'] for result in clustering_results.values()]
            avg_points = [result['avg_points_per_cluster'] for result in clustering_results.values()]
            hit_rates = [result['hit_rate'] for result in clustering_results.values()]
            
            # å‰µå»ºå­åœ–
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
            
            # åœ–1: èšé¡æ•¸é‡ vs åŠå¾‘
            ax1.plot(radii, clusters, 'b-o')
            ax1.set_xlabel('Spatial Radius')
            ax1.set_ylabel('Number of Clusters')
            ax1.set_title('Clusters vs Spatial Radius')
            ax1.grid(True)
            
            # åœ–2: å¹³å‡æ¯èšé¡é»æ•¸ vs åŠå¾‘
            ax2.plot(radii, avg_points, 'g-o')
            ax2.set_xlabel('Spatial Radius')
            ax2.set_ylabel('Avg Points per Cluster')
            ax2.set_title('Cluster Size vs Spatial Radius')
            ax2.grid(True)
            
            # åœ–3: å‘½ä¸­ç‡ vs åŠå¾‘
            ax3.plot(radii, hit_rates, 'r-o')
            ax3.set_xlabel('Spatial Radius')
            ax3.set_ylabel('Hit Rate')
            ax3.set_title('Memory Hit Rate vs Spatial Radius')
            ax3.grid(True)
            
            # åœ–4: èšé¡å¤§å°åˆ†å¸ƒ
            best_radius = max(clustering_results.keys(), 
                            key=lambda r: clustering_results[r]['hit_rate'])
            cluster_sizes = clustering_results[best_radius]['cluster_sizes']
            ax4.hist(cluster_sizes, bins=20, edgecolor='black', alpha=0.7)
            ax4.set_xlabel('Cluster Size')
            ax4.set_ylabel('Frequency')
            ax4.set_title(f'Cluster Size Distribution (radius={best_radius:.6f})')
            ax4.grid(True)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜åˆ°: {save_path}")
            
            plt.show()
            
        except ImportError:
            print("âš ï¸  matplotlibæœªå®‰è£ï¼Œè·³éå¯è¦–åŒ–")


def main():
    parser = argparse.ArgumentParser(description="è¨˜æ†¶åº«åƒæ•¸æ™ºèƒ½åˆ†æå™¨")
    parser.add_argument("train_gps_csv", help="è¨“ç·´é›†GPS CSVæ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--val-gps-csv", help="é©—è­‰é›†GPS CSVæ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--output-dir", default="./memory_analysis", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--visualize", action="store_true", help="ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨")
    
    args = parser.parse_args()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("ğŸ§  è¨˜æ†¶åº«åƒæ•¸æ™ºèƒ½åˆ†æå™¨")
    print("=" * 50)
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = MemoryBankParameterAnalyzer(
        train_gps_csv=args.train_gps_csv,
        val_gps_csv=args.val_gps_csv
    )
    
    # ç”Ÿæˆæ¨è–¦
    recommendations = analyzer.recommend_optimal_parameters()
    
    # æ‰“å°æ¨è–¦
    analyzer.print_recommendations(recommendations)
    
    # ä¿å­˜å ±å‘Š
    report_path = output_dir / "memory_bank_analysis_report.json"
    analyzer.save_analysis_report(recommendations, str(report_path))
    
    # å¯è¦–åŒ–
    if args.visualize:
        clustering_results = analyzer.analyze_spatial_clustering()
        viz_path = output_dir / "clustering_analysis.png"
        analyzer.visualize_clustering_effects(clustering_results, str(viz_path))
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()