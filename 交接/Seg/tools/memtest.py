import os
import pandas as pd
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict, Any, List

import torch
import numpy as np
from rich import print
from rich.progress import Progress, track
from rich.table import Table
from rich.console import Console

import engine.transform as transform
from engine.category import Category
from engine.dataloading import ImgAnnDataset
from engine.metric import Metrics
from engine.visualizer import IdMapVisualizer, ImgSaver
from engine.geo_v2 import create_memory_enhanced_geo_segformer
from geotrain_v2 import MemoryEnhancedGeoSegDataset, setup_gps_normalization

console = Console()


def parse_args() -> Namespace:
    parser = ArgumentParser(description="Memory-Enhanced GeoSegformer Testing")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("img_dir", type=str, help="Test images directory")
    parser.add_argument("ann_dir", type=str, help="Test annotations directory") 
    parser.add_argument("gps_csv", type=str, help="Test GPS CSV file")
    parser.add_argument("category_csv", type=str, help="Category definition CSV")
    parser.add_argument("checkpoint", type=str, help="Model checkpoint path")
    
    # GPSæ­£è§„åŒ–å‚æ•°ï¼ˆå¿…éœ€ï¼Œç”¨äºå’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    parser.add_argument("train_gps_csv", type=str, help="Training GPS CSV for normalization")
    parser.add_argument("val_gps_csv", type=str, help="Validation GPS CSV for normalization")
    
    # è¾“å‡ºè®¾ç½®
    parser.add_argument("--save-dir", type=str, default="./test_results", help="Results save directory")
    parser.add_argument("--save-predictions", action="store_true", help="Save prediction visualizations")
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size for testing")
    parser.add_argument("--num-workers", type=int, default=0, help="Number of workers")
    parser.add_argument("--max-len", type=int, default=None, help="Max number of test samples")
    
    # æµ‹è¯•æ¨¡å¼
    parser.add_argument("--test-mode", type=str, default="memory_aware", 
                       choices=["inference", "progressive", "fast", "benchmark", "memory_aware"],
                       help="Testing mode: inference/progressive/fast/benchmark/memory_aware")
    
    # å¿«é€Ÿæµ‹è¯•é€‰é¡¹
    parser.add_argument("--fast-eval", action="store_true", help="Use faster evaluation (less detailed)")
    parser.add_argument("--skip-memory-update", action="store_true", help="Skip memory updates for speed")
    parser.add_argument("--eval-interval", type=int, default=100, help="Evaluation interval for progressive mode")
    
    # æ¨¡å‹å‚æ•°ï¼ˆä»checkpointè‡ªåŠ¨æ¨æ–­ï¼Œè¿™äº›æ˜¯å¤‡ç”¨å€¼ï¼‰
    parser.add_argument("--model-size", type=str, default="b0")
    parser.add_argument("--feature-dim", type=int, default=512)
    parser.add_argument("--fusion-method", type=str, default="attention")
    parser.add_argument("--memory-size", type=int, default=20)
    parser.add_argument("--spatial-radius", type=float, default=0.00005)
    parser.add_argument("--gps-norm-method", type=str, default="minmax")
    
    return parser.parse_args()


def analyze_gps_overlap(test_gps_csv: str, train_gps_csv: str, val_gps_csv: str, spatial_radius: float = 0.00005):
    """åˆ†ææµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®çš„GPSé‡å æƒ…å†µ"""
    console.print(f"\n[bold cyan]ğŸ“ åˆ†æGPSæ•°æ®é‡å æƒ…å†µ...")
    
    # è¯»å–æ•°æ®
    test_gps = pd.read_csv(test_gps_csv)
    train_gps = pd.read_csv(train_gps_csv)
    val_gps = pd.read_csv(val_gps_csv)
    
    # åˆå¹¶è®­ç»ƒæ•°æ®
    train_val_gps = pd.concat([train_gps, val_gps], ignore_index=True)
    
    console.print(f"  æµ‹è¯•æ•°æ®: {len(test_gps)} ä¸ªGPSç‚¹")
    console.print(f"  è®­ç»ƒæ•°æ®: {len(train_val_gps)} ä¸ªGPSç‚¹")
    
    # åˆ†æé‡å æƒ…å†µ
    exact_matches = 0
    nearby_matches = 0
    no_matches = 0
    
    match_info = []
    
    for _, test_row in test_gps.iterrows():
        test_lat, test_lon = test_row['lat'], test_row['long']
        
        # è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒç‚¹çš„è·ç¦»
        distances = []
        for _, train_row in train_val_gps.iterrows():
            train_lat, train_lon = train_row['lat'], train_row['long']
            distance = ((test_lat - train_lat)**2 + (test_lon - train_lon)**2)**0.5
            distances.append(distance)
        
        min_distance = min(distances)
        
        if min_distance < spatial_radius:
            exact_matches += 1
            match_type = "exact"
        elif min_distance < spatial_radius * 3:
            nearby_matches += 1  
            match_type = "nearby"
        else:
            no_matches += 1
            match_type = "none"
        
        match_info.append({
            'filename': test_row['filename'],
            'lat': test_lat,
            'lon': test_lon,
            'min_distance': min_distance,
            'match_type': match_type
        })
    
    # è¾“å‡ºç»Ÿè®¡
    total = len(test_gps)
    console.print(f"\nğŸ“Š GPSåŒ¹é…ç»Ÿè®¡:")
    console.print(f"  ç²¾ç¡®åŒ¹é… (è·ç¦» < {spatial_radius:.5f}): {exact_matches} ({exact_matches/total*100:.1f}%)")
    console.print(f"  é™„è¿‘åŒ¹é… (è·ç¦» < {spatial_radius*3:.5f}): {nearby_matches} ({nearby_matches/total*100:.1f}%)")
    console.print(f"  æ— åŒ¹é… (è·ç¦» > {spatial_radius*3:.5f}): {no_matches} ({no_matches/total*100:.1f}%)")
    
    memory_coverage = (exact_matches + nearby_matches) / total * 100
    console.print(f"  è®°å¿†è¦†ç›–ç‡: {memory_coverage:.1f}%")
    
    if memory_coverage > 70:
        console.print(f"  âœ… è®°å¿†è¦†ç›–ç‡è‰¯å¥½ï¼Œé€‚åˆæµ‹è¯•è®°å¿†åŠŸèƒ½")
    elif memory_coverage > 30:
        console.print(f"  âš ï¸ è®°å¿†è¦†ç›–ç‡ä¸­ç­‰ï¼Œéƒ¨åˆ†ä½ç½®èƒ½åˆ©ç”¨è®°å¿†")
    else:
        console.print(f"  âŒ è®°å¿†è¦†ç›–ç‡è¾ƒä½ï¼Œå¤§éƒ¨åˆ†ä½ç½®æ— æ³•åˆ©ç”¨è®°å¿†")
    
    return match_info, {
        'exact_matches': exact_matches,
        'nearby_matches': nearby_matches, 
        'no_matches': no_matches,
        'memory_coverage': memory_coverage
    }
    """ä»checkpointåŠ è½½æ¨¡å‹"""
    console.print(f"[bold blue]ğŸ“¥ åŠ è½½æ¨¡å‹checkpoint: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ä»checkpointè·å–è®­ç»ƒæ—¶çš„å‚æ•°
    if 'args' in checkpoint:
        saved_args = checkpoint['args']
        model_size = getattr(saved_args, 'model_size', args.model_size)
        feature_dim = getattr(saved_args, 'feature_dim', args.feature_dim)
        fusion_method = getattr(saved_args, 'fusion_method', args.fusion_method)
        memory_size = getattr(saved_args, 'memory_size', args.memory_size)
        spatial_radius = getattr(saved_args, 'spatial_radius', args.spatial_radius)
        
        console.print(f"âœ… ä½¿ç”¨è®­ç»ƒæ—¶çš„æ¨¡å‹å‚æ•°:")
        console.print(f"  æ¨¡å‹å¤§å°: {model_size}")
        console.print(f"  ç‰¹å¾ç»´åº¦: {feature_dim}")
        console.print(f"  èåˆæ–¹æ³•: {fusion_method}")
        console.print(f"  è®°å¿†åº“å¤§å°: {memory_size}")
        console.print(f"  ç©ºé—´åŠå¾„: {spatial_radius}")
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºå¤‡ç”¨
        model_size = args.model_size
        feature_dim = args.feature_dim
        fusion_method = args.fusion_method
        memory_size = args.memory_size
        spatial_radius = args.spatial_radius
        console.print(f"âš ï¸  ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆcheckpointä¸­æ— ä¿å­˜çš„å‚æ•°ï¼‰")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_memory_enhanced_geo_segformer(
        num_classes=num_classes,
        model_size=model_size,
        feature_dim=feature_dim,
        fusion_method=fusion_method,
        memory_size=memory_size,
        spatial_radius=spatial_radius
    ).to(device)
    
    # åŠ è½½æƒé‡
    try:
        model.load_state_dict(checkpoint["model"])
        console.print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        console.print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        raise e
    
    # æ˜¾ç¤ºè®­ç»ƒæ—¶çš„è®°å¿†åº“ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'memory_stats' in checkpoint:
        stats = checkpoint['memory_stats']
        console.print(f"ğŸ“Š è®­ç»ƒæ—¶çš„è®°å¿†åº“çŠ¶æ€:")
        console.print(f"  æ€»ä½ç½®æ•°: {stats['total_locations']}")
        console.print(f"  æ€»è®°å¿†æ•°: {stats['total_memories']}")
        console.print(f"  å‘½ä¸­ç‡: {stats['hit_rate']:.4f}")
    
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    console.print(f"ğŸ”§ æ¨¡å‹å‚æ•°é‡: {total_params:.2f}M")
    
    return model


def inference_mode_test(model, dataloader, device: str, args: Namespace) -> Dict[str, Any]:
    """æ¨ç†æ¨¡å¼æµ‹è¯•ï¼šä½¿ç”¨è®­ç»ƒæ—¶çš„è®°å¿†ï¼Œä¸æ›´æ–°è®°å¿†åº“"""
    console.print(f"\n[bold green]ğŸ”® æ¨ç†æ¨¡å¼æµ‹è¯• (ä½¿ç”¨è®­ç»ƒæ—¶çš„è®°å¿†)")
    console.print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataloader)}")
    
    num_categories = len(Category.load(args.category_csv))
    if any(cat.id == 255 for cat in Category.load(args.category_csv)):
        num_categories -= 1
        
    metrics = Metrics(num_categories, nan_to_num=0)
    
    model.eval()
    total_memory_weight = 0
    total_samples = 0
    predictions = []
    
    # è·å–åˆå§‹è®°å¿†åº“çŠ¶æ€
    initial_memory_stats = model.get_memory_stats()
    console.print(f"ğŸ“Š åˆå§‹è®°å¿†åº“çŠ¶æ€:")
    console.print(f"  ä½ç½®æ•°: {initial_memory_stats['total_locations']}")
    console.print(f"  è®°å¿†æ•°: {initial_memory_stats['total_memories']}")
    console.print(f"  å‘½ä¸­ç‡: {initial_memory_stats['hit_rate']:.4f}")
    
    with torch.no_grad():
        for batch_idx, data in enumerate(track(dataloader, description="æ¨ç†æµ‹è¯•")):
            img = data["img"].to(device)
            ann = data["ann"].to(device)[:, 0, :, :]
            gps = data["gps"].to(device)
            filename = data.get("filename", [f"sample_{batch_idx}"])
            
            # æ¨ç†ï¼ˆä¸æ›´æ–°è®°å¿†åº“ï¼‰
            outputs = model(img, gps, return_embeddings=False, update_memory=False)
            pred = outputs['segmentation_logits']
            memory_weight = outputs.get('memory_weight', 0)
            
            # è®¡ç®—æŒ‡æ ‡
            pred_labels = pred.argmax(1)
            metrics.compute_and_accum(pred_labels, ann)
            
            total_memory_weight += memory_weight
            total_samples += img.shape[0]
            
            # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
            if args.save_predictions:
                predictions.append({
                    'filename': filename[0] if isinstance(filename, list) else filename,
                    'pred': pred_labels.cpu(),
                    'ann': ann.cpu(),
                    'memory_weight': memory_weight
                })
    
    # è·å–æœ€ç»ˆç»“æœ
    results = metrics.get_and_reset()
    avg_memory_weight = total_memory_weight / total_samples
    
    # è·å–æœ€ç»ˆè®°å¿†åº“çŠ¶æ€ï¼ˆåº”è¯¥å’Œåˆå§‹çŠ¶æ€ç›¸åŒï¼‰
    final_memory_stats = model.get_memory_stats()
    
    console.print(f"\nğŸ“ˆ æ¨ç†æ¨¡å¼ç»“æœ:")
    console.print(f"  mIoU: {results['IoU'].mean():.5f}")
    console.print(f"  mDice: {results['Dice'].mean():.5f}")
    console.print(f"  æ•´ä½“å‡†ç¡®ç‡: {results['aAcc']:.5f}")
    console.print(f"  å¹³å‡è®°å¿†æƒé‡: {avg_memory_weight:.4f}")
    console.print(f"  è®°å¿†åº“çŠ¶æ€: æ— å˜åŒ– (æ¨ç†æ¨¡å¼)")
    
    return {
        'mode': 'inference',
        'miou': results['IoU'].mean(),
        'dice': results['Dice'].mean(),
        'accuracy': results['aAcc'],
        'memory_weight': avg_memory_weight,
        'detailed_results': results,
        'predictions': predictions,
        'memory_stats': {
            'initial': initial_memory_stats,
            'final': final_memory_stats
        }
    }


def fast_mode_test(model, dataloader, device: str, args: Namespace) -> Dict[str, Any]:
    """å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼Œå‡å°‘è¯¦ç»†ç»Ÿè®¡"""
    console.print(f"\n[bold cyan]âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    console.print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataloader)}")
    
    num_categories = len(Category.load(args.category_csv))
    if any(cat.id == 255 for cat in Category.load(args.category_csv)):
        num_categories -= 1
    
    model.eval()
    correct_pixels = 0
    total_pixels = 0
    total_memory_weight = 0
    total_samples = 0
    
    # ç®€åŒ–çš„IoUè®¡ç®—
    class_intersect = torch.zeros(num_categories)
    class_union = torch.zeros(num_categories)
    
    with torch.no_grad():
        for batch_idx, data in enumerate(track(dataloader, description="å¿«é€Ÿæµ‹è¯•")):
            img = data["img"].to(device)
            ann = data["ann"].to(device)[:, 0, :, :]
            gps = data["gps"].to(device)
            
            # å‰å‘ä¼ æ’­ï¼ˆå¯é€‰æ‹©æ˜¯å¦æ›´æ–°è®°å¿†ï¼‰
            update_memory = not args.skip_memory_update
            outputs = model(img, gps, return_embeddings=False, update_memory=update_memory)
            pred = outputs['segmentation_logits']
            memory_weight = outputs.get('memory_weight', 0)
            
            # å¿«é€Ÿå‡†ç¡®ç‡è®¡ç®—
            pred_labels = pred.argmax(1)
            correct_pixels += (pred_labels == ann).sum().item()
            total_pixels += ann.numel()
            
            # ç®€åŒ–çš„ç±»åˆ«IoUè®¡ç®—
            for c in range(num_categories):
                pred_c = (pred_labels == c)
                gt_c = (ann == c)
                class_intersect[c] += (pred_c & gt_c).sum().item()
                class_union[c] += (pred_c | gt_c).sum().item()
            
            total_memory_weight += memory_weight
            total_samples += img.shape[0]
    
    # è®¡ç®—ç»“æœ
    pixel_accuracy = correct_pixels / total_pixels
    class_iou = class_intersect / (class_union + 1e-8)
    mean_iou = class_iou.mean().item()
    avg_memory_weight = total_memory_weight / total_samples
    
    # è·å–è®°å¿†åº“ç»Ÿè®¡
    memory_stats = model.get_memory_stats()
    
    console.print(f"\nğŸ“ˆ å¿«é€Ÿæµ‹è¯•ç»“æœ:")
    console.print(f"  åƒç´ å‡†ç¡®ç‡: {pixel_accuracy:.5f}")
    console.print(f"  mIoU: {mean_iou:.5f}")
    console.print(f"  å¹³å‡è®°å¿†æƒé‡: {avg_memory_weight:.4f}")
    console.print(f"  è®°å¿†åº“çŠ¶æ€: {memory_stats['total_locations']} ä½ç½®, {memory_stats['total_memories']} è®°å¿†")
    
    return {
        'mode': 'fast',
        'pixel_accuracy': pixel_accuracy,
        'miou': mean_iou,
        'memory_weight': avg_memory_weight,
        'memory_stats': {'final': memory_stats}
    }


def benchmark_mode_test(model, dataloader, device: str, args: Namespace) -> Dict[str, Any]:
    """åŸºå‡†æµ‹è¯•æ¨¡å¼ï¼šä¸»è¦æµ‹è¯•é€Ÿåº¦æ€§èƒ½"""
    console.print(f"\n[bold magenta]ğŸƒ åŸºå‡†æµ‹è¯•æ¨¡å¼ (æµ‹è¯•é€Ÿåº¦)")
    console.print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataloader)}")
    
    import time
    
    model.eval()
    
    # é¢„çƒ­
    console.print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
    with torch.no_grad():
        for i, data in enumerate(dataloader):
            if i >= 5:  # é¢„çƒ­5ä¸ªæ‰¹æ¬¡
                break
            img = data["img"].to(device)
            gps = data["gps"].to(device)
            _ = model(img, gps, update_memory=False)
    
    # æ­£å¼åŸºå‡†æµ‹è¯•
    times_inference = []
    times_memory = []
    memory_weights = []
    
    with torch.no_grad():
        for batch_idx, data in enumerate(track(dataloader, description="åŸºå‡†æµ‹è¯•")):
            if batch_idx >= 100:  # åªæµ‹è¯•100ä¸ªæ ·æœ¬
                break
                
            img = data["img"].to(device)
            gps = data["gps"].to(device)
            
            # æµ‹è¯•çº¯æ¨ç†é€Ÿåº¦
            torch.cuda.synchronize() if device == 'cuda' else None
            start_time = time.time()
            outputs = model(img, gps, update_memory=False)
            torch.cuda.synchronize() if device == 'cuda' else None
            inference_time = time.time() - start_time
            times_inference.append(inference_time)
            
            # æµ‹è¯•åŒ…å«è®°å¿†æ›´æ–°çš„é€Ÿåº¦
            torch.cuda.synchronize() if device == 'cuda' else None
            start_time = time.time()
            outputs = model(img, gps, update_memory=True)
            torch.cuda.synchronize() if device == 'cuda' else None
            memory_time = time.time() - start_time
            times_memory.append(memory_time)
            
            memory_weights.append(outputs.get('memory_weight', 0))
    
    # è®¡ç®—ç»Ÿè®¡
    avg_inference_time = np.mean(times_inference) * 1000  # ms
    avg_memory_time = np.mean(times_memory) * 1000  # ms
    memory_overhead = avg_memory_time - avg_inference_time
    avg_memory_weight = np.mean(memory_weights)
    
    fps_inference = 1000 / avg_inference_time
    fps_memory = 1000 / avg_memory_time
    
    console.print(f"\nâš¡ åŸºå‡†æµ‹è¯•ç»“æœ:")
    console.print(f"  çº¯æ¨ç†æ—¶é—´: {avg_inference_time:.2f} ms ({fps_inference:.1f} FPS)")
    console.print(f"  è®°å¿†æ¨¡å¼æ—¶é—´: {avg_memory_time:.2f} ms ({fps_memory:.1f} FPS)")
    console.print(f"  è®°å¿†å¼€é”€: {memory_overhead:.2f} ms ({memory_overhead/avg_inference_time*100:.1f}%)")
    console.print(f"  å¹³å‡è®°å¿†æƒé‡: {avg_memory_weight:.4f}")
    
    return {
        'mode': 'benchmark', 
        'inference_time_ms': avg_inference_time,
        'memory_time_ms': avg_memory_time,
        'memory_overhead_ms': memory_overhead,
        'fps_inference': fps_inference,
        'fps_memory': fps_memory,
        'memory_weight': avg_memory_weight
    }
def progressive_mode_test(model, dataloader, device: str, args: Namespace) -> Dict[str, Any]:
    """æ¸è¿›å¼æµ‹è¯•ï¼šè¾¹æµ‹è¯•è¾¹å»ºç«‹æ–°è®°å¿†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    console.print(f"\n[bold yellow]ğŸ§ª æ¸è¿›å¼æµ‹è¯• (å»ºç«‹æ–°è®°å¿†)")
    console.print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(dataloader)}")
    console.print(f"  è¯„ä¼°é—´éš”: {args.eval_interval}")
    
    num_categories = len(Category.load(args.category_csv))
    if any(cat.id == 255 for cat in Category.load(args.category_csv)):
        num_categories -= 1
    
    model.eval()
    total_memory_weight = 0
    total_samples = 0
    predictions = []
    
    # è®°å½•è¿›åº¦ï¼ˆå‡å°‘è®°å½•é¢‘ç‡ï¼‰
    progress_log = {
        'samples': [],
        'memory_locations': [],
        'memory_count': [],
        'hit_rates': [],
        'memory_weights': []
    }
    
    # è·å–åˆå§‹è®°å¿†åº“çŠ¶æ€
    initial_memory_stats = model.get_memory_stats()
    console.print(f"ğŸ“Š åˆå§‹è®°å¿†åº“çŠ¶æ€:")
    console.print(f"  ä½ç½®æ•°: {initial_memory_stats['total_locations']}")
    console.print(f"  è®°å¿†æ•°: {initial_memory_stats['total_memories']}")
    console.print(f"  å‘½ä¸­ç‡: {initial_memory_stats['hit_rate']:.4f}")
    
    # åˆ†æ‰¹è®¡ç®—æŒ‡æ ‡ä»¥èŠ‚çœå†…å­˜å’Œæ—¶é—´
    metrics = None
    if not args.fast_eval:
        metrics = Metrics(num_categories, nan_to_num=0)
    
    with torch.no_grad():
        for batch_idx, data in enumerate(track(dataloader, description="æ¸è¿›å¼æµ‹è¯•")):
            img = data["img"].to(device)
            ann = data["ann"].to(device)[:, 0, :, :]
            gps = data["gps"].to(device)
            filename = data.get("filename", [f"sample_{batch_idx}"])
            
            # å‰å‘ä¼ æ’­å¹¶æ›´æ–°è®°å¿†ï¼ˆå…³é”®ï¼šupdate_memory=Trueï¼‰  
            outputs = model(img, gps, return_embeddings=False, update_memory=True)
            pred = outputs['segmentation_logits']
            memory_weight = outputs.get('memory_weight', 0)
            pred_labels = pred.argmax(1)
            
            # åªåœ¨éœ€è¦æ—¶è®¡ç®—è¯¦ç»†æŒ‡æ ‡
            if not args.fast_eval and metrics is not None:
                metrics.compute_and_accum(pred_labels, ann)
            
            total_memory_weight += memory_weight
            total_samples += img.shape[0]
            
            # å‡å°‘è®°å½•é¢‘ç‡
            if batch_idx % args.eval_interval == 0:
                current_stats = model.get_memory_stats()
                progress_log['samples'].append(batch_idx + 1)
                progress_log['memory_locations'].append(current_stats['total_locations'])
                progress_log['memory_count'].append(current_stats['total_memories'])
                progress_log['hit_rates'].append(current_stats['hit_rate'])
                progress_log['memory_weights'].append(memory_weight)
                
                console.print(f"  æ ·æœ¬ {batch_idx + 1}: è®°å¿†åº“ {current_stats['total_locations']} ä½ç½®, "
                             f"å‘½ä¸­ç‡ {current_stats['hit_rate']:.3f}, è®°å¿†æƒé‡ {memory_weight:.3f}")
            
            # åªä¿å­˜éƒ¨åˆ†é¢„æµ‹ç»“æœ
            if args.save_predictions and len(predictions) < 50:
                predictions.append({
                    'filename': filename[0] if isinstance(filename, list) else filename,
                    'pred': pred_labels.cpu(),
                    'ann': ann.cpu(),
                    'memory_weight': memory_weight
                })
    
    # è·å–ç»“æœ
    results = {}
    if not args.fast_eval and metrics is not None:
        results = metrics.get_and_reset()
        miou = results['IoU'].mean()
        dice = results['Dice'].mean()
        accuracy = results['aAcc']
    else:
        # å¿«é€Ÿè¯„ä¼°æ¨¡å¼
        miou = dice = accuracy = 0.0
    
    avg_memory_weight = total_memory_weight / total_samples
    
    # è·å–æœ€ç»ˆè®°å¿†åº“çŠ¶æ€
    final_memory_stats = model.get_memory_stats()
    
    console.print(f"\nğŸ“ˆ æ¸è¿›å¼æµ‹è¯•ç»“æœ:")
    if not args.fast_eval:
        console.print(f"  mIoU: {miou:.5f}")
        console.print(f"  mDice: {dice:.5f}")
        console.print(f"  æ•´ä½“å‡†ç¡®ç‡: {accuracy:.5f}")
    console.print(f"  å¹³å‡è®°å¿†æƒé‡: {avg_memory_weight:.4f}")
    console.print(f"ğŸ“Š è®°å¿†åº“å˜åŒ–:")
    console.print(f"  ä½ç½®æ•°: {initial_memory_stats['total_locations']} â†’ {final_memory_stats['total_locations']}")
    console.print(f"  è®°å¿†æ•°: {initial_memory_stats['total_memories']} â†’ {final_memory_stats['total_memories']}")
    console.print(f"  å‘½ä¸­ç‡: {initial_memory_stats['hit_rate']:.4f} â†’ {final_memory_stats['hit_rate']:.4f}")
    
    return {
        'mode': 'progressive',
        'miou': miou,
        'dice': dice,
        'accuracy': accuracy,
        'memory_weight': avg_memory_weight,
        'detailed_results': results,
        'predictions': predictions,
        'memory_stats': {
            'initial': initial_memory_stats,
            'final': final_memory_stats
        },
        'progress_log': progress_log
    }


def save_predictions(predictions: List[Dict], categories: List, save_dir: str):
    """ä¿å­˜é¢„æµ‹ç»“æœå¯è§†åŒ–"""
    console.print(f"\n[bold cyan]ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœ...")
    
    pred_dir = os.path.join(save_dir, "predictions")
    os.makedirs(pred_dir, exist_ok=True)
    
    visualizer = IdMapVisualizer(categories)
    img_saver = ImgSaver(pred_dir, visualizer)
    
    for i, pred_data in enumerate(predictions[:20]):  # åªä¿å­˜å‰20ä¸ª
        filename = pred_data['filename']
        pred = pred_data['pred']
        ann = pred_data['ann']
        memory_weight = pred_data['memory_weight']
        
        # ä¿å­˜é¢„æµ‹å’Œæ ‡æ³¨
        img_saver.save_pred(pred[None, :], f"{filename}_pred.png")
        img_saver.save_ann(ann, f"{filename}_gt.png")
        
        console.print(f"  {filename}: è®°å¿†æƒé‡ {memory_weight:.4f}")
    
    console.print(f"âœ… é¢„æµ‹ç»“æœä¿å­˜åˆ°: {pred_dir}")


def save_results(results: Dict, args: Namespace):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    import json
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ä¿å­˜æ•°å€¼ç»“æœ
    results_file = os.path.join(args.save_dir, f"test_results_{results['mode']}.json")
    
    # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
    serializable_results = {
        'mode': results['mode'],
        'miou': float(results['miou']),
        'dice': float(results['dice']),
        'accuracy': float(results['accuracy']),
        'memory_weight': float(results['memory_weight']),
        'memory_stats': results['memory_stats']
    }
    
    if 'progress_log' in results:
        serializable_results['progress_log'] = results['progress_log']
    
    with open(results_file, 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_file = os.path.join(args.save_dir, f"test_report_{results['mode']}.txt")
    with open(report_file, 'w') as f:
        f.write(f"Memory-Enhanced GeoSegformer Test Report ({results['mode'].upper()} Mode)\n")
        f.write("=" * 60 + "\n\n")
        
        f.write("Test Configuration:\n")
        f.write(f"  Checkpoint: {args.checkpoint}\n")
        f.write(f"  Test Data: {args.img_dir}\n")
        f.write(f"  GPS Data: {args.gps_csv}\n")
        f.write(f"  Test Mode: {results['mode']}\n")
        f.write(f"  Batch Size: {args.batch_size}\n\n")
        
        f.write("Performance Results:\n")
        f.write(f"  mIoU: {results['miou']:.5f}\n")
        f.write(f"  mDice: {results['dice']:.5f}\n")
        f.write(f"  Overall Accuracy: {results['accuracy']:.5f}\n")
        f.write(f"  Average Memory Weight: {results['memory_weight']:.4f}\n\n")
        
        f.write("Memory Statistics:\n")
        initial_stats = results['memory_stats']['initial']
        final_stats = results['memory_stats']['final']
        f.write(f"  Initial Locations: {initial_stats['total_locations']}\n")
        f.write(f"  Final Locations: {final_stats['total_locations']}\n")
        f.write(f"  Initial Memories: {initial_stats['total_memories']}\n")
        f.write(f"  Final Memories: {final_stats['total_memories']}\n")
        f.write(f"  Initial Hit Rate: {initial_stats['hit_rate']:.4f}\n")
        f.write(f"  Final Hit Rate: {final_stats['hit_rate']:.4f}\n")
    
    console.print(f"âœ… ç»“æœä¿å­˜åˆ°:")
    console.print(f"  æ•°å€¼ç»“æœ: {results_file}")
    console.print(f"  æ–‡æœ¬æŠ¥å‘Š: {report_file}")


def main(args: Namespace):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    console.print(f"[bold green]ğŸš€ è®°å¿†å¢å¼ºç‰ˆ GeoSegformer æµ‹è¯•")
    console.print(f"  è®¾å¤‡: {device}")
    console.print(f"  æµ‹è¯•æ¨¡å¼: {args.test_mode}")
    
    # åŠ è½½ç±»åˆ«
    categories = Category.load(args.category_csv)
    num_categories = Category.get_num_categories(categories)
    console.print(f"  ç±»åˆ«æ•°: {num_categories}")
    
    # è®¾ç½®GPSæ­£è§„åŒ–ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    console.print(f"\nğŸ“ è®¾ç½®GPSæ­£è§„åŒ–...")
    gps_normalizer = setup_gps_normalization(
        args.train_gps_csv, 
        args.val_gps_csv,
        method=args.gps_norm_method
    )
    
    # æ•°æ®å˜æ¢
    image_size = 1080, 1920
    transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,  # å…³é”®ï¼šä½¿ç”¨å’Œè®­ç»ƒæ—¶ç›¸åŒçš„GPSæ­£è§„åŒ–
        transform.Resize(image_size),
        transform.Normalize(),
    ]
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    console.print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
    dataset = MemoryEnhancedGeoSegDataset(
        transforms=transforms,
        img_dir=args.img_dir,
        ann_dir=args.ann_dir,
        gps_csv=args.gps_csv,
        max_len=args.max_len,
    )
    
    dataloader = dataset.get_loader(
        batch_size=args.batch_size,
        pin_memory=False,
        num_workers=args.num_workers
    )
    
    # åŠ è½½æ¨¡å‹
    model = load_model_from_checkpoint(args.checkpoint, num_categories, args, device)
    
    # æ ¹æ®æµ‹è¯•æ¨¡å¼è¿›è¡Œæµ‹è¯•
    if args.test_mode == "inference":
        results = inference_mode_test(model, dataloader, device, args)
    elif args.test_mode == "progressive":
        results = progressive_mode_test(model, dataloader, device, args)
    elif args.test_mode == "fast":
        results = fast_mode_test(model, dataloader, device, args)
    elif args.test_mode == "benchmark":
        results = benchmark_mode_test(model, dataloader, device, args)
    else:  # memory_aware
        results = memory_aware_test(model, dataloader, device, args, match_info)
        results['overlap_stats'] = overlap_stats
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if args.save_predictions and results['predictions']:
        save_predictions(results['predictions'], categories, args.save_dir)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    save_results(results, args)
    
    # æ˜¾ç¤ºæ€»ç»“
    console.print(f"\n[bold green]ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    console.print(f"ğŸ“Š {args.test_mode.upper()} æ¨¡å¼ç»“æœ:")
    console.print(f"  mIoU: {results['miou']:.5f}")
    console.print(f"  è®°å¿†æƒé‡: {results['memory_weight']:.4f}")
    if args.test_mode == "progressive":
        initial = results['memory_stats']['initial']
        final = results['memory_stats']['final']
        console.print(f"  è®°å¿†å¢é•¿: {initial['total_locations']} â†’ {final['total_locations']} ä½ç½®")
    console.print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {args.save_dir}")


if __name__ == "__main__":
    args = parse_args()
    main(args)