#!/usr/bin/env python3
"""
Complete GPS Parameter Analyzer for GeoSegformer
è‡ªå‹•åˆ†æGPSæ•¸æ“šä¸¦æ¨è–¦æœ€é©åˆçš„GeoSegformeråƒæ•¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict
import argparse
from typing import Dict, List, Tuple
import json
from datetime import datetime
import sys
import os

class GPSParameterAnalyzer:
    def __init__(self, train_gps_csv: str, val_gps_csv: str):
        """
        GPSæ•¸æ“šåˆ†æå™¨ï¼Œè‡ªå‹•æ¨è–¦GeoSegformeråƒæ•¸
        
        Args:
            train_gps_csv: è¨“ç·´é›†GPS CSVæª”æ¡ˆè·¯å¾‘
            val_gps_csv: é©—è­‰é›†GPS CSVæª”æ¡ˆè·¯å¾‘
        """
        self.train_gps_csv = train_gps_csv
        self.val_gps_csv = val_gps_csv
        self.train_data = None
        self.val_data = None
        self.all_data = None
        self.analysis_results = {}
        
    def load_data(self):
        """è¼‰å…¥GPSæ•¸æ“š"""
        print("ğŸ“‚ Loading GPS data...")
        
        try:
            self.train_data = pd.read_csv(self.train_gps_csv)
            self.val_data = pd.read_csv(self.val_gps_csv)
            
            print(f"âœ… Training data: {len(self.train_data)} samples")
            print(f"âœ… Validation data: {len(self.val_data)} samples")
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_cols = ['filename', 'lat', 'long']
            for df_name, df in [('train', self.train_data), ('val', self.val_data)]:
                missing_cols = [col for col in required_cols if col not in df.columns]
                if missing_cols:
                    raise ValueError(f"Missing columns in {df_name} data: {missing_cols}")
            
            # åˆä½µæ•¸æ“šç”¨æ–¼å…¨å±€åˆ†æ
            self.all_data = pd.concat([self.train_data, self.val_data], ignore_index=True)
            print(f"âœ… Total data: {len(self.all_data)} samples")
            
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            raise
    
    def analyze_spatial_distribution(self):
        """åˆ†æç©ºé–“åˆ†å¸ƒ"""
        print("\nğŸ—ºï¸  Analyzing spatial distribution...")
        
        # åŸºæœ¬çµ±è¨ˆ
        lat_stats = {
            'min': float(self.all_data['lat'].min()),
            'max': float(self.all_data['lat'].max()),
            'mean': float(self.all_data['lat'].mean()),
            'std': float(self.all_data['lat'].std()),
            'range': float(self.all_data['lat'].max() - self.all_data['lat'].min())
        }
        
        long_stats = {
            'min': float(self.all_data['long'].min()),
            'max': float(self.all_data['long'].max()),
            'mean': float(self.all_data['long'].mean()),
            'std': float(self.all_data['long'].std()),
            'range': float(self.all_data['long'].max() - self.all_data['long'].min())
        }
        
        print(f"ğŸ“Š Latitude:  [{lat_stats['min']:.6f}, {lat_stats['max']:.6f}] (range: {lat_stats['range']:.6f})")
        print(f"ğŸ“Š Longitude: [{long_stats['min']:.6f}, {long_stats['max']:.6f}] (range: {long_stats['range']:.6f})")
        
        # è¨ˆç®—ç©ºé–“å¯†åº¦
        spatial_area = lat_stats['range'] * long_stats['range']
        point_density = len(self.all_data) / spatial_area if spatial_area > 0 else 0
        
        self.analysis_results['spatial'] = {
            'lat_stats': lat_stats,
            'long_stats': long_stats,
            'spatial_area': float(spatial_area),
            'point_density': float(point_density)
        }
        
        return lat_stats, long_stats, spatial_area, point_density
    
    def analyze_location_clusters(self):
        """åˆ†æä½ç½®èšé¡"""
        print("\nğŸ¯ Analyzing location clusters...")
        
        # ä½¿ç”¨ä¸åŒç²¾åº¦åˆ†æé‡è¤‡ä½ç½®
        precisions = [4, 5, 6, 7]  # å°æ•¸é»å¾Œä½æ•¸
        cluster_analysis = {}
        
        for precision in precisions:
            # å››æ¨äº”å…¥åˆ°æŒ‡å®šç²¾åº¦
            rounded_coords = self.all_data[['lat', 'long']].round(precision)
            coord_strings = rounded_coords.apply(lambda x: f"{x['lat']:.{precision}f},{x['long']:.{precision}f}", axis=1)
            
            # çµ±è¨ˆæ¯å€‹ä½ç½®çš„åœ–åƒæ•¸é‡
            location_counts = coord_strings.value_counts()
            
            cluster_analysis[precision] = {
                'unique_locations': int(len(location_counts)),
                'avg_images_per_location': float(location_counts.mean()),
                'max_images_per_location': int(location_counts.max()),
                'locations_with_multiple_images': int((location_counts > 1).sum()),
                'duplicate_rate': float((location_counts > 1).sum() / len(location_counts) * 100)
            }
            
            print(f"ğŸ“ Precision {precision}: {cluster_analysis[precision]['unique_locations']} unique locations, "
                  f"avg {cluster_analysis[precision]['avg_images_per_location']:.1f} images/location, "
                  f"duplicate rate {cluster_analysis[precision]['duplicate_rate']:.1f}%")
        
        # é¸æ“‡æœ€ä½³ç²¾åº¦ï¼ˆå¹³è¡¡å”¯ä¸€æ€§å’Œå¯¦ç”¨æ€§ï¼‰
        best_precision = self.select_best_precision(cluster_analysis)
        
        self.analysis_results['clusters'] = {
            'precision_analysis': cluster_analysis,
            'best_precision': best_precision,
            'recommended_stats': cluster_analysis[best_precision]
        }
        
        return cluster_analysis, best_precision
    
    def select_best_precision(self, cluster_analysis: Dict) -> int:
        """é¸æ“‡æœ€ä½³ç²¾åº¦"""
        # ç›®æ¨™ï¼šæ‰¾åˆ°æ—¢æœ‰è¶³å¤ å”¯ä¸€ä½ç½®ï¼Œåˆæœ‰åˆç†é‡è¤‡ç‡çš„ç²¾åº¦
        scores = {}
        
        for precision, stats in cluster_analysis.items():
            # è©•åˆ†æ¨™æº–ï¼š
            # 1. å”¯ä¸€ä½ç½®æ•¸é‡é©ä¸­ï¼ˆä¸è¦å¤ªå¤šä¹Ÿä¸è¦å¤ªå°‘ï¼‰
            # 2. å¹³å‡æ¯ä½ç½®åœ–åƒæ•¸åœ¨2-10ä¹‹é–“è¼ƒå¥½
            # 3. é‡è¤‡ç‡åœ¨10-50%ä¹‹é–“è¼ƒå¥½
            
            unique_score = min(stats['unique_locations'] / 100, 10)  # ä¸è¦å¤ªå¤šä½ç½®
            avg_images_score = max(0, 10 - abs(stats['avg_images_per_location'] - 5))  # ç†æƒ³æ˜¯5å¼µ/ä½ç½®
            duplicate_score = max(0, 10 - abs(stats['duplicate_rate'] - 30))  # ç†æƒ³æ˜¯30%é‡è¤‡ç‡
            
            scores[precision] = unique_score + avg_images_score + duplicate_score
        
        return max(scores.keys(), key=lambda x: scores[x])
    
    def calculate_distance_statistics(self, precision: int):
        """è¨ˆç®—è·é›¢çµ±è¨ˆ"""
        print(f"\nğŸ“ Calculating distance statistics (precision {precision})...")
        
        # ä½¿ç”¨é¸å®šç²¾åº¦çš„åæ¨™
        coords = self.all_data[['lat', 'long']].round(precision)
        unique_coords = coords.drop_duplicates().values
        
        if len(unique_coords) < 2:
            print("âš ï¸  Not enough unique coordinates for distance analysis")
            return None
        
        # è¨ˆç®—æ‰€æœ‰é»å°ä¹‹é–“çš„è·é›¢ï¼ˆé™åˆ¶æ•¸é‡é¿å…è¨ˆç®—éä¹…ï¼‰
        distances = []
        max_pairs = min(10000, len(unique_coords) * (len(unique_coords) - 1) // 2)
        
        for i in range(len(unique_coords)):
            for j in range(i + 1, len(unique_coords)):
                if len(distances) >= max_pairs:
                    break
                    
                lat1, lon1 = unique_coords[i]
                lat2, lon2 = unique_coords[j]
                
                # è¨ˆç®—æ­æ°è·é›¢ï¼ˆé©åˆå°ç¯„åœï¼‰
                dist = np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)
                distances.append(dist)
            
            if len(distances) >= max_pairs:
                break
        
        distances = np.array(distances)
        
        distance_stats = {
            'min_distance': float(distances.min()),
            'max_distance': float(distances.max()),
            'mean_distance': float(distances.mean()),
            'median_distance': float(np.median(distances)),
            'std_distance': float(distances.std()),
            'percentiles': {
                'p10': float(np.percentile(distances, 10)),
                'p25': float(np.percentile(distances, 25)),
                'p75': float(np.percentile(distances, 75)),
                'p90': float(np.percentile(distances, 90))
            }
        }
        
        print(f"ğŸ“ Distance statistics:")
        print(f"   Min: {distance_stats['min_distance']:.6f}")
        print(f"   Mean: {distance_stats['mean_distance']:.6f}")
        print(f"   Median: {distance_stats['median_distance']:.6f}")
        print(f"   P25: {distance_stats['percentiles']['p25']:.6f}")
        print(f"   P75: {distance_stats['percentiles']['p75']:.6f}")
        
        self.analysis_results['distances'] = distance_stats
        
        return distance_stats
    
    def recommend_parameters(self):
        """æ¨è–¦GeoSegformeråƒæ•¸"""
        print("\nğŸ¯ Recommending GeoSegformer parameters...")
        
        # ç²å–åˆ†æçµæœ
        spatial = self.analysis_results['spatial']
        clusters = self.analysis_results['clusters']
        distances = self.analysis_results.get('distances')
        
        recommended_stats = clusters['recommended_stats']
        
        # 1. ç©ºé–“åŠå¾‘ (spatial_radius)
        if distances:
            # åŸºæ–¼è·é›¢çµ±è¨ˆæ¨è–¦
            spatial_radius = max(
                distances['percentiles']['p10'] * 0.5,  # ä¸è¦å¤ªå°
                distances['mean_distance'] * 0.05,      # å¹³å‡è·é›¢çš„5%
                0.00001  # æœ€å°å€¼
            )
            spatial_radius = min(spatial_radius, 0.001)  # æœ€å¤§å€¼
        else:
            # åŸºæ–¼ç©ºé–“ç¯„åœæ¨è–¦
            avg_range = (spatial['lat_stats']['range'] + spatial['long_stats']['range']) / 2
            spatial_radius = avg_range / (recommended_stats['unique_locations'] ** 0.5) * 0.1
        
        # 2. ç©ºé–“é–¾å€¼ (spatial_threshold)
        spatial_threshold = spatial_radius * 3
        
        # 3. è¨˜æ†¶åº«å¤§å° (memory_size)
        avg_imgs_per_location = recommended_stats['avg_images_per_location']
        if avg_imgs_per_location < 2:
            memory_size = 10
        elif avg_imgs_per_location < 5:
            memory_size = 15
        elif avg_imgs_per_location < 10:
            memory_size = 20
        else:
            memory_size = min(30, int(avg_imgs_per_location * 1.5))
        
        # 4. å°æ¯”å­¸ç¿’æ¬Šé‡ (contrastive_weight)
        duplicate_rate = recommended_stats['duplicate_rate']
        if duplicate_rate < 10:
            contrastive_weight = 0.01
        elif duplicate_rate < 30:
            contrastive_weight = 0.05
        elif duplicate_rate < 50:
            contrastive_weight = 0.03
        else:
            contrastive_weight = 0.02
        
        # 5. è¨˜æ†¶åº«é ç†±è¼ªæ•¸ (memory_warmup_epochs)
        total_samples = len(self.all_data)
        if total_samples < 1000:
            memory_warmup_epochs = 2
        elif total_samples < 5000:
            memory_warmup_epochs = 3
        else:
            memory_warmup_epochs = 5
        
        # 6. æ¨¡å‹å¤§å° (model_size)
        if total_samples < 2000:
            model_size = "b0"
            feature_dim = 256
        elif total_samples < 8000:
            model_size = "b0"
            feature_dim = 512
        else:
            model_size = "b1"
            feature_dim = 512
        
        # 7. èåˆæ–¹æ³• (fusion_method)
        if total_samples < 3000:
            fusion_method = "concat"  # è¼ƒå¿«
        else:
            fusion_method = "attention"  # è¼ƒå¥½æ•ˆæœ
        
        # 8. å­¸ç¿’ç‡æ¨è–¦
        lr_scale = 1.0
        if total_samples < 2000:
            lr_scale = 0.7  # å°æ•¸æ“šé›†ç”¨è¼ƒå°å­¸ç¿’ç‡
        elif total_samples > 10000:
            lr_scale = 1.3  # å¤§æ•¸æ“šé›†å¯ä»¥ç”¨è¼ƒå¤§å­¸ç¿’ç‡
        
        recommendations = {
            'spatial_radius': float(round(spatial_radius, 6)),
            'spatial_threshold': float(round(spatial_threshold, 6)),
            'memory_size': int(memory_size),
            'contrastive_weight': float(contrastive_weight),
            'memory_warmup_epochs': int(memory_warmup_epochs),
            'model_size': model_size,
            'feature_dim': int(feature_dim),
            'fusion_method': fusion_method,
            'lr_backbone': float(round(6e-5 * lr_scale, 6)),
            'lr_head': float(round(6e-4 * lr_scale, 6)),
            'lr_gps': float(round(3e-4 * lr_scale, 6)),
            'lr_memory': float(round(6e-4 * lr_scale, 6)),
            'lr_fusion': float(round(6e-4 * lr_scale, 6)),
            'gps_norm_method': 'minmax',
            'seg_weight': float(1.0),
            'temperature': float(0.07)
        }
        
        self.analysis_results['recommendations'] = recommendations
        
        return recommendations
    
    def print_recommendations(self):
        """æ‰“å°æ¨è–¦çµæœ"""
        recommendations = self.analysis_results['recommendations']
        
        print(f"\nğŸ¯ RECOMMENDED GEOSEGFORMER PARAMETERS")
        print("=" * 60)
        
        print(f"\nğŸ—ï¸  Model Architecture:")
        print(f"--model-size {recommendations['model_size']}")
        print(f"--feature-dim {recommendations['feature_dim']}")
        print(f"--fusion-method {recommendations['fusion_method']}")
        
        print(f"\nğŸ—ºï¸  Spatial Parameters:")
        print(f"--spatial-radius {recommendations['spatial_radius']}")
        print(f"--spatial-threshold {recommendations['spatial_threshold']}")
        print(f"--gps-norm-method {recommendations['gps_norm_method']}")
        
        print(f"\nğŸ§  Memory Parameters:")
        print(f"--memory-size {recommendations['memory_size']}")
        print(f"--memory-warmup-epochs {recommendations['memory_warmup_epochs']}")
        
        print(f"\nâš–ï¸  Loss Parameters:")
        print(f"--seg-weight {recommendations['seg_weight']}")
        print(f"--contrastive-weight {recommendations['contrastive_weight']}")
        print(f"--temperature {recommendations['temperature']}")
        
        print(f"\nğŸ“ Learning Rate Parameters:")
        print(f"--lr-backbone {recommendations['lr_backbone']}")
        print(f"--lr-head {recommendations['lr_head']}")
        print(f"--lr-gps {recommendations['lr_gps']}")
        print(f"--lr-memory {recommendations['lr_memory']}")
        print(f"--lr-fusion {recommendations['lr_fusion']}")
        
        print(f"\nğŸ“‹ Complete Command Line:")
        print("=" * 60)
        cmd_parts = []
        for key, value in recommendations.items():
            if key in ['gps_norm_method', 'model_size', 'fusion_method']:
                cmd_parts.append(f"--{key.replace('_', '-')} {value}")
            else:
                cmd_parts.append(f"--{key.replace('_', '-')} {value}")
        
        print(" \\\n  ".join(cmd_parts))
    
    def save_analysis_report(self, output_path: str = "gps_analysis_report.json"):
        """å„²å­˜åˆ†æå ±å‘Š"""
        
        def convert_numpy_types(obj):
            """å°‡numpyé¡å‹è½‰æ›ç‚ºPythonåŸç”Ÿé¡å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        report = {
            'analysis_date': datetime.now().isoformat(),
            'input_files': {
                'train_gps_csv': self.train_gps_csv,
                'val_gps_csv': self.val_gps_csv
            },
            'data_summary': {
                'train_samples': len(self.train_data),
                'val_samples': len(self.val_data),
                'total_samples': len(self.all_data)
            },
            'analysis_results': convert_numpy_types(self.analysis_results)
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Analysis report saved to: {output_path}")
    
    def create_visualization(self, output_dir: str = "gps_analysis_plots"):
        """å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨"""
        print(f"\nğŸ“Š Creating visualizations...")
        
        Path(output_dir).mkdir(exist_ok=True)
        
        try:
            # è¨­å®šåœ–è¡¨é¢¨æ ¼
            plt.style.use('default')
            
            # å‰µå»ºåœ–è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. GPSé»åˆ†å¸ƒåœ–
            axes[0, 0].scatter(self.train_data['long'], self.train_data['lat'], 
                           alpha=0.6, s=30, c='blue', label='Training')
            axes[0, 0].scatter(self.val_data['long'], self.val_data['lat'], 
                           alpha=0.6, s=30, c='red', label='Validation')
            axes[0, 0].set_xlabel('Longitude')
            axes[0, 0].set_ylabel('Latitude')
            axes[0, 0].set_title('GPS Points Distribution')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. è·é›¢åˆ†å¸ƒç›´æ–¹åœ–
            if 'distances' in self.analysis_results:
                distances = []
                coords = self.all_data[['lat', 'long']].values
                for i in range(min(100, len(coords))):
                    for j in range(i + 1, min(i + 20, len(coords))):
                        lat1, lon1 = coords[i]
                        lat2, lon2 = coords[j]
                        dist = np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)
                        distances.append(dist)
                
                axes[0, 1].hist(distances, bins=30, alpha=0.7, color='green')
                axes[0, 1].set_xlabel('Distance')
                axes[0, 1].set_ylabel('Frequency')
                axes[0, 1].set_title('Pairwise Distance Distribution')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ¯å€‹ä½ç½®çš„åœ–åƒæ•¸é‡åˆ†å¸ƒ
            coords = self.all_data[['lat', 'long']].round(5)
            coord_strings = coords.apply(lambda x: f"{x['lat']:.5f},{x['long']:.5f}", axis=1)
            location_counts = coord_strings.value_counts()
            
            axes[1, 0].hist(location_counts, bins=min(20, len(location_counts)//2), 
                    alpha=0.7, color='orange')
            axes[1, 0].set_xlabel('Images per Location')
            axes[1, 0].set_ylabel('Number of Locations')
            axes[1, 0].set_title('Images per Location Distribution')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. æ¨è–¦åƒæ•¸ç¸½çµ
            axes[1, 1].axis('off')
            
            recommendations = self.analysis_results['recommendations']
            text_content = f"""Recommended Parameters:

Spatial Radius: {recommendations['spatial_radius']}
Memory Size: {recommendations['memory_size']}
Contrastive Weight: {recommendations['contrastive_weight']}
Model Size: {recommendations['model_size']}
Feature Dim: {recommendations['feature_dim']}
Fusion Method: {recommendations['fusion_method']}

Data Summary:
Total Samples: {len(self.all_data)}
Unique Locations: {self.analysis_results['clusters']['recommended_stats']['unique_locations']}
Avg Images/Location: {self.analysis_results['clusters']['recommended_stats']['avg_images_per_location']:.1f}
Duplicate Rate: {self.analysis_results['clusters']['recommended_stats']['duplicate_rate']:.1f}%
            """
            
            axes[1, 1].text(0.05, 0.95, text_content, transform=axes[1, 1].transAxes, 
                    fontsize=10, verticalalignment='top', fontfamily='monospace')
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/gps_analysis_summary.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š Visualization saved to: {output_dir}/gps_analysis_summary.png")
            
        except Exception as e:
            print(f"âš ï¸  Could not create visualizations: {e}")
    
    def run_analysis(self, save_report: bool = True, create_plots: bool = True):
        """é‹è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ Starting GPS Parameter Analysis...")
        print("=" * 60)
        
        # è¼‰å…¥æ•¸æ“š
        self.load_data()
        
        # åˆ†æç©ºé–“åˆ†å¸ƒ
        self.analyze_spatial_distribution()
        
        # åˆ†æä½ç½®èšé¡
        _, best_precision = self.analyze_location_clusters()
        
        # è¨ˆç®—è·é›¢çµ±è¨ˆ
        self.calculate_distance_statistics(best_precision)
        
        # æ¨è–¦åƒæ•¸
        self.recommend_parameters()
        
        # æ‰“å°æ¨è–¦çµæœ
        self.print_recommendations()
        
        # ä¿å­˜å ±å‘Š
        if save_report:
            self.save_analysis_report()
        
        # å‰µå»ºè¦–è¦ºåŒ–
        if create_plots:
            self.create_visualization()
        
        print(f"\nâœ… Analysis completed!")
        return self.analysis_results['recommendations']


def check_gps_csv_format(csv_path: str):
    """æª¢æŸ¥GPS CSVæ ¼å¼"""
    print(f"\nğŸ” Checking GPS CSV format: {csv_path}")
    
    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… File loaded successfully: {len(df)} rows")
        
        # æª¢æŸ¥æ¬„ä½
        print(f"ğŸ“‹ Columns: {list(df.columns)}")
        
        required_cols = ['filename', 'lat', 'long']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"âŒ Missing required columns: {missing_cols}")
            print("Required columns: filename, lat, long")
            return False
        else:
            print("âœ… All required columns present")
        
        # æª¢æŸ¥æ•¸æ“šç¯„åœ
        print(f"ğŸ“Š Latitude range: [{df['lat'].min():.6f}, {df['lat'].max():.6f}]")
        print(f"ğŸ“Š Longitude range: [{df['long'].min():.6f}, {df['long'].max():.6f}]")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼
        if df[required_cols].isnull().any().any():
            print("âš ï¸  Warning: Found missing values in GPS data")
            print(df[required_cols].isnull().sum())
        else:
            print("âœ… No missing values in GPS data")
        
        # é¡¯ç¤ºå‰å¹¾è¡Œ
        print("\nğŸ“‹ First 5 rows:")
        print(df.head())
        
        return True
        
    except Exception as e:
        print(f"âŒ Error reading file: {e}")
        return False


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ GPS Parameter Analyzer for GeoSegformer")
    print("=" * 50)
    
    if len(sys.argv) == 1:
        print("Usage options:")
        print("1. python gps_analyzer.py check <gps_csv_file>")
        print("2. python gps_analyzer.py analyze <train_gps_csv> <val_gps_csv>")
        print()
        print("Example:")
        print("python gps_analyzer.py analyze data/train_gps.csv data/val_gps.csv")
        return
        
    elif len(sys.argv) == 3 and sys.argv[1] == "check":
        # æª¢æŸ¥å–®å€‹GPSæª”æ¡ˆæ ¼å¼
        csv_file = sys.argv[2]
        check_gps_csv_format(csv_file)
        
    elif len(sys.argv) == 4 and sys.argv[1] == "analyze":
        # åˆ†ææŒ‡å®šçš„GPSæª”æ¡ˆ
        train_gps_csv = sys.argv[2]
        val_gps_csv = sys.argv[3]
        
        print(f"ğŸ“‚ Training GPS: {train_gps_csv}")
        print(f"ğŸ“‚ Validation GPS: {val_gps_csv}")
        
        # æª¢æŸ¥æª”æ¡ˆå­˜åœ¨æ€§
        if not os.path.exists(train_gps_csv):
            print(f"âŒ Training GPS file not found: {train_gps_csv}")
            sys.exit(1)
            
        if not os.path.exists(val_gps_csv):
            print(f"âŒ Validation GPS file not found: {val_gps_csv}")
            sys.exit(1)
        
        # å…ˆæª¢æŸ¥æ ¼å¼
        print("\nğŸ” Checking file formats...")
        if not check_gps_csv_format(train_gps_csv):
            print("âŒ Training GPS file format error")
            sys.exit(1)
            
        if not check_gps_csv_format(val_gps_csv):
            print("âŒ Validation GPS file format error")
            sys.exit(1)
        
        # é‹è¡Œåˆ†æ
        print("\nğŸš€ Starting parameter analysis...")
        analyzer = GPSParameterAnalyzer(train_gps_csv, val_gps_csv)
        
        try:
            recommendations = analyzer.run_analysis(
                save_report=True,
                create_plots=True
            )
            
            print("\nğŸ‰ Analysis completed successfully!")
            
            # ç”Ÿæˆå¯ä»¥ç›´æ¥ä½¿ç”¨çš„å‘½ä»¤
            print("\nğŸ“‹ Ready-to-use training command:")
            print("=" * 60)
            
            # ç”Ÿæˆå®Œæ•´çš„è¨“ç·´å‘½ä»¤
            cmd = f"""python universal_enhanced_train.py \\
  [train_img_dir] [train_ann_dir] [val_img_dir] [val_ann_dir] [category_csv] [max_epochs] [logdir] \\
  --use-geo \\
  --train-gps-csv {train_gps_csv} \\
  --val-gps-csv {val_gps_csv} \\
  --model-size {recommendations['model_size']} \\
  --feature-dim {recommendations['feature_dim']} \\
  --fusion-method {recommendations['fusion_method']} \\
  --spatial-radius {recommendations['spatial_radius']} \\
  --spatial-threshold {recommendations['spatial_threshold']} \\
  --memory-size {recommendations['memory_size']} \\
  --memory-warmup-epochs {recommendations['memory_warmup_epochs']} \\
  --contrastive-weight {recommendations['contrastive_weight']} \\
  --lr-backbone {recommendations['lr_backbone']} \\
  --lr-head {recommendations['lr_head']} \\
  --lr-gps {recommendations['lr_gps']} \\
  --lr-memory {recommendations['lr_memory']} \\
  --lr-fusion {recommendations['lr_fusion']} \\
  --early-stop --patience 50 --batch-size 8"""
            
            print(cmd)
            
            # ä¿å­˜å‘½ä»¤åˆ°æª”æ¡ˆ
            with open("recommended_geosegformer_command.txt", "w") as f:
                f.write("# Recommended GeoSegformer Training Command\n")
                f.write("# Generated by GPS Parameter Analyzer\n\n")
                f.write(cmd)
                f.write("\n\n# Please replace the following placeholders:\n")
                f.write("# [train_img_dir] - path to training images\n")
                f.write("# [train_ann_dir] - path to training annotations\n")
                f.write("# [val_img_dir] - path to validation images\n")
                f.write("# [val_ann_dir] - path to validation annotations\n")
                f.write("# [category_csv] - path to category definition CSV\n")
                f.write("# [max_epochs] - maximum training epochs (e.g., 1000)\n")
                f.write("# [logdir] - output directory for logs and models\n")
                
            print(f"\nğŸ’¾ Command saved to: recommended_geosegformer_command.txt")
            
            # é¡å¤–çš„å»ºè­°
            print(f"\nğŸ’¡ Additional suggestions:")
            print(f"   - Start with a small test run (e.g., 10 epochs) to validate parameters")
            print(f"   - Monitor 'Memory Hit Rate' during training (should be 0.3-0.8)")
            print(f"   - If hit rate is too low, increase --memory-size")
            print(f"   - If hit rate is too high, decrease --spatial-radius")
            print(f"   - Consider using --multi-seed for robust results")
            
            # æ ¹æ“šæ•¸æ“šç‰¹å¾µçµ¦å‡ºç‰¹æ®Šå»ºè­°
            total_samples = len(analyzer.all_data)
            unique_locations = recommendations['memory_size']
            duplicate_rate = analyzer.analysis_results['clusters']['recommended_stats']['duplicate_rate']
            
            print(f"\nğŸ“Š Dataset Analysis Summary:")
            print(f"   Dataset size: {total_samples} images")
            print(f"   Unique locations: {analyzer.analysis_results['clusters']['recommended_stats']['unique_locations']}")
            print(f"   Duplicate rate: {duplicate_rate:.1f}%")
            print(f"   Avg images per location: {analyzer.analysis_results['clusters']['recommended_stats']['avg_images_per_location']:.1f}")
            
            # æ ¹æ“šæ•¸æ“šç‰¹å¾µçµ¦å‡ºå€‹æ€§åŒ–å»ºè­°
            print(f"\nğŸ¯ Personalized Recommendations:")
            if duplicate_rate < 10:
                print("   - Your dataset has low location overlap")
                print("   - Consider using lower contrastive weight")
                print("   - Focus on spatial accuracy rather than memory efficiency")
            elif duplicate_rate > 50:
                print("   - Your dataset has high location overlap")
                print("   - Memory system will be very effective")
                print("   - Consider increasing memory size for better performance")
            
            if total_samples < 2000:
                print("   - Small dataset: use conservative parameters")
                print("   - Consider data augmentation")
                print("   - Start with lower learning rates")
            elif total_samples > 10000:
                print("   - Large dataset: can use more aggressive parameters")
                print("   - Consider using larger model (b1 or b2)")
                print("   - Multi-seed training highly recommended")
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
            
    else:
        print("âŒ Invalid arguments")
        print("Usage:")
        print("  python gps_analyzer.py check <gps_csv_file>")
        print("  python gps_analyzer.py analyze <train_gps_csv> <val_gps_csv>")
        sys.exit(1)


if __name__ == "__main__":
    main()