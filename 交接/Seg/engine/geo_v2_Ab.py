import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import SegformerForSemanticSegmentation
from typing import Any, Dict, List, Optional
import json
import os
import pickle
from collections import defaultdict


class MultiScaleGPSToRFF(nn.Module):
    """
    å„ªåŒ–ç‰ˆå¤šå°ºåº¦ GPS Random Fourier Features ç·¨ç¢¼å™¨
    é‡å°å°ç¯„åœGPSåº§æ¨™å„ªåŒ–
    """
    def __init__(
        self, 
        rff_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],  # é‡å°ä½ çš„GPSç¯„åœå„ªåŒ–
        device: str = "cuda"
    ) -> None:
        super().__init__()
        self.rff_dim = rff_dim
        self.sigmas = sigmas
        self.num_scales = len(sigmas)
        
        # æ™ºèƒ½åˆ†é…ç¶­åº¦ - ç¢ºä¿ç¸½å’Œç­‰æ–¼ç›®æ¨™ç¶­åº¦
        base_features_per_scale = rff_dim // (2 * self.num_scales)
        remainder = rff_dim - (base_features_per_scale * 2 * self.num_scales)
        
        # å°‡å‰©é¤˜ç¶­åº¦åˆ†é…çµ¦å‰å¹¾å€‹å°ºåº¦
        self.features_per_scale = []
        for i in range(self.num_scales):
            extra = 1 if i < remainder // 2 else 0
            features_count = base_features_per_scale + extra
            self.features_per_scale.append(features_count)
        
        # å¦‚æœ remainder æ˜¯å¥‡æ•¸ï¼Œæœ€å¾Œä¸€å€‹å°ºåº¦å¤šåˆ†é… 1 å€‹ç‰¹å¾µ
        if remainder % 2 == 1:
            self.features_per_scale[-1] += 1
            
        # é©—è­‰ç¸½ç¶­åº¦
        total_dim = sum(f * 2 for f in self.features_per_scale)
        assert total_dim == rff_dim, f"ç¶­åº¦ä¸åŒ¹é…: {total_dim} != {rff_dim}"
        
        # ç‚ºæ¯å€‹å°ºåº¦å‰µå»ºä¸åŒçš„ RFF åƒæ•¸
        for i, (sigma, features_count) in enumerate(zip(sigmas, self.features_per_scale)):
            omega = torch.randn(features_count, 2) / sigma
            b = 2 * math.pi * torch.rand(features_count)
            
            # ä½¿ç”¨ register_bufferï¼Œé€™äº›ä¸éœ€è¦æ¢¯åº¦
            self.register_buffer(f'omega_{i}', omega)
            self.register_buffer(f'b_{i}', b)
        
        print(f"âœ… Optimized MultiScaleGPSToRFF:")
        print(f"  Target RFF dim: {rff_dim}")
        print(f"  Optimized sigmas: {sigmas}")
        print(f"  Features per scale: {self.features_per_scale}")
        print(f"  Total dim: {total_dim}")
    
    def forward(self, gps: torch.Tensor) -> torch.Tensor:
        """
        Args:
            gps: GPS coordinates tensor, shape (batch_size, 2) [lat, lon]
        Returns:
            GPS embeddings tensor, shape (batch_size, rff_dim)
        """
        batch_size = gps.shape[0]
        rff_features = []
        
        for i in range(self.num_scales):
            # ç²å–å°æ‡‰çš„ omega å’Œ b
            omega = getattr(self, f'omega_{i}')
            b = getattr(self, f'b_{i}')
            
            # è¨ˆç®—æŠ•å½± gps @ omega^T
            proj = torch.matmul(gps, omega.T)  # (batch_size, features_per_scale[i])
            
            # åŠ ä¸Šåç½®
            y = proj + b  # (batch_size, features_per_scale[i])
            
            # è¨ˆç®— cos å’Œ sin ç‰¹å¾µ
            rff = torch.cat([torch.cos(y), torch.sin(y)], dim=-1)  # (batch_size, features_per_scale[i] * 2)
            rff_features.append(rff)
        
        # é€£æ¥æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾µ
        gps_embeddings = torch.cat(rff_features, dim=-1)  # (batch_size, rff_dim)
        
        # é©—è­‰è¼¸å‡ºç¶­åº¦
        assert gps_embeddings.shape[-1] == self.rff_dim, f"è¼¸å‡ºç¶­åº¦éŒ¯èª¤: {gps_embeddings.shape[-1]} != {self.rff_dim}"
        
        return gps_embeddings


class LocationEncoder(nn.Module):
    """
    GPS ä½ç½®ç·¨ç¢¼å™¨ï¼Œå°‡ GPS åº§æ¨™è½‰æ›ç‚ºèªç¾©è±å¯Œçš„é«˜ç¶­ç‰¹å¾µ
    """
    def __init__(
        self, 
        rff_dim: int = 512,
        hidden_dim: int = 1024,
        output_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],
        dropout: float = 0.1
    ):
        super().__init__()
        
        # å¤šå°ºåº¦ RFF ç·¨ç¢¼å™¨
        self.rff_encoder = MultiScaleGPSToRFF(rff_dim, sigmas)
        
        # MLP å±¤ä¾†å­¸ç¿’æ›´è±å¯Œçš„ä½ç½®è¡¨ç¤º
        self.mlp = nn.Sequential(
            nn.Linear(rff_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim),
        )
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, gps: torch.Tensor) -> torch.Tensor:
        """
        Args:
            gps: GPS coordinates, shape (batch_size, 2)
        Returns:
            Location embeddings, shape (batch_size, output_dim)
        """
        # GPS RFF ç·¨ç¢¼
        rff_features = self.rff_encoder(gps)  # (batch_size, rff_dim)
        
        # MLP è™•ç†
        location_embeddings = self.mlp(rff_features)  # (batch_size, output_dim)
        
        # æ­£è¦åŒ–
        location_embeddings = self.layer_norm(location_embeddings)
        
        return location_embeddings


class MultiLayerGPSImageEncoder(nn.Module):
    """
    ğŸŒ å¤šå±¤GPSèåˆçš„å½±åƒç·¨ç¢¼å™¨ - ä»¿ç…§ GeoClip
    åœ¨ SegFormer çš„æ¯å€‹éšæ®µéƒ½æ³¨å…¥GPSä¿¡æ¯
    """
    def __init__(
        self, 
        segformer_model: str = "nvidia/mit-b0",
        feature_dim: int = 512
    ):
        super().__init__()
        
        # è¼‰å…¥é è¨“ç·´çš„ Segformer æ¨¡å‹
        self.segformer = SegformerForSemanticSegmentation.from_pretrained(segformer_model)
        
        # å–å¾— Segformer çš„ç‰¹å¾µæå–å™¨
        self.feature_extractor = self.segformer.segformer
        
        # ç²å¾— backbone çš„è¼¸å‡ºç¶­åº¦
        if "mit-b0" in segformer_model:
            backbone_dims = [32, 64, 160, 256]
        elif "mit-b1" in segformer_model:
            backbone_dims = [64, 128, 320, 512]
        else:
            backbone_dims = [32, 64, 160, 256]  # é»˜èªä½¿ç”¨ b0
        
        self.backbone_dims = backbone_dims
        
        # ğŸŒ ç‚ºæ¯å€‹éšæ®µå‰µå»ºGPSç·¨ç¢¼å™¨
        self.stage_gps_encoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(2, 128),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(128, dim),
                nn.LayerNorm(dim)
            ) for dim in backbone_dims
        ])
        
        # ğŸ”„ æ¯å€‹éšæ®µçš„GPS-Imageèåˆæ¨¡çµ„
        self.fusion_modules = nn.ModuleList([
            nn.MultiheadAttention(
                embed_dim=dim, 
                num_heads=min(8, dim//32),  # å‹•æ…‹èª¿æ•´æ³¨æ„åŠ›é ­æ•¸
                dropout=0.1,
                batch_first=True
            ) for dim in backbone_dims
        ])
        
        # ç‰¹å¾µèåˆå±¤
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(sum(backbone_dims), feature_dim, 1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU()
        )
        
        # å…¨å±€å¹³å‡æ± åŒ–ä¾†ç²å¾—å½±åƒç´šåˆ¥çš„åµŒå…¥
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        print(f"âœ… MultiLayerGPSImageEncoder initialized:")
        print(f"  Backbone dims: {backbone_dims}")
        print(f"  Feature dim: {feature_dim}")
        print(f"  GPSèåˆéšæ®µæ•¸: {len(backbone_dims)}")
        
    def forward(self, images: torch.Tensor, gps: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: Input images, shape (batch_size, 3, H, W)
            gps: GPS coordinates, shape (batch_size, 2)
        Returns:
            Dictionary containing:
                - 'features': Multi-scale feature maps with GPS fusion
                - 'embeddings': Global image embeddings
        """
        # æå–å¤šå°ºåº¦ç‰¹å¾µ
        outputs = self.feature_extractor(images, output_hidden_states=True)
        hidden_states = outputs.hidden_states
        
        # èª¿æ•´ç‰¹å¾µåœ–å°ºå¯¸ä¸¦é€²è¡ŒGPSèåˆ
        enhanced_features = []
        
        # ç²å–ç›®æ¨™å°ºå¯¸ï¼ˆä½¿ç”¨æœ€å¤§çš„ç‰¹å¾µåœ–å°ºå¯¸ï¼‰
        target_h = target_w = None
        for feature_map in hidden_states:
            if len(feature_map.shape) == 4:  # (B, C, H, W)
                if target_h is None:
                    target_h, target_w = feature_map.shape[-2:]
                else:
                    h, w = feature_map.shape[-2:]
                    if h * w > target_h * target_w:
                        target_h, target_w = h, w
            elif len(feature_map.shape) == 3:  # (B, HW, C) 
                B, HW, C = feature_map.shape
                H = W = int(math.sqrt(HW))
                if target_h is None:
                    target_h, target_w = H, W
                else:
                    if H * W > target_h * target_w:
                        target_h, target_w = H, W
        
        target_size = (target_h, target_w)
        
        for stage_idx, feature_map in enumerate(hidden_states):
            # è™•ç†ç‰¹å¾µåœ–æ ¼å¼
            if len(feature_map.shape) == 3:  # (B, HW, C) æ ¼å¼
                B, HW, C = feature_map.shape
                H = W = int(math.sqrt(HW))
                feature_map = feature_map.transpose(1, 2).reshape(B, C, H, W)
            elif len(feature_map.shape) == 4:  # (B, C, H, W) æ ¼å¼
                pass  # å·²ç¶“æ˜¯æ­£ç¢ºæ ¼å¼
            else:
                continue
            
            # èª¿æ•´ç©ºé–“å°ºå¯¸åˆ°ç›®æ¨™å¤§å°
            if feature_map.shape[-2:] != target_size:
                feature_map = F.interpolate(
                    feature_map, size=target_size, mode='bilinear', align_corners=False
                )
            
            # ğŸŒ ç¬¬stage_idxéšæ®µçš„GPSç·¨ç¢¼
            gps_embedding = self.stage_gps_encoders[stage_idx](gps)  # (B, stage_dim)
            
            # ğŸ”„ GPS-Image èåˆ
            B, C, H, W = feature_map.shape
            
            # å°‡åœ–åƒç‰¹å¾µé‡å¡‘ç‚ºåºåˆ—æ ¼å¼
            img_seq = feature_map.permute(0, 2, 3, 1).reshape(B, H*W, C)  # (B, HW, C)
            gps_seq = gps_embedding.unsqueeze(1)  # (B, 1, C)
            
            # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼šåœ–åƒç‰¹å¾µä½œç‚ºqueryï¼ŒGPSç‰¹å¾µä½œç‚ºkeyå’Œvalue
            enhanced_seq, attention_weights = self.fusion_modules[stage_idx](
                img_seq,      # query: åœ–åƒç‰¹å¾µåºåˆ—
                gps_seq,      # key: GPSç‰¹å¾µ
                gps_seq       # value: GPSç‰¹å¾µ
            )
            
            # é‡å¡‘å›ç‰¹å¾µåœ–æ ¼å¼
            enhanced_feature = enhanced_seq.reshape(B, H, W, C).permute(0, 3, 1, 2)
            
            # æ®˜å·®é€£æ¥ï¼šä¿ç•™åŸå§‹åœ–åƒä¿¡æ¯
            enhanced_feature = enhanced_feature + feature_map
            
            enhanced_features.append(enhanced_feature)
        
        if not enhanced_features:
            raise ValueError("No valid features extracted from Segformer")
        
        # é€£æ¥æ‰€æœ‰GPSå¢å¼·çš„ç‰¹å¾µ
        fused_features = torch.cat(enhanced_features, dim=1)
        
        # ç‰¹å¾µèåˆ
        processed_features = self.feature_fusion(fused_features)
        
        # å…¨å±€å½±åƒåµŒå…¥
        global_embeddings = self.global_pool(processed_features).flatten(1)
        
        return {
            'features': processed_features,
            'embeddings': global_embeddings,
            'stage_features': enhanced_features  # è¿”å›å„éšæ®µå¢å¼·ç‰¹å¾µ
        }


class LocationMemoryBank(nn.Module):
    """
    ğŸ§  ä½ç½®è¨˜æ†¶åº« - ç‚ºæ¯å€‹GPSä½ç½®å»ºç«‹ç‰¹å¾µè¨˜æ†¶
    âœ¨ ä¿®å¾©è¨­å‚™ä¸åŒ¹é…å•é¡Œï¼ŒCPUå„²å­˜GPUè¨ˆç®—
    """
    def __init__(
        self, 
        feature_dim: int = 512,
        memory_size: int = 20,
        spatial_radius: float = 0.00005,
        save_path: Optional[str] = None
    ):
        super().__init__()
        self.feature_dim = feature_dim
        self.memory_size = memory_size
        self.spatial_radius = spatial_radius
        self.save_path = save_path
        
        # å‹•æ…‹è¨˜æ†¶åº«ï¼šGPSä½ç½® -> ç‰¹å¾µå’Œå…¶ä»–ä¿¡æ¯
        self.memory_bank = defaultdict(lambda: {
            'features': [],
            'count': 0,
            'last_updated': 0
        })
        
        # çµ±è¨ˆä¿¡æ¯
        self.total_updates = 0
        self.total_queries = 0
        self.hit_count = 0
        
        # èª¿è©¦è¨ˆæ•¸å™¨
        self.debug_info = {
            'last_locations': 0,
            'last_memories': 0,
            'last_hit_rate': 0.0
        }
        
        print(f"âœ… LocationMemoryBank initialized:")
        print(f"  Feature dim: {feature_dim}")
        print(f"  Memory size per location: {memory_size}")
        print(f"  Spatial radius: {spatial_radius}")
        
    def gps_to_key(self, gps: torch.Tensor) -> str:
        """å°‡GPSåº§æ¨™è½‰æ›ç‚ºè¨˜æ†¶åº«çš„éµ"""
        # é‡åŒ–GPSåº§æ¨™åˆ°å›ºå®šç¶²æ ¼
        lat_grid = round(gps[0].item() / self.spatial_radius) * self.spatial_radius
        lon_grid = round(gps[1].item() / self.spatial_radius) * self.spatial_radius
        return f"{lat_grid:.7f},{lon_grid:.7f}"
    
    def update_memory(self, gps_coords: torch.Tensor, features: torch.Tensor):
        """æ›´æ–°ä½ç½®è¨˜æ†¶åº«"""
        batch_size = gps_coords.shape[0]
        
        for i in range(batch_size):
            gps_key = self.gps_to_key(gps_coords[i])
            
            # æª¢æŸ¥ç‰¹å¾µæœ‰æ•ˆæ€§
            feature_norm = torch.norm(features[i]).item()
            if feature_norm < 1e-6:
                continue  # è·³éç„¡æ•ˆç‰¹å¾µ
            
            # ğŸ†• ç¢ºä¿ç‰¹å¾µåœ¨CPUä¸Šå„²å­˜ï¼ˆç¯€çœGPUè¨˜æ†¶é«”ï¼‰
            cpu_feature = features[i].detach().cpu().clone()
            self.memory_bank[gps_key]['features'].append(cpu_feature)
            self.memory_bank[gps_key]['count'] += 1
            self.memory_bank[gps_key]['last_updated'] = self.total_updates
            
            # ä¿æŒè¨˜æ†¶åº«å¤§å°
            if len(self.memory_bank[gps_key]['features']) > self.memory_size:
                self.memory_bank[gps_key]['features'].pop(0)
        
        self.total_updates += 1
        
        # æ¯100æ¬¡æ›´æ–°æ‰“å°èª¿è©¦ä¿¡æ¯
        if self.total_updates % 100 == 0:
            current_stats = self.get_memory_stats()
            print(f"ğŸ”„ Memory Update #{self.total_updates}:")
            print(f"  Locations: {current_stats['total_locations']} "
                  f"(+{current_stats['total_locations'] - self.debug_info['last_locations']})")
            print(f"  Memories: {current_stats['total_memories']} "
                  f"(+{current_stats['total_memories'] - self.debug_info['last_memories']})")
            
            # æ›´æ–°èª¿è©¦ä¿¡æ¯
            self.debug_info['last_locations'] = current_stats['total_locations']
            self.debug_info['last_memories'] = current_stats['total_memories']
    
    def retrieve_memory(self, gps_coords: torch.Tensor) -> torch.Tensor:
        """æª¢ç´¢ç›¸é—œä½ç½®çš„æ­·å²ç‰¹å¾µ"""
        batch_size = gps_coords.shape[0]
        memory_features = []
        
        # ğŸ†• ç²å–ç›®æ¨™è¨­å‚™
        target_device = gps_coords.device
        
        # æ›´æ–°æŸ¥è©¢è¨ˆæ•¸
        self.total_queries += batch_size
        
        hits_in_batch = 0
        
        for i in range(batch_size):
            gps_key = self.gps_to_key(gps_coords[i])
            retrieved_features = []
            
            # 1. ç²¾ç¢ºåŒ¹é…
            if gps_key in self.memory_bank and len(self.memory_bank[gps_key]['features']) > 0:
                retrieved_features.extend(self.memory_bank[gps_key]['features'])
                self.hit_count += 1
                hits_in_batch += 1
            
            # 2. é„°è¿‘ä½ç½®åŒ¹é…
            if len(retrieved_features) < 5:  # å¦‚æœç²¾ç¢ºåŒ¹é…çš„ç‰¹å¾µä¸å¤ 
                for key, memory in self.memory_bank.items():
                    if key != gps_key and len(memory['features']) > 0:
                        stored_lat, stored_lon = map(float, key.split(','))
                        current_lat, current_lon = gps_coords[i][0].item(), gps_coords[i][1].item()
                        distance = ((current_lat - stored_lat)**2 + (current_lon - stored_lon)**2)**0.5
                        
                        if distance < self.spatial_radius * 3:  # æ“´å¤§æœç´¢ç¯„åœ
                            retrieved_features.extend(memory['features'][-2:])  # åªå–æœ€è¿‘çš„2å€‹
                            if len(retrieved_features) >= 10:  # é™åˆ¶ç¸½æ•¸
                                break
            
            # èšåˆæª¢ç´¢åˆ°çš„ç‰¹å¾µ
            if retrieved_features:
                # å–æœ€è¿‘çš„ç‰¹å¾µä¸¦åšåŠ æ¬Šå¹³å‡
                recent_features = retrieved_features[-8:]  # æœ€å¤š8å€‹ç‰¹å¾µ
                if len(recent_features) == 1:
                    aggregated = recent_features[0]
                else:
                    # åŠ æ¬Šå¹³å‡ï¼šè¶Šæ–°çš„ç‰¹å¾µæ¬Šé‡è¶Šå¤§
                    weights = torch.softmax(
                        torch.tensor([i for i in range(len(recent_features))], dtype=torch.float32),
                        dim=0
                    )
                    
                    stacked_features = torch.stack(recent_features)
                    aggregated = (stacked_features * weights.unsqueeze(-1)).sum(dim=0)
                
                # ğŸ†• ç¢ºä¿è¿”å›çš„ç‰¹å¾µåœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                aggregated = aggregated.to(target_device)
            else:
                # æ²’æœ‰æ­·å²è¨˜éŒ„ï¼Œä½¿ç”¨é›¶å‘é‡
                aggregated = torch.zeros(self.feature_dim, device=target_device)
            
            memory_features.append(aggregated)
        
        # æ¯200æ¬¡æŸ¥è©¢æ‰“å°çµ±è¨ˆä¿¡æ¯
        if self.total_queries % 200 == 0 and self.total_queries > 0:
            current_hit_rate = self.hit_count / self.total_queries
            print(f"ğŸ” Memory Query #{self.total_queries}:")
            print(f"  Total hits: {self.hit_count}")
            print(f"  Hit rate: {current_hit_rate:.4f} "
                  f"(Î”{current_hit_rate - self.debug_info['last_hit_rate']:+.4f})")
            print(f"  Hits in batch: {hits_in_batch}/{batch_size}")
            
            self.debug_info['last_hit_rate'] = current_hit_rate
        
        return torch.stack(memory_features)
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶åº«çµ±è¨ˆä¿¡æ¯"""
        total_locations = len(self.memory_bank)
        total_memories = sum(len(memory['features']) for memory in self.memory_bank.values())
        
        # ä½¿ç”¨ total_queries è¨ˆç®—å‘½ä¸­ç‡
        hit_rate = self.hit_count / max(self.total_queries, 1)
        
        return {
            'total_locations': total_locations,
            'total_memories': total_memories,
            'hit_rate': hit_rate,
            'avg_memories_per_location': total_memories / max(total_locations, 1),
            'total_queries': self.total_queries,
            'total_updates': self.total_updates,
            'hit_count': self.hit_count
        }
    
    def save_memory_bank(self):
        """ğŸ†• ä¿å­˜å®Œæ•´è¨˜æ†¶åº«åˆ°æ–‡ä»¶"""
        if self.save_path:
            # æº–å‚™ä¿å­˜æ•¸æ“š
            memory_data = {
                'spatial_radius': self.spatial_radius,
                'feature_dim': self.feature_dim,
                'memory_size': self.memory_size,
                'total_updates': self.total_updates,
                'total_queries': self.total_queries,
                'hit_count': self.hit_count,
                'memory_bank': {}
            }
            
            # ä¿å­˜è¨˜æ†¶åº«å…§å®¹
            for gps_key, memory_info in self.memory_bank.items():
                if len(memory_info['features']) > 0:
                    # å°‡ç‰¹å¾µè½‰æ›ç‚ºCPUä¸¦å †ç–Š
                    features_tensor = torch.stack([f.cpu() for f in memory_info['features']])
                    memory_data['memory_bank'][gps_key] = {
                        'features': features_tensor,  # é€™æœƒæ˜¯ä¸€å€‹ (num_features, feature_dim) çš„tensor
                        'count': memory_info['count'],
                        'last_updated': memory_info['last_updated']
                    }
            
            # ä¿å­˜åˆ°.pthæ–‡ä»¶
            memory_file = self.save_path.replace('.json', '.pth')
            torch.save(memory_data, memory_file)
            
            # ä¹Ÿä¿å­˜çµ±è¨ˆä¿¡æ¯åˆ°JSON (å‘å¾Œå…¼å®¹)
            stats = {
                'locations': list(self.memory_bank.keys()),
                'counts': {k: v['count'] for k, v in self.memory_bank.items()},
                'stats': self.get_memory_stats()
            }
            
            os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
            with open(self.save_path, 'w') as f:
                json.dump(stats, f, indent=2)
            
            print(f"ğŸ’¾ Memory bank saved to {memory_file}")
            print(f"ğŸ“Š Memory stats saved to {self.save_path}")
            print(f"ğŸ“ˆ Saved {len(memory_data['memory_bank'])} GPS locations with features")
    
    def load_memory_bank(self, memory_path: str):
        """ğŸ†• å¾æ–‡ä»¶è¼‰å…¥è¨˜æ†¶åº«"""
        try:
            # å˜—è©¦è¼‰å…¥.pthæ ¼å¼çš„å®Œæ•´è¨˜æ†¶åº«
            memory_file = memory_path.replace('.json', '.pth')
            
            if os.path.exists(memory_file):
                print(f"ğŸ“‚ Loading memory bank from {memory_file}")
                memory_data = torch.load(memory_file, map_location='cpu')  # ğŸ†• å¼·åˆ¶è¼‰å…¥åˆ°CPU
                
                # æª¢æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
                if 'feature_dim' in memory_data and memory_data['feature_dim'] != self.feature_dim:
                    print(f"âš ï¸  Feature dimension mismatch: saved={memory_data['feature_dim']}, current={self.feature_dim}")
                    print("ğŸ”„ Will attempt to resize features...")
                
                # è¼‰å…¥çµ±è¨ˆä¿¡æ¯
                self.spatial_radius = memory_data.get('spatial_radius', self.spatial_radius)
                self.total_updates = memory_data.get('total_updates', 0)
                self.total_queries = memory_data.get('total_queries', 0)
                self.hit_count = memory_data.get('hit_count', 0)
                
                # è¼‰å…¥è¨˜æ†¶åº«å…§å®¹
                self.memory_bank = defaultdict(lambda: {'features': [], 'count': 0, 'last_updated': 0})
                
                for gps_key, memory_info in memory_data['memory_bank'].items():
                    features_tensor = memory_info['features']  # (num_features, feature_dim)
                    
                    # ğŸ†• ç¢ºä¿æ‰€æœ‰ç‰¹å¾µéƒ½åœ¨CPUä¸Š
                    if features_tensor.device != torch.device('cpu'):
                        features_tensor = features_tensor.cpu()
                    
                    # è™•ç†ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…çš„æƒ…æ³
                    if features_tensor.shape[1] != self.feature_dim:
                        if features_tensor.shape[1] < self.feature_dim:
                            # Padding
                            padding = torch.zeros(features_tensor.shape[0], 
                                                self.feature_dim - features_tensor.shape[1])
                            features_tensor = torch.cat([features_tensor, padding], dim=1)
                        else:
                            # Truncate
                            features_tensor = features_tensor[:, :self.feature_dim]
                    
                    # å°‡ç‰¹å¾µè½‰æ›ç‚ºåˆ—è¡¨å½¢å¼
                    feature_list = [features_tensor[i] for i in range(features_tensor.shape[0])]
                    
                    self.memory_bank[gps_key] = {
                        'features': feature_list,
                        'count': memory_info['count'],
                        'last_updated': memory_info['last_updated']
                    }
                
                loaded_locations = len(self.memory_bank)
                total_features = sum(len(memory['features']) for memory in self.memory_bank.values())
                
                print(f"âœ… Memory bank loaded successfully!")
                print(f"ğŸ“ Loaded {loaded_locations} GPS locations")
                print(f"ğŸ§  Loaded {total_features} feature memories")
                print(f"ğŸ“Š Historical stats: queries={self.total_queries}, hits={self.hit_count}")
                
                return True
            
            else:
                print(f"âŒ Memory bank file not found: {memory_file}")
                if os.path.exists(memory_path):
                    print(f"ğŸ“‹ Found stats file {memory_path}, but no feature data")
                    print("ğŸ’¡ Testing will start with empty memory bank")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to load memory bank: {e}")
            print("ğŸ’¡ Testing will start with empty memory bank")
            return False


class CrossModalFusion(nn.Module):
    """
    è·¨æ¨¡æ…‹èåˆæ¨¡çµ„ï¼Œå°‡å½±åƒç‰¹å¾µå’Œ GPS ç‰¹å¾µèåˆ
    """
    def __init__(
        self, 
        feature_dim: int = 512,
        fusion_method: str = "attention"
    ):
        super().__init__()
        self.fusion_method = fusion_method
        self.feature_dim = feature_dim
        
        if fusion_method == "attention":
            # æ³¨æ„åŠ›æ©Ÿåˆ¶èåˆ
            self.query_proj = nn.Linear(feature_dim, feature_dim)
            self.key_proj = nn.Linear(feature_dim, feature_dim)
            self.value_proj = nn.Linear(feature_dim, feature_dim)
            self.scale = math.sqrt(feature_dim)
            
        elif fusion_method == "concat":
            # é€£æ¥å¾Œé™ç¶­
            self.fusion_proj = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        
    def forward(
        self, 
        image_features: torch.Tensor, 
        location_embeddings: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            image_features: Image feature maps, shape (batch_size, feature_dim, H, W)
            location_embeddings: GPS embeddings, shape (batch_size, feature_dim)
        Returns:
            Fused features, shape (batch_size, feature_dim, H, W)
        """
        batch_size, feature_dim, H, W = image_features.shape
        
        if self.fusion_method == "add":
            # ç°¡å–®ç›¸åŠ èåˆ
            location_map = location_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)
            fused_features = image_features + location_map
            
        elif self.fusion_method == "concat":
            # é€£æ¥èåˆ
            location_map = location_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)
            concat_features = torch.cat([image_features, location_map], dim=1)
            concat_features = concat_features.permute(0, 2, 3, 1).reshape(batch_size, H*W, -1)
            fused_features = self.fusion_proj(concat_features)
            fused_features = fused_features.reshape(batch_size, H, W, feature_dim).permute(0, 3, 1, 2)
            
        elif self.fusion_method == "attention":
            # æ³¨æ„åŠ›èåˆ
            img_seq = image_features.permute(0, 2, 3, 1).reshape(batch_size, H*W, feature_dim)
            
            Q = self.query_proj(img_seq)
            K = self.key_proj(location_embeddings.unsqueeze(1))
            V = self.value_proj(location_embeddings.unsqueeze(1))
            
            attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
            attention_weights = F.softmax(attention_weights, dim=-1)
            
            attended_location = torch.matmul(attention_weights, V)
            fused_seq = img_seq + attended_location
            fused_features = fused_seq.reshape(batch_size, H, W, feature_dim).permute(0, 3, 1, 2)
        
        return fused_features


class MemoryEnhancedGeoSegformer(nn.Module):
    """
    ğŸŒ å¤šå±¤GPSèåˆ + è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer
    âœ¨ æ”¯æŒè¨˜æ†¶åº«æ¶ˆèå¯¦é©—ï¼ˆmemory_size=0æ™‚ç¦ç”¨è¨˜æ†¶åº«ï¼‰
    """
    def __init__(
        self,
        num_classes: int,
        segformer_model: str = "nvidia/mit-b0",
        feature_dim: int = 512,
        rff_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],
        fusion_method: str = "attention",
        dropout: float = 0.1,
        memory_size: int = 20,  # ğŸ†• è¨­ç‚º0ç¦ç”¨è¨˜æ†¶åº«
        spatial_radius: float = 0.00005,
        memory_save_path: Optional[str] = None
    ):
        super().__init__()
        
        self.num_classes = num_classes
        self.feature_dim = feature_dim
        self.memory_enabled = memory_size > 0  # ğŸ†• è¨˜æ†¶åº«å•Ÿç”¨æ¨™èªŒ
        
        print(f"ğŸš€ Initializing MultiLayer GPS + Memory Enhanced GeoSegformer")
        print(f"  Num classes: {num_classes}")
        print(f"  Feature dim: {feature_dim}")
        print(f"  Memory enabled: {self.memory_enabled}")
        if self.memory_enabled:
            print(f"  Memory size: {memory_size}")
            print(f"  Spatial radius: {spatial_radius}")
        
        # GPS ä½ç½®ç·¨ç¢¼å™¨ï¼ˆç¸½æ˜¯éœ€è¦ï¼‰
        self.location_encoder = LocationEncoder(
            rff_dim=rff_dim,
            output_dim=feature_dim,
            sigmas=sigmas,
            dropout=dropout
        )
        
        # ğŸŒ å¤šå±¤GPSèåˆçš„å½±åƒç·¨ç¢¼å™¨ï¼ˆç¸½æ˜¯éœ€è¦ï¼‰
        self.image_encoder = MultiLayerGPSImageEncoder(
            segformer_model=segformer_model,
            feature_dim=feature_dim
        )
        
        # ğŸ†• æ¢ä»¶æ€§è¨˜æ†¶åº«
        if self.memory_enabled:
            self.memory_bank = LocationMemoryBank(
                feature_dim=feature_dim,
                memory_size=memory_size,
                spatial_radius=spatial_radius,
                save_path=memory_save_path
            )
            
            # è¨˜æ†¶èåˆå±¤
            self.memory_fusion = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(feature_dim, feature_dim)
            )
            
            # è¨˜æ†¶æ³¨æ„åŠ›æ©Ÿåˆ¶
            self.memory_attention = nn.MultiheadAttention(
                embed_dim=feature_dim,
                num_heads=8,
                dropout=0.1,
                batch_first=True
            )
            print(f"âœ… Memory bank components initialized")
        else:
            self.memory_bank = None
            self.memory_fusion = None  # ğŸ”§ ç¢ºä¿è¨­ç‚ºNone
            self.memory_attention = None  # ğŸ”§ ç¢ºä¿è¨­ç‚ºNone
            print(f"ğŸš« Memory bank disabled")
        
        # è·¨æ¨¡æ…‹èåˆï¼ˆç¸½æ˜¯éœ€è¦ï¼‰
        self.cross_modal_fusion = CrossModalFusion(
            feature_dim=feature_dim,
            fusion_method=fusion_method
        )
        
        # åˆ†å‰²é ­ï¼ˆç¸½æ˜¯éœ€è¦ï¼‰
        self.segmentation_head = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(),
            nn.Dropout2d(dropout),
            nn.Conv2d(feature_dim, num_classes, 1)
        )
        
        # ç”¨æ–¼å°æ¯”å­¸ç¿’çš„æŠ•å½±é ­ï¼ˆç¸½æ˜¯éœ€è¦ï¼‰
        self.contrastive_proj = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, 256)
        )
        
        total_params = sum(p.numel() for p in self.parameters()) / 1e6
        print(f"âœ… MultiLayer GPS + Memory Enhanced GeoSegformer initialized with {total_params:.2f}M parameters")
    
    def forward(
        self, 
        images: torch.Tensor, 
        gps: torch.Tensor,
        return_embeddings: bool = False,
        update_memory: bool = True
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: Input images, shape (batch_size, 3, H, W)
            gps: GPS coordinates, shape (batch_size, 2)
            return_embeddings: Whether to return embeddings for contrastive learning
            update_memory: Whether to update memory bank
        Returns:
            Dictionary containing segmentation results and optional embeddings
        """
        # GPS ä½ç½®ç·¨ç¢¼
        location_embeddings = self.location_encoder(gps)
        
        # ğŸŒ å¤šå±¤GPSèåˆçš„å½±åƒç‰¹å¾µæå–
        image_outputs = self.image_encoder(images, gps)
        image_features = image_outputs['features']
        image_embeddings = image_outputs['embeddings']
        
        # ğŸ†• æ¢ä»¶æ€§è¨˜æ†¶å¢å¼·è™•ç†
        enhanced_embeddings = image_embeddings
        memory_weight = 0.0
        
        if self.memory_enabled and self.memory_bank is not None:
            # æª¢ç´¢ä½ç½®è¨˜æ†¶
            memory_features = self.memory_bank.retrieve_memory(gps)
            
            # è¨ˆç®—è¨˜æ†¶ç‰¹å¾µçš„æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨L2ç¯„æ•¸ï¼‰
            memory_norms = torch.norm(memory_features, dim=-1)
            valid_memory_mask = memory_norms > 1e-6  # éé›¶ç‰¹å¾µåˆ¤æ–·
            
            if valid_memory_mask.any():
                memory_weight = valid_memory_mask.float().mean().item()
                
                if memory_weight > 0:
                    # åªå°æœ‰æ•ˆè¨˜æ†¶é€²è¡Œè™•ç†
                    valid_indices = valid_memory_mask.nonzero(as_tuple=True)[0]
                    
                    if len(valid_indices) > 0:
                        valid_memory_features = memory_features[valid_indices]
                        valid_image_embeddings = image_embeddings[valid_indices]
                        
                        # ğŸ†• ç¢ºä¿è¨­å‚™ä¸€è‡´æ€§ - æª¢æŸ¥å’Œä¿®å¾©è¨­å‚™ä¸åŒ¹é…
                        if valid_memory_features.device != valid_image_embeddings.device:
                            print(f"âš ï¸  Device mismatch detected: memory={valid_memory_features.device}, image={valid_image_embeddings.device}")
                            valid_memory_features = valid_memory_features.to(valid_image_embeddings.device)
                        
                        # æ–¹æ³•1ï¼šç‰¹å¾µèåˆ
                        combined_features = torch.cat([valid_image_embeddings, valid_memory_features], dim=-1)
                        fused_features = self.memory_fusion(combined_features)
                        
                        # æ–¹æ³•2ï¼šæ³¨æ„åŠ›èåˆ
                        memory_enhanced, attention_weights = self.memory_attention(
                            valid_image_embeddings.unsqueeze(1),  # query: (valid_batch, 1, feature_dim)
                            valid_memory_features.unsqueeze(1),   # key: (valid_batch, 1, feature_dim)
                            valid_memory_features.unsqueeze(1)    # value: (valid_batch, 1, feature_dim)
                        )
                        
                        # çµåˆå…©ç¨®èåˆæ–¹å¼
                        enhanced_part = (
                            0.6 * fused_features + 
                            0.4 * memory_enhanced.squeeze(1)
                        )
                        
                        # æ®˜å·®é€£æ¥
                        enhanced_part = enhanced_part + valid_image_embeddings
                        
                        # æ›´æ–°å°æ‡‰çš„åµŒå…¥
                        enhanced_embeddings = image_embeddings.clone()
                        enhanced_embeddings[valid_indices] = enhanced_part
            
            # æ›´æ–°è¨˜æ†¶åº«ï¼ˆåƒ…åœ¨è¨“ç·´æ™‚ï¼‰
            if update_memory and self.training:
                self.memory_bank.update_memory(gps, image_embeddings)
        
        # å°‡å¢å¼·çš„ç‰¹å¾µæŠ•å½±å›ç©ºé–“ç¶­åº¦ä»¥é€²è¡Œè·¨æ¨¡æ…‹èåˆ
        enhanced_location_embeddings = location_embeddings + 0.3 * enhanced_embeddings
        
        # è·¨æ¨¡æ…‹ç‰¹å¾µèåˆ
        fused_features = self.cross_modal_fusion(image_features, enhanced_location_embeddings)
        
        # èªç¾©åˆ†å‰²é æ¸¬
        segmentation_logits = self.segmentation_head(fused_features)
        
        # èª¿æ•´åˆ°è¼¸å…¥å½±åƒå°ºå¯¸
        segmentation_logits = F.interpolate(
            segmentation_logits, 
            size=images.shape[-2:], 
            mode='bilinear', 
            align_corners=False
        )
        
        outputs = {
            'segmentation_logits': segmentation_logits,
            'fused_features': fused_features,
            'memory_weight': memory_weight  # è¨˜æ†¶ç‰¹å¾µçš„æœ‰æ•ˆæ¬Šé‡
        }
        
        # å¦‚æœéœ€è¦è¿”å›åµŒå…¥ç”¨æ–¼å°æ¯”å­¸ç¿’
        if return_embeddings:
            image_proj = self.contrastive_proj(enhanced_embeddings)
            location_proj = self.contrastive_proj(enhanced_location_embeddings)
            outputs.update({
                'image_embeddings': image_proj,
                'location_embeddings': location_proj
            })
        
        return outputs
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ç²å–è¨˜æ†¶åº«çµ±è¨ˆä¿¡æ¯"""
        if self.memory_enabled and self.memory_bank is not None:
            return self.memory_bank.get_memory_stats()
        else:
            return {
                'total_locations': 0,
                'total_memories': 0,
                'hit_rate': 0.0,
                'avg_memories_per_location': 0.0,
                'total_queries': 0,
                'total_updates': 0,
                'hit_count': 0,
                'memory_enabled': False
            }
    
    def save_memory_bank(self):
        """ä¿å­˜è¨˜æ†¶åº«"""
        if self.memory_enabled and self.memory_bank is not None:
            self.memory_bank.save_memory_bank()
        else:
            print("ğŸš« Memory bank is disabled, nothing to save")
    
    def load_memory_bank(self, memory_path: str):
        """ğŸ†• è¼‰å…¥è¨˜æ†¶åº«"""
        if self.memory_enabled and self.memory_bank is not None:
            return self.memory_bank.load_memory_bank(memory_path)
        else:
            print("ğŸš« Memory bank is disabled, cannot load")
            return False


# å·¥å» å‡½æ•¸
def create_memory_enhanced_geo_segformer(
    num_classes: int,
    model_size: str = "b0",
    feature_dim: int = 512,
    fusion_method: str = "attention",
    memory_size: int = 20,  # ğŸ†• è¨­ç‚º0ç¦ç”¨è¨˜æ†¶åº«
    spatial_radius: float = 0.00005,
    memory_save_path: Optional[str] = None
) -> MemoryEnhancedGeoSegformer:
    """
    å‰µå»ºå¤šå±¤GPSèåˆ + å¯é¸è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer æ¨¡å‹çš„å·¥å» å‡½æ•¸
    
    Args:
        memory_size: è¨˜æ†¶åº«å¤§å°ï¼Œè¨­ç‚º0æ™‚ç¦ç”¨è¨˜æ†¶åº«
    """
    segformer_model = f"nvidia/mit-{model_size}"
    
    # ğŸ†• æ ¹æ“šmemory_sizeæ±ºå®šæ˜¯å¦å•Ÿç”¨è¨˜æ†¶åº«
    if memory_size == 0:
        print(f"ğŸš« Creating GPS-only model (memory disabled)")
        memory_save_path = None  # ä¸ä¿å­˜è¨˜æ†¶åº«
    else:
        print(f"ğŸ§  Creating full model with memory bank (size={memory_size})")
    
    return MemoryEnhancedGeoSegformer(
        num_classes=num_classes,
        segformer_model=segformer_model,
        feature_dim=feature_dim,
        fusion_method=fusion_method,
        memory_size=memory_size,  # ğŸ†• æœƒè‡ªå‹•è™•ç†0çš„æƒ…æ³
        spatial_radius=spatial_radius,
        memory_save_path=memory_save_path
    )


# èª¿è©¦å’Œåˆ†æå‡½æ•¸
def analyze_gps_quantization(gps_csv_path: str, spatial_radius: float = 0.00005):
    """åˆ†æGPSé‡åŒ–æ•ˆæœï¼Œå¹«åŠ©èª¿æ•´spatial_radius"""
    import pandas as pd
    
    print(f"\nğŸ“Š GPSé‡åŒ–åˆ†æ (spatial_radius={spatial_radius}):")
    
    # è®€å–GPSæ•¸æ“š
    gps_data = pd.read_csv(gps_csv_path)
    print(f"  ç¸½GPSè¨˜éŒ„æ•¸: {len(gps_data)}")
    
    # è¨ˆç®—åŸå§‹å”¯ä¸€ä½ç½®
    original_coords = set()
    for _, row in gps_data.iterrows():
        lat, lon = row['lat'], row['long']
        original_coords.add(f"{lat:.7f},{lon:.7f}")
    
    # æ¨¡æ“¬é‡åŒ–éç¨‹
    def gps_to_key(lat, lon, radius):
        lat_grid = round(lat / radius) * radius
        lon_grid = round(lon / radius) * radius
        return f"{lat_grid:.7f},{lon_grid:.7f}"
    
    quantized_keys = set()
    for _, row in gps_data.iterrows():
        lat, lon = row['lat'], row['long']
        quantized_keys.add(gps_to_key(lat, lon, spatial_radius))
    
    # åˆ†æçµæœ
    original_unique = len(original_coords)
    quantized_unique = len(quantized_keys)
    compression_rate = quantized_unique / original_unique
    
    print(f"  åŸå§‹å”¯ä¸€ä½ç½®æ•¸: {original_unique}")
    print(f"  é‡åŒ–å¾Œå”¯ä¸€ä½ç½®æ•¸: {quantized_unique}")
    print(f"  ä½ç½®ä¿ç•™ç‡: {compression_rate*100:.1f}%")
    
    # å»ºè­°
    if compression_rate < 0.3:
        suggested_radius = spatial_radius * 0.1
        print(f"âš ï¸  ä½ç½®ä¿ç•™ç‡å¤ªä½ï¼å»ºè­°å°‡spatial_radiusç¸®å°åˆ°: {suggested_radius:.7f}")
    elif compression_rate > 0.9:
        suggested_radius = spatial_radius * 2
        print(f"ğŸ’¡ ä½ç½®å¹¾ä¹æ²’æœ‰èšåˆï¼Œå¯è€ƒæ…®å°‡spatial_radiuså¢å¤§åˆ°: {suggested_radius:.7f}")
    else:
        print(f"âœ… spatial_radiusè¨­ç½®åˆç†")
    
    return original_unique, quantized_unique, compression_rate


def debug_memory_system(train_gps_csv: str, spatial_radius: float = 0.00005):
    """å®Œæ•´çš„è¨˜æ†¶ç³»çµ±èª¿è©¦"""
    print("ğŸ”§ è¨˜æ†¶ç³»çµ±èª¿è©¦åˆ†æ:")
    print("=" * 50)
    
    # 1. GPSé‡åŒ–åˆ†æ
    analyze_gps_quantization(train_gps_csv, spatial_radius)
    
    # 2. GPSæ•¸æ“šçµ±è¨ˆ
    import pandas as pd
    gps_data = pd.read_csv(train_gps_csv)
    
    lats = gps_data['lat'].values
    lons = gps_data['long'].values
    
    print(f"\nğŸ“ˆ GPSæ•¸æ“šçµ±è¨ˆ:")
    print(f"  ç·¯åº¦ç¯„åœ: [{lats.min():.6f}, {lats.max():.6f}] (è·¨åº¦: {lats.max()-lats.min():.6f})")
    print(f"  ç¶“åº¦ç¯„åœ: [{lons.min():.6f}, {lons.max():.6f}] (è·¨åº¦: {lons.max()-lons.min():.6f})")
    print(f"  ç·¯åº¦æ¨™æº–å·®: {lats.std():.6f}")
    print(f"  ç¶“åº¦æ¨™æº–å·®: {lons.std():.6f}")
    
    # 3. é‡è¤‡ç‡åˆ†æ
    unique_coords = set((lat, lon) for lat, lon in zip(lats, lons))
    duplicate_rate = (len(gps_data) - len(unique_coords)) / len(gps_data) * 100
    print(f"  é‡è¤‡åº§æ¨™ç‡: {duplicate_rate:.2f}%")
    
    # 4. è·é›¢åˆ†æ
    import numpy as np
    
    # éš¨æ©Ÿæ¡æ¨£è¨ˆç®—å¹³å‡è·é›¢
    if len(gps_data) > 1000:
        sample_indices = np.random.choice(len(gps_data), 1000, replace=False)
        sample_coords = [(lats[i], lons[i]) for i in sample_indices]
    else:
        sample_coords = [(lat, lon) for lat, lon in zip(lats, lons)]
    
    distances = []
    for i in range(len(sample_coords)):
        for j in range(i+1, min(i+10, len(sample_coords))):  # åªè¨ˆç®—å‰10å€‹é„°å±…
            lat1, lon1 = sample_coords[i]
            lat2, lon2 = sample_coords[j]
            dist = ((lat1-lat2)**2 + (lon1-lon2)**2)**0.5
            distances.append(dist)
    
    if distances:
        distances = np.array(distances)
        print(f"  GPSé»é–“è·é›¢çµ±è¨ˆ:")
        print(f"    å¹³å‡è·é›¢: {distances.mean():.6f}")
        print(f"    æœ€å°è·é›¢: {distances.min():.6f}")
        print(f"    ä¸­ä½æ•¸è·é›¢: {np.median(distances):.6f}")
        print(f"    90%åˆ†ä½æ•¸: {np.percentile(distances, 90):.6f}")
        
        # èˆ‡spatial_radiusæ¯”è¼ƒ
        print(f"  èˆ‡spatial_radius ({spatial_radius:.6f}) æ¯”è¼ƒ:")
        smaller_than_radius = (distances < spatial_radius).sum()
        print(f"    å°æ–¼radiusçš„è·é›¢å°æ•¸: {smaller_than_radius}/{len(distances)} ({smaller_than_radius/len(distances)*100:.1f}%)")
    
    print("\nğŸ¯ èª¿è©¦å»ºè­°:")
    print("1. å¦‚æœä½ç½®ä¿ç•™ç‡ < 30%ï¼Œç¸®å° spatial_radius")
    print("2. å¦‚æœé‡è¤‡åº§æ¨™ç‡ > 80%ï¼Œè€ƒæ…®å¢åŠ æ•¸æ“šå¤šæ¨£æ€§")
    print("3. å¦‚æœå¹³å‡è·é›¢ >> spatial_radiusï¼Œè€ƒæ…®å¢å¤§ spatial_radius")
    print("4. è§€å¯Ÿè¨“ç·´éç¨‹ä¸­è¨˜æ†¶åº«çµ±è¨ˆçš„è®ŠåŒ–")


if __name__ == "__main__":
    # æ¸¬è©¦æ”¯æŒæ¶ˆèå¯¦é©—çš„æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("=" * 70)
    print("ğŸ§ª æ¸¬è©¦æ”¯æŒæ¶ˆèå¯¦é©—çš„ GeoSegformer")
    print("=" * 70)
    
    # æ¸¬è©¦GPS-onlyæ¨¡å‹ï¼ˆè¨˜æ†¶åº«ç¦ç”¨ï¼‰
    print(f"\nğŸš« æ¸¬è©¦GPS-onlyæ¨¡å‹ï¼ˆè¨˜æ†¶åº«ç¦ç”¨ï¼‰...")
    gps_only_model = create_memory_enhanced_geo_segformer(
        num_classes=25, 
        memory_size=0,  # ğŸ†• è¨­ç‚º0ç¦ç”¨è¨˜æ†¶åº«
        spatial_radius=0.00005,  # é€™å€‹åƒæ•¸æœƒè¢«å¿½ç•¥
        memory_save_path=None
    ).to(device)
    
    # æ¸¬è©¦å®Œæ•´æ¨¡å‹ï¼ˆè¨˜æ†¶åº«å•Ÿç”¨ï¼‰
    print(f"\nğŸ§  æ¸¬è©¦å®Œæ•´æ¨¡å‹ï¼ˆè¨˜æ†¶åº«å•Ÿç”¨ï¼‰...")
    full_model = create_memory_enhanced_geo_segformer(
        num_classes=25, 
        memory_size=15,  # å•Ÿç”¨è¨˜æ†¶åº«
        spatial_radius=0.00005,
        memory_save_path="./test_memory_stats.json"
    ).to(device)
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 4
    images = torch.randn(batch_size, 3, 512, 512).to(device)
    
    # æ¨¡æ“¬ä½ çš„GPSæ•¸æ“šç¯„åœ
    gps = torch.tensor([
        [-0.001057, -0.000368],
        [-0.000738, -0.000405],
        [-0.000545, -0.000406],
        [-0.001057, -0.000368]
    ], dtype=torch.float32).to(device)
    
    print(f"\nğŸ” æ¸¬è©¦é…ç½®:")
    print(f"  Batch size: {batch_size}")
    print(f"  Image shape: {images.shape}")
    print(f"  GPS shape: {gps.shape}")
    
    # æ¸¬è©¦GPS-onlyæ¨¡å‹
    print(f"\nğŸš« æ¸¬è©¦GPS-onlyæ¨¡å‹...")
    gps_only_model.train()
    outputs1 = gps_only_model(images, gps, return_embeddings=True, update_memory=True)
    
    print(f"  åˆ†å‰²è¼¸å‡ºå½¢ç‹€: {outputs1['segmentation_logits'].shape}")
    print(f"  è¨˜æ†¶æ¬Šé‡: {outputs1['memory_weight']:.4f}")
    print(f"  å½±åƒåµŒå…¥å½¢ç‹€: {outputs1['image_embeddings'].shape}")
    print(f"  ä½ç½®åµŒå…¥å½¢ç‹€: {outputs1['location_embeddings'].shape}")
    
    # æª¢æŸ¥è¨˜æ†¶åº«çµ±è¨ˆ
    memory_stats = gps_only_model.get_memory_stats()
    print(f"  è¨˜æ†¶åº«çµ±è¨ˆ: {memory_stats}")
    
    # æ¸¬è©¦å®Œæ•´æ¨¡å‹
    print(f"\nğŸ§  æ¸¬è©¦å®Œæ•´æ¨¡å‹...")
    full_model.train()
    outputs2 = full_model(images, gps, return_embeddings=True, update_memory=True)
    
    print(f"  åˆ†å‰²è¼¸å‡ºå½¢ç‹€: {outputs2['segmentation_logits'].shape}")
    print(f"  è¨˜æ†¶æ¬Šé‡: {outputs2['memory_weight']:.4f}")
    print(f"  å½±åƒåµŒå…¥å½¢ç‹€: {outputs2['image_embeddings'].shape}")
    print(f"  ä½ç½®åµŒå…¥å½¢ç‹€: {outputs2['location_embeddings'].shape}")
    
    # æª¢æŸ¥è¨˜æ†¶åº«çµ±è¨ˆ
    memory_stats = full_model.get_memory_stats()
    print(f"  è¨˜æ†¶åº«çµ±è¨ˆ: {memory_stats}")
    
    print(f"\nğŸ‰ æ¶ˆèå¯¦é©—æ”¯æŒæ¸¬è©¦å®Œæˆï¼")
    print(f"âœ… GPS-onlyæ¨¡å‹ï¼šè¨˜æ†¶åº«å·²ç¦ç”¨")
    print(f"âœ… å®Œæ•´æ¨¡å‹ï¼šè¨˜æ†¶åº«æ­£å¸¸å·¥ä½œ")
    print(f"âœ… å…©å€‹æ¨¡å‹éƒ½å¯ä»¥æ­£å¸¸è¨“ç·´å’Œæ¨ç†")
    print(f"ğŸ“ ç¾åœ¨å¯ä»¥é–‹å§‹æ¶ˆèå¯¦é©—äº†ï¼")