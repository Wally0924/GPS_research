import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Any, Dict, List, Optional
import json
import os
from collections import defaultdict

# YOLOv10ç›¸å…³å¯¼å…¥
try:
    from ultralytics import YOLO
    from ultralytics.nn.modules import C2f, SPPF, Conv, Bottleneck
except ImportError:
    print("è¯·å®‰è£…ultralytics: pip install ultralytics")
    raise


class MultiScaleGPSToRFF(nn.Module):
    """GPS Random Fourier Features ç¼–ç å™¨"""
    def __init__(
        self, 
        rff_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],
        device: str = "cuda"
    ) -> None:
        super().__init__()
        self.rff_dim = rff_dim
        self.sigmas = sigmas
        self.num_scales = len(sigmas)
        
        # æ™ºèƒ½åˆ†é…ç»´åº¦
        base_features_per_scale = rff_dim // (2 * self.num_scales)
        remainder = rff_dim - (base_features_per_scale * 2 * self.num_scales)
        
        self.features_per_scale = []
        for i in range(self.num_scales):
            extra = 1 if i < remainder // 2 else 0
            features_count = base_features_per_scale + extra
            self.features_per_scale.append(features_count)
        
        if remainder % 2 == 1:
            self.features_per_scale[-1] += 1
            
        total_dim = sum(f * 2 for f in self.features_per_scale)
        assert total_dim == rff_dim, f"ç»´åº¦ä¸åŒ¹é…: {total_dim} != {rff_dim}"
        
        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºä¸åŒçš„ RFF å‚æ•°
        for i, (sigma, features_count) in enumerate(zip(sigmas, self.features_per_scale)):
            omega = torch.randn(features_count, 2) / sigma
            b = 2 * math.pi * torch.rand(features_count)
            
            self.register_buffer(f'omega_{i}', omega)
            self.register_buffer(f'b_{i}', b)
        
        print(f"âœ… MultiScaleGPSToRFF initialized: dim={rff_dim}")
    
    def forward(self, gps: torch.Tensor) -> torch.Tensor:
        batch_size = gps.shape[0]
        rff_features = []
        
        for i in range(self.num_scales):
            omega = getattr(self, f'omega_{i}')
            b = getattr(self, f'b_{i}')
            
            proj = torch.matmul(gps, omega.T)
            y = proj + b
            rff = torch.cat([torch.cos(y), torch.sin(y)], dim=-1)
            rff_features.append(rff)
        
        gps_embeddings = torch.cat(rff_features, dim=-1)
        return gps_embeddings


class LocationEncoder(nn.Module):
    """GPS ä½ç½®ç¼–ç å™¨"""
    def __init__(
        self, 
        rff_dim: int = 512,
        hidden_dim: int = 1024,
        output_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.rff_encoder = MultiScaleGPSToRFF(rff_dim, sigmas)
        
        self.mlp = nn.Sequential(
            nn.Linear(rff_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim),
        )
        
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, gps: torch.Tensor) -> torch.Tensor:
        rff_features = self.rff_encoder(gps)
        location_embeddings = self.mlp(rff_features)
        location_embeddings = self.layer_norm(location_embeddings)
        return location_embeddings


class YOLOv10FeatureExtractor(nn.Module):
    """
    â­ å…³é”®ä¿®å¤ï¼šåªæå–YOLOv10çš„ç‰¹å¾ï¼Œé¿å…è®­ç»ƒæ–¹æ³•å†²çª
    """
    def __init__(
        self, 
        model_name: str = "yolov10n",
        feature_dim: int = 512,
        pretrained: bool = True
    ):
        super().__init__()
        
        # ä¸´æ—¶åŠ è½½YOLOæ¨¡å‹æ¥æå–æƒé‡
        if pretrained:
            print(f"Loading pretrained YOLOv10 model: {model_name}")
            temp_yolo = YOLO(f"{model_name}.pt")
        else:
            print(f"Loading YOLOv10 model without pretrained weights: {model_name}")
            temp_yolo = YOLO(f"{model_name}.yaml")
        
        # â­ å…³é”®ä¿®å¤ï¼šåŠ¨æ€æ£€æµ‹å®é™…çš„è¾“å‡ºé€šé“æ•°
        self.model_name = model_name
        self.feature_dim = feature_dim
        
        # â­ å…³é”®ï¼šåˆ›å»ºç‹¬ç«‹çš„ModuleListï¼Œå¤åˆ¶æƒé‡ä½†é¿å…æ–¹æ³•å†²çª
        self.feature_layers = nn.ModuleList()
        
        # æå–backboneå±‚å¹¶å¤åˆ¶æƒé‡
        original_model = temp_yolo.model.model
        
        # é€‰æ‹©å…³é”®çš„ç‰¹å¾æå–å±‚
        layer_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # å‰10å±‚é€šå¸¸æ˜¯backbone
        
        for idx in layer_indices:
            if idx < len(original_model):
                layer = original_model[idx]
                # æ·±æ‹·è´å±‚å¹¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æ¨¡å—ä¸­
                copied_layer = self._copy_layer(layer)
                self.feature_layers.append(copied_layer)
        
        # â­ é‡è¦ï¼šåˆ é™¤ä¸´æ—¶YOLOå¯¹è±¡ï¼Œé¿å…æ–¹æ³•å†²çª
        del temp_yolo
        
        # â­ å»¶è¿Ÿåˆå§‹åŒ–ç‰¹å¾èåˆå±‚ - ç­‰åˆ°ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶å†åˆ›å»º
        self.feature_fusion = None
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        print(f"âœ… YOLOv10FeatureExtractor initialized:")
        print(f"  Model: {model_name}")
        print(f"  Output feature dim: {feature_dim}")
        print(f"  Extracted {len(self.feature_layers)} layers")
        print(f"  Feature fusion will be dynamically created")
    
    def _copy_layer(self, original_layer):
        """æ·±æ‹·è´å±‚ï¼Œé¿å…å¼•ç”¨åŸå§‹YOLOå¯¹è±¡"""
        import copy
        try:
            # å°è¯•æ·±æ‹·è´
            copied_layer = copy.deepcopy(original_layer)
            return copied_layer
        except:
            # å¦‚æœæ·±æ‹·è´å¤±è´¥ï¼Œæ‰‹åŠ¨é‡å»ºå±‚
            print(f"Warning: Failed to copy layer {type(original_layer)}, creating new layer")
            if hasattr(original_layer, 'in_channels') and hasattr(original_layer, 'out_channels'):
                # å¯¹äºå·ç§¯å±‚ï¼Œæ‰‹åŠ¨é‡å»º
                return nn.Conv2d(
                    original_layer.in_channels, 
                    original_layer.out_channels,
                    original_layer.kernel_size,
                    original_layer.stride,
                    original_layer.padding,
                    bias=original_layer.bias is not None
                )
            else:
                # å…¶ä»–å±‚ç±»å‹ï¼Œè¿”å›æ’ç­‰æ˜ å°„
                return nn.Identity()
    
    def _create_feature_fusion(self, total_channels: int):
        """åŠ¨æ€åˆ›å»ºç‰¹å¾èåˆå±‚"""
        if self.feature_fusion is None:
            print(f"ğŸ”§ Creating feature fusion layer: {total_channels} -> {self.feature_dim}")
            self.feature_fusion = nn.Sequential(
                nn.Conv2d(total_channels, self.feature_dim, 1, bias=False),
                nn.BatchNorm2d(self.feature_dim),
                nn.ReLU()
            )
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            if hasattr(self, 'feature_layers') and len(self.feature_layers) > 0:
                device = next(self.feature_layers[0].parameters()).device
                self.feature_fusion = self.feature_fusion.to(device)
    
    def forward(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: Input images, shape (batch_size, 3, H, W)
        Returns:
            Dictionary containing features and embeddings
        """
        x = images
        multi_scale_features = []
        
        # é€šè¿‡å¤åˆ¶çš„å±‚è¿›è¡Œå‰å‘ä¼ æ’­
        for i, layer in enumerate(self.feature_layers):
            try:
                x = layer(x)
                
                # åœ¨ç‰¹å®šå±‚æ”¶é›†ç‰¹å¾ï¼ˆæ ¹æ®ä¸‹é‡‡æ ·ä½ç½®ï¼‰
                if i in [2, 4, 6, 8]:  # æ ¹æ®YOLOv10æ¶æ„è°ƒæ•´
                    multi_scale_features.append(x)
                    
            except Exception as e:
                print(f"Warning: Error in layer {i}: {e}")
                continue
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„ç‰¹å¾å›¾
        if len(multi_scale_features) < 1:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾å›¾ï¼Œä½¿ç”¨æœ€åçš„è¾“å‡º
            multi_scale_features = [x]
        elif len(multi_scale_features) < 4:
            # å¦‚æœç‰¹å¾å›¾ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨ç°æœ‰ç‰¹å¾
            while len(multi_scale_features) < 4:
                multi_scale_features.append(multi_scale_features[-1])
        
        # â­ å…³é”®ä¿®å¤ï¼šåŠ¨æ€è°ƒæ•´ç‰¹å¾å›¾å¹¶è®¡ç®—æ€»é€šé“æ•°
        if multi_scale_features:
            # è·å–ç¬¬ä¸€ä¸ªç‰¹å¾å›¾çš„å°ºå¯¸ä½œä¸ºç›®æ ‡å°ºå¯¸
            target_size = multi_scale_features[0].shape[-2:]
            
            aligned_features = []
            total_channels = 0
            
            for feature_map in multi_scale_features[:4]:  # åªä½¿ç”¨å‰4ä¸ª
                # è°ƒæ•´å°ºå¯¸
                if feature_map.shape[-2:] != target_size:
                    feature_map = F.interpolate(
                        feature_map, size=target_size, mode='bilinear', align_corners=False
                    )
                
                aligned_features.append(feature_map)
                total_channels += feature_map.shape[1]  # ç´¯è®¡é€šé“æ•°
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡ï¼‰
                if self.feature_fusion is None:
                    print(f"    Feature {len(aligned_features)}: {feature_map.shape}")
            
            # â­ åŠ¨æ€åˆ›å»ºç‰¹å¾èåˆå±‚
            self._create_feature_fusion(total_channels)
            
            # è¿æ¥æ‰€æœ‰ç‰¹å¾
            if aligned_features:
                fused_features = torch.cat(aligned_features, dim=1)
                
                # ç‰¹å¾èåˆ
                processed_features = self.feature_fusion(fused_features)
                
                # å…¨å±€å›¾åƒåµŒå…¥
                global_embeddings = self.global_pool(processed_features).flatten(1)
                
                return {
                    'features': processed_features,
                    'embeddings': global_embeddings,
                    'multi_scale_features': multi_scale_features
                }
        
        # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œè¿”å›é›¶ç‰¹å¾
        batch_size = images.shape[0]
        device = images.device
        dummy_features = torch.zeros(batch_size, self.feature_dim, 32, 32, device=device)
        dummy_embeddings = torch.zeros(batch_size, self.feature_dim, device=device)
        
        print(f"âš ï¸ Feature extraction failed, returning dummy features")
        
        return {
            'features': dummy_features,
            'embeddings': dummy_embeddings,
            'multi_scale_features': [dummy_features]
        }


class LocationMemoryBank(nn.Module):
    """ä½ç½®è®°å¿†åº“"""
    def __init__(
        self, 
        feature_dim: int = 512,
        memory_size: int = 20,
        spatial_radius: float = 0.00005,
        save_path: Optional[str] = None
    ):
        super().__init__()
        self.feature_dim = feature_dim
        self.memory_size = memory_size
        self.spatial_radius = spatial_radius
        self.save_path = save_path
        
        self.memory_bank = defaultdict(lambda: {
            'features': [],
            'count': 0,
            'last_updated': 0
        })
        
        self.total_updates = 0
        self.total_queries = 0
        self.hit_count = 0
        
        print(f"âœ… LocationMemoryBank initialized")
        
    def gps_to_key(self, gps: torch.Tensor) -> str:
        lat_grid = round(gps[0].item() / self.spatial_radius) * self.spatial_radius
        lon_grid = round(gps[1].item() / self.spatial_radius) * self.spatial_radius
        return f"{lat_grid:.7f},{lon_grid:.7f}"
    
    def update_memory(self, gps_coords: torch.Tensor, features: torch.Tensor):
        batch_size = gps_coords.shape[0]
        
        for i in range(batch_size):
            gps_key = self.gps_to_key(gps_coords[i])
            
            feature_norm = torch.norm(features[i]).item()
            if feature_norm < 1e-6:
                continue
            
            self.memory_bank[gps_key]['features'].append(features[i].detach().clone())
            self.memory_bank[gps_key]['count'] += 1
            self.memory_bank[gps_key]['last_updated'] = self.total_updates
            
            if len(self.memory_bank[gps_key]['features']) > self.memory_size:
                self.memory_bank[gps_key]['features'].pop(0)
        
        self.total_updates += 1
    
    def retrieve_memory(self, gps_coords: torch.Tensor) -> torch.Tensor:
        batch_size = gps_coords.shape[0]
        memory_features = []
        
        self.total_queries += batch_size
        
        for i in range(batch_size):
            gps_key = self.gps_to_key(gps_coords[i])
            retrieved_features = []
            
            if gps_key in self.memory_bank and len(self.memory_bank[gps_key]['features']) > 0:
                retrieved_features.extend(self.memory_bank[gps_key]['features'])
                self.hit_count += 1
            
            if retrieved_features:
                recent_features = retrieved_features[-8:]
                if len(recent_features) == 1:
                    aggregated = recent_features[0]
                else:
                    weights = torch.softmax(
                        torch.tensor([i for i in range(len(recent_features))], dtype=torch.float32),
                        dim=0
                    ).to(recent_features[0].device)
                    
                    stacked_features = torch.stack(recent_features)
                    aggregated = (stacked_features * weights.unsqueeze(-1)).sum(dim=0)
            else:
                aggregated = torch.zeros(self.feature_dim, device=gps_coords.device)
            
            memory_features.append(aggregated)
        
        return torch.stack(memory_features)
    
    def get_memory_stats(self) -> Dict[str, Any]:
        total_locations = len(self.memory_bank)
        total_memories = sum(len(memory['features']) for memory in self.memory_bank.values())
        hit_rate = self.hit_count / max(self.total_queries, 1)
        
        return {
            'total_locations': total_locations,
            'total_memories': total_memories,
            'hit_rate': hit_rate,
            'avg_memories_per_location': total_memories / max(total_locations, 1),
        }
    
    def save_memory_bank(self):
        if self.save_path:
            stats = {
                'locations': list(self.memory_bank.keys()),
                'counts': {k: v['count'] for k, v in self.memory_bank.items()},
                'stats': self.get_memory_stats()
            }
            
            os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
            with open(self.save_path, 'w') as f:
                json.dump(stats, f, indent=2)


class CrossModalFusion(nn.Module):
    """è·¨æ¨¡æ€èåˆæ¨¡å—"""
    def __init__(
        self, 
        feature_dim: int = 512,
        fusion_method: str = "attention"
    ):
        super().__init__()
        self.fusion_method = fusion_method
        self.feature_dim = feature_dim
        
        if fusion_method == "attention":
            self.query_proj = nn.Linear(feature_dim, feature_dim)
            self.key_proj = nn.Linear(feature_dim, feature_dim)
            self.value_proj = nn.Linear(feature_dim, feature_dim)
            self.scale = math.sqrt(feature_dim)
            
        elif fusion_method == "concat":
            self.fusion_proj = nn.Sequential(
                nn.Linear(feature_dim * 2, feature_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        
    def forward(
        self, 
        image_features: torch.Tensor, 
        location_embeddings: torch.Tensor
    ) -> torch.Tensor:
        batch_size, feature_dim, H, W = image_features.shape
        
        if self.fusion_method == "add":
            location_map = location_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)
            fused_features = image_features + location_map
            
        elif self.fusion_method == "concat":
            location_map = location_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H, W)
            concat_features = torch.cat([image_features, location_map], dim=1)
            concat_features = concat_features.permute(0, 2, 3, 1).reshape(batch_size, H*W, -1)
            fused_features = self.fusion_proj(concat_features)
            fused_features = fused_features.reshape(batch_size, H, W, feature_dim).permute(0, 3, 1, 2)
            
        elif self.fusion_method == "attention":
            img_seq = image_features.permute(0, 2, 3, 1).reshape(batch_size, H*W, feature_dim)
            
            Q = self.query_proj(img_seq)
            K = self.key_proj(location_embeddings.unsqueeze(1))
            V = self.value_proj(location_embeddings.unsqueeze(1))
            
            attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
            attention_weights = F.softmax(attention_weights, dim=-1)
            
            attended_location = torch.matmul(attention_weights, V)
            fused_seq = img_seq + attended_location
            fused_features = fused_seq.reshape(batch_size, H, W, feature_dim).permute(0, 3, 1, 2)
        
        return fused_features


class YOLOv10MemoryEnhancedGeoSegformer(nn.Module):
    """
    â­ ä¿®å¤ç‰ˆï¼šä½¿ç”¨å®˜æ–¹YOLOv10æƒé‡ä½†é¿å…æ–¹æ³•å†²çª
    """
    def __init__(
        self,
        num_classes: int,
        yolo_model: str = "yolov10n",
        feature_dim: int = 512,
        rff_dim: int = 512,
        sigmas: List[float] = [0.0001, 0.001, 0.01],
        fusion_method: str = "attention",
        dropout: float = 0.1,
        memory_size: int = 20,
        spatial_radius: float = 0.00005,
        memory_save_path: Optional[str] = None,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.num_classes = num_classes
        self.feature_dim = feature_dim
        
        print(f"ğŸš€ Initializing YOLOv10MemoryEnhancedGeoSegformer (Fixed)")
        print(f"  YOLO model: {yolo_model}")
        print(f"  Num classes: {num_classes}")
        print(f"  Feature dim: {feature_dim}")
        print(f"  Using official pretrained weights: True")
        
        # GPS ä½ç½®ç¼–ç å™¨
        self.location_encoder = LocationEncoder(
            rff_dim=rff_dim,
            output_dim=feature_dim,
            sigmas=sigmas,
            dropout=dropout
        )
        
        # â­ ä¿®å¤ç‰ˆYOLOv10ç‰¹å¾æå–å™¨
        self.image_encoder = YOLOv10FeatureExtractor(
            model_name=yolo_model,
            feature_dim=feature_dim,
            pretrained=True
        )
        
        if freeze_backbone:
            for param in self.image_encoder.parameters():
                param.requires_grad = False
            print("ğŸ”’ YOLOv10 backbone frozen")
        
        # ä½ç½®è®°å¿†åº“
        self.memory_bank = LocationMemoryBank(
            feature_dim=feature_dim,
            memory_size=memory_size,
            spatial_radius=spatial_radius,
            save_path=memory_save_path
        )
        
        # è®°å¿†èåˆå±‚
        self.memory_fusion = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim, feature_dim)
        )
        
        # è®°å¿†æ³¨æ„åŠ›æœºåˆ¶
        self.memory_attention = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # è·¨æ¨¡æ€èåˆ
        self.cross_modal_fusion = CrossModalFusion(
            feature_dim=feature_dim,
            fusion_method=fusion_method
        )
        
        # åˆ†å‰²å¤´
        self.segmentation_head = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim),
            nn.ReLU(),
            nn.Dropout2d(dropout),
            nn.Conv2d(feature_dim, feature_dim // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(feature_dim // 2),
            nn.ReLU(),
            nn.Conv2d(feature_dim // 2, num_classes, 1)
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.contrastive_proj = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, 256)
        )
        
        total_params = sum(p.numel() for p in self.parameters()) / 1e6
        print(f"âœ… Model initialized with {total_params:.2f}M parameters")
    
    def forward(
        self, 
        images: torch.Tensor, 
        gps: torch.Tensor,
        return_embeddings: bool = False,
        update_memory: bool = True,
        return_intermediate_features: bool = False
    ) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - ç°åœ¨ä¸ä¼šæœ‰æ–¹æ³•å†²çª"""
        # GPS ä½ç½®ç¼–ç 
        location_embeddings = self.location_encoder(gps)
        
        # YOLOv10 å›¾åƒç‰¹å¾æå–
        image_outputs = self.image_encoder(images)
        image_features = image_outputs['features']
        image_embeddings = image_outputs['embeddings']
        
        # ä¿å­˜åŸå§‹ç‰¹å¾
        original_image_embeddings = image_embeddings.clone()
        
        # æ£€ç´¢ä½ç½®è®°å¿†
        memory_features = self.memory_bank.retrieve_memory(gps)
        
        # è®°å¿†å¢å¼ºå¤„ç† (ä¸åŸç‰ˆç›¸åŒ)
        enhanced_embeddings = image_embeddings
        memory_weight = 0.0
        
        memory_norms = torch.norm(memory_features, dim=-1)
        valid_memory_mask = memory_norms > 1e-6
        
        if valid_memory_mask.any():
            memory_weight = valid_memory_mask.float().mean().item()
            
            if memory_weight > 0:
                valid_indices = valid_memory_mask.nonzero(as_tuple=True)[0]
                
                if len(valid_indices) > 0:
                    valid_memory_features = memory_features[valid_indices]
                    valid_image_embeddings = image_embeddings[valid_indices]
                    
                    combined_features = torch.cat([valid_image_embeddings, valid_memory_features], dim=-1)
                    fused_features = self.memory_fusion(combined_features)
                    
                    memory_enhanced, attention_weights = self.memory_attention(
                        valid_image_embeddings.unsqueeze(1),
                        valid_memory_features.unsqueeze(1),
                        valid_memory_features.unsqueeze(1)
                    )
                    
                    enhanced_part = (
                        0.6 * fused_features + 
                        0.4 * memory_enhanced.squeeze(1)
                    )
                    
                    enhanced_part = enhanced_part + valid_image_embeddings
                    
                    enhanced_embeddings = image_embeddings.clone()
                    enhanced_embeddings[valid_indices] = enhanced_part
        
        # å¢å¼ºçš„ä½ç½®åµŒå…¥
        enhanced_location_embeddings = location_embeddings + 0.3 * enhanced_embeddings
        
        # è·¨æ¨¡æ€ç‰¹å¾èåˆ
        fused_features = self.cross_modal_fusion(image_features, enhanced_location_embeddings)
        
        # è¯­ä¹‰åˆ†å‰²é¢„æµ‹
        segmentation_logits = self.segmentation_head(fused_features)
        
        # è°ƒæ•´åˆ°è¾“å…¥å›¾åƒå°ºå¯¸
        segmentation_logits = F.interpolate(
            segmentation_logits, 
            size=images.shape[-2:], 
            mode='bilinear', 
            align_corners=False
        )
        
        # æ›´æ–°è®°å¿†åº“
        if update_memory and self.training:
            self.memory_bank.update_memory(gps, image_embeddings)
        
        outputs = {
            'segmentation_logits': segmentation_logits,
            'fused_features': fused_features,
            'memory_weight': memory_weight
        }
        
        if return_intermediate_features:
            outputs.update({
                'original_image_embeddings': original_image_embeddings,
                'enhanced_image_embeddings': enhanced_embeddings,
                'valid_memory_mask': valid_memory_mask,
                'yolo_features': image_outputs.get('multi_scale_features', [])
            })
        
        if return_embeddings:
            image_proj = self.contrastive_proj(enhanced_embeddings)
            location_proj = self.contrastive_proj(enhanced_location_embeddings)
            outputs.update({
                'image_embeddings': image_proj,
                'location_embeddings': location_proj
            })
        
        return outputs
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.memory_bank.get_memory_stats()
    
    def save_memory_bank(self):
        """ä¿å­˜è®°å¿†åº“"""
        self.memory_bank.save_memory_bank()


def create_yolov10_memory_enhanced_geo_segformer(
    num_classes: int,
    yolo_model: str = "yolov10n",
    feature_dim: int = 512,
    fusion_method: str = "attention",
    memory_size: int = 20,
    spatial_radius: float = 0.00005,
    memory_save_path: Optional[str] = None,
    freeze_backbone: bool = False
) -> YOLOv10MemoryEnhancedGeoSegformer:
    """
    åˆ›å»ºä¿®å¤ç‰ˆåŸºäºYOLOv10çš„è®°å¿†å¢å¼ºGeoSegformer
    
    âœ… ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæƒé‡
    âœ… é¿å…æ–¹æ³•å†²çª  
    âœ… ä¿æŒæ‰€æœ‰è®°å¿†åŠŸèƒ½
    """
    
    return YOLOv10MemoryEnhancedGeoSegformer(
        num_classes=num_classes,
        yolo_model=yolo_model,
        feature_dim=feature_dim,
        fusion_method=fusion_method,
        memory_size=memory_size,
        spatial_radius=spatial_radius,
        memory_save_path=memory_save_path,
        freeze_backbone=freeze_backbone
    )


if __name__ == "__main__":
    # æµ‹è¯•ä¿®å¤ç‰ˆYOLOv10
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("=" * 70)
    print("ğŸ§ª æµ‹è¯•ä¿®å¤ç‰ˆå®˜æ–¹YOLOv10è®°å¿†å¢å¼ºGeoSegformer")
    print("=" * 70)
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = create_yolov10_memory_enhanced_geo_segformer(
            num_classes=25, 
            yolo_model="yolov10n",
            memory_size=15,
            spatial_radius=0.00005,
            memory_save_path="./fixed_yolov10_memory_stats.json"
        ).to(device)
        
        print(f"âœ… åˆ›å»ºä¿®å¤ç‰ˆYOLOv10æ¨¡å‹æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®
        batch_size = 2
        images = torch.randn(batch_size, 3, 640, 640).to(device)
        gps = torch.tensor([
            [-0.001057, -0.000368],
            [-0.000738, -0.000405]
        ], dtype=torch.float32).to(device)
        
        print(f"\nğŸ” æµ‹è¯•é…ç½®:")
        print(f"  Batch size: {batch_size}")
        print(f"  Image shape: {images.shape}")
        print(f"  GPS shape: {gps.shape}")
        print(f"  ä½¿ç”¨å®˜æ–¹YOLOv10æƒé‡: True")
        
        # â­ å…³é”®æµ‹è¯•ï¼šè®­ç»ƒæ¨¡å¼è®¾ç½®ä¸ä¼šå†²çª
        print(f"\nğŸš€ æµ‹è¯•è®­ç»ƒæ¨¡å¼è®¾ç½®...")
        model.train()  # è¿™é‡Œä¸åº”è¯¥å†æœ‰å†²çª
        print(f"  âœ… model.train() æˆåŠŸ - æ— å†²çª")
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        print(f"\nğŸš€ å‰å‘ä¼ æ’­æµ‹è¯•...")
        outputs = model(images, gps, return_embeddings=True, update_memory=True, 
                       return_intermediate_features=True)
        
        print(f"  åˆ†å‰²è¾“å‡ºå½¢çŠ¶: {outputs['segmentation_logits'].shape}")
        print(f"  è®°å¿†æƒé‡: {outputs['memory_weight']:.4f}")
        print(f"  å›¾åƒåµŒå…¥å½¢çŠ¶: {outputs['image_embeddings'].shape}")
        print(f"  ä½ç½®åµŒå…¥å½¢çŠ¶: {outputs['location_embeddings'].shape}")
        
        # æ£€æŸ¥YOLOv10ç‰¹å¾
        if 'yolo_features' in outputs:
            print(f"  å®˜æ–¹YOLOv10å¤šå°ºåº¦ç‰¹å¾æ•°é‡: {len(outputs['yolo_features'])}")
            for i, feat in enumerate(outputs['yolo_features']):
                print(f"    å®˜æ–¹ç‰¹å¾{i+1}å½¢çŠ¶: {feat.shape}")
        
        # æ£€æŸ¥è®°å¿†åº“ç»Ÿè®¡
        memory_stats = model.get_memory_stats()
        print(f"\nğŸ“Š è®°å¿†åº“ç»Ÿè®¡:")
        print(f"  æ€»ä½ç½®æ•°: {memory_stats['total_locations']}")
        print(f"  æ€»è®°å¿†æ•°: {memory_stats['total_memories']}")
        print(f"  å‘½ä¸­ç‡: {memory_stats['hit_rate']:.4f}")
        
        # å¤šæ¬¡å‰å‘ä¼ æ’­æµ‹è¯•è®°å¿†ç´¯ç§¯
        print(f"\nğŸ”„ å¤šæ¬¡å‰å‘ä¼ æ’­æµ‹è¯•...")
        for i in range(2, 6):
            outputs = model(images, gps, return_embeddings=True, update_memory=True)
            memory_stats = model.get_memory_stats()
            print(f"  ç¬¬{i}æ¬¡ - ä½ç½®æ•°: {memory_stats['total_locations']}, "
                  f"è®°å¿†æ•°: {memory_stats['total_memories']}, "
                  f"å‘½ä¸­ç‡: {memory_stats['hit_rate']:.4f}")
        
        # æµ‹è¯•æ¨ç†æ¨¡å¼
        print(f"\nğŸ”® æµ‹è¯•æ¨ç†æ¨¡å¼...")
        model.eval()
        with torch.no_grad():
            outputs_inference = model(images, gps, return_embeddings=False, update_memory=False)
            print(f"  æ¨ç†æ¨¡å¼è®°å¿†æƒé‡: {outputs_inference['memory_weight']:.4f}")
        
        # ä¿å­˜è®°å¿†åº“ç»Ÿè®¡
        model.save_memory_bank()
        
        print(f"\nğŸ‰ ä¿®å¤ç‰ˆå®˜æ–¹YOLOv10æµ‹è¯•å®Œæˆï¼")
        print(f"âœ… ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæƒé‡")
        print(f"âœ… æˆåŠŸé¿å…ultralyticså†²çª")
        print(f"âœ… è®°å¿†æœºåˆ¶æ­£å¸¸å·¥ä½œ")
        print(f"âœ… è®­ç»ƒæ¨¡å¼åˆ‡æ¢æ­£å¸¸")
        print(f"ğŸ“ è®°å¿†åº“ç»Ÿè®¡å·²ä¿å­˜")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("è¯·ç¡®ä¿å·²å®‰è£…ultralytics: pip install ultralytics")
        import traceback
        traceback.print_exc()