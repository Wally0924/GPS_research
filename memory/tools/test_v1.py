import os
import sys
from argparse import ArgumentParser, Namespace
from pathlib import Path

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

import torch
from rich import print
from rich.progress import Progress
from rich.table import Table
import pandas as pd
from typing import Dict, Any

import engine.transform as transform
from engine.category import Category
from engine.metric import Metrics
from engine.visualizer import IdMapVisualizer, ImgSaver
from engine.geo_v2 import create_memory_enhanced_geo_segformer
from engine.dataloading import ImgAnnDataset


class MemoryEnhancedGeoSegDataset(ImgAnnDataset):
    """
    è¨˜æ†¶å¢å¼·ç‰ˆæ•¸æ“šé›†
    """
    def __init__(
        self,
        transforms: list,
        img_dir: str,
        ann_dir: str,
        gps_csv: str,
        max_len: int = None,
    ):
        super().__init__(transforms, img_dir, ann_dir, max_len)
        
        # è¼‰å…¥ GPS æ•¸æ“š
        self.gps_data = pd.read_csv(gps_csv)
        
        # å‰µå»ºæª”ååˆ° GPS çš„æ˜ å°„
        self.filename_to_gps = {}
        for _, row in self.gps_data.iterrows():
            filename = os.path.splitext(row['filename'])[0]
            self.filename_to_gps[filename] = [row['lat'], row['long']]
        
        print(f"âœ… Loaded GPS data for {len(self.filename_to_gps)} images")
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        # ç²å–åŸå§‹æ•¸æ“š
        data = super().__getitem__(idx)
        
        # å¾è·¯å¾‘ä¸­æå–æª”å
        img_path = self.img_ann_paths[idx][0]
        filename = os.path.splitext(os.path.basename(img_path))[0]
        
        # æ·»åŠ  GPS æ•¸æ“šå’Œæª”å
        if filename in self.filename_to_gps:
            gps_coords = self.filename_to_gps[filename]
            data['gps'] = torch.tensor(gps_coords, dtype=torch.float32)
        else:
            print(f"âš ï¸ Warning: No GPS data found for {filename}")
            data['gps'] = torch.zeros(2, dtype=torch.float32)
        
        # æ·»åŠ æª”åç”¨æ–¼è¿½è¹¤
        data['filename'] = filename
        
        return data


def setup_gps_normalization(train_gps_csv: str, val_gps_csv: str, method: str = "minmax"):
    """
    è¨­ç½®GPSæ­£è¦åŒ–
    """
    
    # åˆä½µè¨“ç·´å’Œé©—è­‰é›†è¨ˆç®—å…¨å±€çµ±è¨ˆ
    train_gps = pd.read_csv(train_gps_csv)
    val_gps = pd.read_csv(val_gps_csv)
    all_gps = pd.concat([train_gps, val_gps], ignore_index=True)
    
    if method == "minmax":
        lat_min = all_gps['lat'].min()
        lat_max = all_gps['lat'].max()
        lon_min = all_gps['long'].min()
        lon_max = all_gps['long'].max()
        
        # æ·»åŠ å°é‡paddingé¿å…é‚Šç•Œå•é¡Œ
        lat_range = lat_max - lat_min
        lon_range = lon_max - lon_min
        padding = 0.01  # 1%çš„padding
        
        lat_min -= lat_range * padding
        lat_max += lat_range * padding
        lon_min -= lon_range * padding
        lon_max += lon_range * padding
        
        return transform.GPSNormalize(
            lat_range=(lat_min, lat_max),
            lon_range=(lon_min, lon_max)
        )
    else:
        raise ValueError(f"Method {method} not implemented yet")


class GeoSlideInferencer:
    """
    å°ˆç‚ºGeoSegformerè¨­è¨ˆçš„æ»‘çª—æ¨ç†å™¨
    """
    def __init__(
        self, 
        crop_size: tuple[int, int], 
        stride: tuple[int, int], 
        num_categories: int
    ) -> None:
        self.crop_size = crop_size
        self.stride = stride
        self.num_categories = num_categories

    def inference(self, model: torch.nn.Module, img: torch.Tensor, gps: torch.Tensor) -> torch.Tensor:
        """
        å°GeoSegformeré€²è¡Œæ»‘çª—æ¨ç†
        """
        return self.slide_inference(model, img, gps, self.crop_size, self.stride, self.num_categories)
    
    def slide_inference(
        self,
        model: torch.nn.Module,
        img: torch.Tensor,
        gps: torch.Tensor,
        crop_size: tuple[int, int],
        stride: tuple[int, int],
        num_classes: int,
    ):
        """æ»‘çª—æ¨ç†å¯¦ç¾"""
        import torch.nn.functional as F
        
        h_stride, w_stride = stride
        h_crop, w_crop = crop_size
        batch_size, _, h_img, w_img = img.size()
        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1
        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1
        preds = img.new_zeros((batch_size, num_classes, h_img, w_img))
        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))
        
        for h_idx in range(h_grids):
            for w_idx in range(w_grids):
                y1 = h_idx * h_stride
                x1 = w_idx * w_stride
                y2 = min(y1 + h_crop, h_img)
                x2 = min(x1 + w_crop, w_img)
                y1 = max(y2 - h_crop, 0)
                x1 = max(x2 - w_crop, 0)
                crop_img = img[:, :, y1:y2, x1:x2]
                
                # å°æ–¼GeoSegformerï¼Œéœ€è¦å‚³å…¥GPSåº§æ¨™
                crop_outputs = model(crop_img, gps, return_embeddings=False, update_memory=False)
                crop_seg_logit = crop_outputs['segmentation_logits']
                
                preds += F.pad(
                    crop_seg_logit,
                    (int(x1), int(preds.shape[3] - x2), int(y1), int(preds.shape[2] - y2)),
                )
                count_mat[:, :, y1:y2, x1:x2] += 1
        
        assert (count_mat == 0).sum() == 0
        preds = preds / count_mat
        return preds


def load_model_weights_safely(checkpoint_path: str, model: torch.nn.Module, device: str):
    """
    å®‰å…¨è¼‰å…¥æ¨¡å‹æ¬Šé‡
    """
    try:
        # æ–¹æ³•1ï¼šå˜—è©¦å®‰å…¨æ¨¡å¼
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
        if isinstance(checkpoint, dict) and "model" in checkpoint:
            model.load_state_dict(checkpoint["model"])
        else:
            model.load_state_dict(checkpoint)
        print("âœ… ä½¿ç”¨å®‰å…¨æ¨¡å¼è¼‰å…¥æª¢æŸ¥é»")
        return None
        
    except Exception:
        # æ–¹æ³•2ï¼šä½¿ç”¨ argparse.Namespace å®‰å…¨å…¨å±€è®Šæ•¸
        try:
            from argparse import Namespace
            torch.serialization.add_safe_globals([Namespace])
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
            if isinstance(checkpoint, dict) and "model" in checkpoint:
                model.load_state_dict(checkpoint["model"])
            else:
                model.load_state_dict(checkpoint)
            print("âœ… ä½¿ç”¨å®‰å…¨å…¨å±€è®Šæ•¸è¼‰å…¥æª¢æŸ¥é»")
            return checkpoint if isinstance(checkpoint, dict) else None
            
        except Exception:
            # æ–¹æ³•3ï¼šå‚³çµ±æ¨¡å¼ï¼ˆå‡è¨­æª”æ¡ˆå¯ä¿¡ï¼‰
            print("âš ï¸ ä½¿ç”¨å‚³çµ±æ¨¡å¼è¼‰å…¥æª¢æŸ¥é»ï¼ˆè«‹ç¢ºä¿æª”æ¡ˆä¾†æºå¯ä¿¡ï¼‰")
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            if isinstance(checkpoint, dict) and "model" in checkpoint:
                model.load_state_dict(checkpoint["model"])
            else:
                model.load_state_dict(checkpoint)
            return checkpoint if isinstance(checkpoint, dict) else None


def parse_args() -> Namespace:
    parser = ArgumentParser(description="Memory-Enhanced GeoSegformer Testing")
    # åŸºæœ¬åƒæ•¸
    parser.add_argument("img_dir", type=str, help="æ¸¬è©¦å½±åƒç›®éŒ„")
    parser.add_argument("ann_dir", type=str, help="æ¸¬è©¦æ¨™è¨»ç›®éŒ„")
    parser.add_argument("category_csv", type=str, help="é¡åˆ¥å®šç¾©CSVæ–‡ä»¶")
    parser.add_argument("checkpoint", type=str, help="æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘")
    parser.add_argument("test_gps_csv", type=str, help="æ¸¬è©¦é›†GPS CSVæ–‡ä»¶")
    
    # GPSæ­£è¦åŒ–åƒæ•¸ï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    parser.add_argument("--train-gps-csv", type=str, required=True, 
                       help="è¨“ç·´é›†GPS CSVï¼ˆç”¨æ–¼è¨ˆç®—æ­£è¦åŒ–åƒæ•¸ï¼‰")
    parser.add_argument("--val-gps-csv", type=str, required=True,
                       help="é©—è­‰é›†GPS CSVï¼ˆç”¨æ–¼è¨ˆç®—æ­£è¦åŒ–åƒæ•¸ï¼‰") 
    parser.add_argument("--gps-norm-method", type=str, default="minmax",
                       choices=["minmax", "zscore"],
                       help="GPSæ­£è¦åŒ–æ–¹æ³•ï¼ˆå¿…é ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰")
    
    # æ¨¡å‹åƒæ•¸ï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    parser.add_argument("--model-size", type=str, default="b0", choices=["b0", "b1", "b2"])
    parser.add_argument("--feature-dim", type=int, default=512)
    parser.add_argument("--fusion-method", type=str, default="attention", 
                       choices=["add", "concat", "attention"])
    parser.add_argument("--memory-size", type=int, default=20)
    parser.add_argument("--spatial-radius", type=float, default=0.00005)
    
    # æ¸¬è©¦åƒæ•¸
    parser.add_argument("--save-dir", type=str, default=None, help="çµæœä¿å­˜ç›®éŒ„")
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--num-workers", type=int, default=0)
    parser.add_argument("--max-len", type=int, default=None)
    
    # æ¨ç†åƒæ•¸
    parser.add_argument("--use-slide-inference", action="store_true", default=True,
                       help="ä½¿ç”¨æ»‘çª—æ¨ç†")
    
    return parser.parse_args()


def main(args: Namespace):
    print("ğŸ§ª è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer æ¸¬è©¦é–‹å§‹")
    print("=" * 60)
    
    # åœ–åƒå°ºå¯¸è¨­å®š
    image_size = 1080, 1920  # H x W
    crop_size = 512, 512
    stride = 384, 384
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥é¡åˆ¥å®šç¾©
    categories = Category.load(args.category_csv)
    num_categories = Category.get_num_categories(categories)
    print(f"ğŸ“Š é¡åˆ¥æ•¸é‡: {num_categories}")
    
    # è¨­ç½®GPSæ­£è¦åŒ–ï¼ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
    print(f"ğŸŒ è¨­ç½®GPSæ­£è¦åŒ– (æ–¹æ³•: {args.gps_norm_method})")
    gps_normalizer = setup_gps_normalization(
        args.train_gps_csv, 
        args.val_gps_csv,
        method=args.gps_norm_method
    )
    
    # å‰µå»ºæ¨¡å‹
    print(f"ğŸš€ å‰µå»ºè¨˜æ†¶å¢å¼·ç‰ˆæ¨¡å‹...")
    model = create_memory_enhanced_geo_segformer(
        num_classes=num_categories,
        model_size=args.model_size,
        feature_dim=args.feature_dim,
        fusion_method=args.fusion_method,
        memory_size=args.memory_size,
        spatial_radius=args.spatial_radius,
        memory_save_path=None  # æ¸¬è©¦æ™‚ä¸ä¿å­˜è¨˜æ†¶åº«
    ).to(device)
    
    # è¼‰å…¥æª¢æŸ¥é»
    print(f"ğŸ“‚ è¼‰å…¥æª¢æŸ¥é»: {args.checkpoint}")
    checkpoint = load_model_weights_safely(args.checkpoint, model, device)
    model.eval()
    
    # é¡¯ç¤ºæ¨¡å‹åƒæ•¸çµ±è¨ˆ
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œåƒæ•¸é‡: {total_params:.2f}M")
    
    # å¦‚æœæª¢æŸ¥é»ä¸­æœ‰è¨˜æ†¶åº«çµ±è¨ˆï¼Œé¡¯ç¤ºè¨“ç·´æ™‚çš„çµ±è¨ˆ
    if isinstance(checkpoint, dict) and "memory_stats" in checkpoint:
        memory_stats = checkpoint["memory_stats"]
        print(f"ğŸ“ˆ è¨“ç·´æ™‚è¨˜æ†¶åº«çµ±è¨ˆ:")
        print(f"  ç¸½ä½ç½®æ•¸: {memory_stats.get('total_locations', 'N/A')}")
        print(f"  ç¸½è¨˜æ†¶æ•¸: {memory_stats.get('total_memories', 'N/A')}")
        print(f"  å‘½ä¸­ç‡: {memory_stats.get('hit_rate', 'N/A'):.4f}")
    else:
        print(f"â„¹ï¸ æª¢æŸ¥é»ä¸­æœªåŒ…å«è¨˜æ†¶åº«çµ±è¨ˆä¿¡æ¯")
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    if args.save_dir and not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
        print(f"ğŸ“ å‰µå»ºä¿å­˜ç›®éŒ„: {args.save_dir}")
    
    # æ•¸æ“šè®Šæ›ï¼ˆèˆ‡è¨“ç·´æ™‚çš„é©—è­‰é›†è®Šæ›ä¸€è‡´ï¼‰
    transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,  # â­ é‡è¦ï¼šå¿…é ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´
        transform.Resize(image_size),
        transform.Normalize(),
    ]
    
    # å‰µå»ºæ¨ç†å™¨
    if args.use_slide_inference:
        inferencer = GeoSlideInferencer(crop_size, stride, num_categories)
        print(f"ğŸ” ä½¿ç”¨æ»‘çª—æ¨ç† (crop: {crop_size}, stride: {stride})")
    else:
        # å¦‚æœä¸ä½¿ç”¨æ»‘çª—æ¨ç†ï¼Œç›´æ¥èª¿ç”¨æ¨¡å‹
        inferencer = None
        print(f"ğŸ” ä½¿ç”¨ç›´æ¥æ¨ç†")
    
    # å¯è¦–åŒ–å·¥å…·
    if args.save_dir:
        visualizer = IdMapVisualizer(categories)
        img_saver = ImgSaver(args.save_dir, visualizer)
    
    # æå¤±å‡½æ•¸å’Œè©•ä¼°æŒ‡æ¨™
    criterion = torch.nn.CrossEntropyLoss().to(device)
    metrics = Metrics(num_categories, nan_to_num=0)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
    print(f"ğŸ“ è¼‰å…¥æ¸¬è©¦æ•¸æ“šé›†...")
    test_dataset = MemoryEnhancedGeoSegDataset(
        transforms=transforms,
        img_dir=args.img_dir,
        ann_dir=args.ann_dir,
        gps_csv=args.test_gps_csv,
        max_len=args.max_len,
    )
    
    test_dataloader = test_dataset.get_loader(
        batch_size=args.batch_size, 
        pin_memory=False, 
        num_workers=args.num_workers
    )
    
    print(f"ğŸ“Š æ¸¬è©¦æ•¸æ“š: {len(test_dataset)} å¼µå½±åƒ")
    
    # é–‹å§‹æ¸¬è©¦
    print(f"\nğŸ§ª é–‹å§‹æ¸¬è©¦...")
    with Progress() as prog:
        with torch.no_grad():
            task = prog.add_task("Testing", total=len(test_dataloader))
            avg_loss = 0
            memory_weight_sum = 0
            
            for batch_idx, data in enumerate(test_dataloader):
                img = data["img"].to(device)
                ann = data["ann"].to(device)[:, 0, :, :]
                gps = data["gps"].to(device)
                
                # æ¨ç†
                if inferencer is not None:
                    # æ»‘çª—æ¨ç†
                    pred = inferencer.inference(model, img, gps)
                else:
                    # ç›´æ¥æ¨ç†
                    outputs = model(img, gps, return_embeddings=False, update_memory=False)
                    pred = outputs['segmentation_logits']
                    memory_weight_sum += outputs.get('memory_weight', 0)
                
                # è¨ˆç®—æå¤±
                loss = criterion(pred, ann)
                avg_loss += loss.item()
                
                # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
                metrics.compute_and_accum(pred.argmax(1), ann)
                
                # ä¿å­˜çµæœ
                if args.save_dir:
                    filenames = data.get("filename", [f"test_{batch_idx}"])
                    for i, (filename, p) in enumerate(zip(filenames, pred)):
                        save_name = f"{filename}_pred.png"
                        img_saver.save_pred(p[None, :], save_name)
                
                prog.update(task, advance=1)
            
            # ç²å–æœ€çµ‚çµæœ
            result = metrics.get_and_reset()
            avg_loss /= len(test_dataloader)
            avg_memory_weight = memory_weight_sum / len(test_dataloader) if inferencer is None else 0
            
            prog.remove_task(task)
    
    # å‰µå»ºçµæœè¡¨æ ¼
    print(f"\nğŸ“Š æ¸¬è©¦çµæœ:")
    table = Table()
    table.add_column("Category")
    table.add_column("Acc")
    table.add_column("IoU")
    table.add_column("Dice")
    table.add_column("Fscore")
    table.add_column("Precision")
    table.add_column("Recall")
    
    for cat, acc, iou, dice, fs, pre, rec in zip(
        categories,
        result["Acc"],
        result["IoU"],
        result["Dice"],
        result["Fscore"],
        result["Precision"],
        result["Recall"],
    ):
        table.add_row(
            cat.name,
            "{:.5f}".format(acc),
            "{:.5f}".format(iou),
            "{:.5f}".format(dice),
            "{:.5f}".format(fs),
            "{:.5f}".format(pre),
            "{:.5f}".format(rec),
        )
    
    # æ·»åŠ å¹³å‡å€¼
    table.add_row(
        "Avg.",
        "{:.5f}".format(result["Acc"].mean()),
        "{:.5f}".format(result["IoU"].mean()),
        "{:.5f}".format(result["Dice"].mean()),
        "{:.5f}".format(result["Fscore"].mean()),
        "{:.5f}".format(result["Precision"].mean()),
        "{:.5f}".format(result["Recall"].mean()),
    )
    
    print(table)
    print(f"\nğŸ“ˆ ç¸½é«”æŒ‡æ¨™:")
    print(f"  å¹³å‡æå¤±: {avg_loss:.5f}")
    print(f"  å¹³å‡IoU: {result['IoU'].mean():.5f}")
    print(f"  å¹³å‡Dice: {result['Dice'].mean():.5f}")
    if avg_memory_weight > 0:
        print(f"  å¹³å‡è¨˜æ†¶æ¬Šé‡: {avg_memory_weight:.4f}")
    
    # ç²å–æ¸¬è©¦æ™‚è¨˜æ†¶åº«çµ±è¨ˆ
    test_memory_stats = model.get_memory_stats()
    print(f"\nğŸ§  æ¸¬è©¦æ™‚è¨˜æ†¶åº«çµ±è¨ˆ:")
    print(f"  ç¸½ä½ç½®æ•¸: {test_memory_stats['total_locations']}")
    print(f"  ç¸½è¨˜æ†¶æ•¸: {test_memory_stats['total_memories']}")
    print(f"  å‘½ä¸­ç‡: {test_memory_stats['hit_rate']:.4f}")
    print(f"  ç¸½æŸ¥è©¢æ•¸: {test_memory_stats['total_queries']}")
    
    print(f"\nğŸ‰ è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer æ¸¬è©¦å®Œæˆï¼")
    if args.save_dir:
        print(f"ğŸ“ çµæœå·²ä¿å­˜åˆ°: {args.save_dir}")


if __name__ == "__main__":
    args = parse_args()
    main(args)