import os
from argparse import ArgumentParser, Namespace
from pathlib import Path

import torch
from rich import print
from rich.progress import Progress
from rich.table import Table

import engine.transform as transform
from engine.category import Category
from engine.metric import Metrics
from engine.visualizer import IdMapVisualizer, ImgSaver
from engine.geo_v2 import create_memory_enhanced_geo_segformer
from tools.geotrain_v2 import MemoryEnhancedGeoSegDataset, setup_gps_normalization


class GeoSlideInferencer:
    """
    å°ˆç‚ºGeoSegformerè¨­è¨ˆçš„æ»‘çª—æ¨ç†å™¨
    """
    def __init__(
        self, 
        crop_size: tuple[int, int], 
        stride: tuple[int, int], 
        num_categories: int
    ) -> None:
        self.crop_size = crop_size
        self.stride = stride
        self.num_categories = num_categories

    def inference(self, model: torch.nn.Module, img: torch.Tensor, gps: torch.Tensor) -> torch.Tensor:
        """
        å°GeoSegformeré€²è¡Œæ»‘çª—æ¨ç†
        """
        return self.slide_inference(model, img, gps, self.crop_size, self.stride, self.num_categories)
    
    def slide_inference(
        self,
        model: torch.nn.Module,
        img: torch.Tensor,
        gps: torch.Tensor,
        crop_size: tuple[int, int],
        stride: tuple[int, int],
        num_classes: int,
    ):
        """æ»‘çª—æ¨ç†å¯¦ç¾"""
        import torch.nn.functional as F
        
        h_stride, w_stride = stride
        h_crop, w_crop = crop_size
        batch_size, _, h_img, w_img = img.size()
        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1
        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1
        preds = img.new_zeros((batch_size, num_classes, h_img, w_img))
        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))
        
        for h_idx in range(h_grids):
            for w_idx in range(w_grids):
                y1 = h_idx * h_stride
                x1 = w_idx * w_stride
                y2 = min(y1 + h_crop, h_img)
                x2 = min(x1 + w_crop, w_img)
                y1 = max(y2 - h_crop, 0)
                x1 = max(x2 - w_crop, 0)
                crop_img = img[:, :, y1:y2, x1:x2]
                
                # å°æ–¼GeoSegformerï¼Œéœ€è¦å‚³å…¥GPSåº§æ¨™
                crop_outputs = model(crop_img, gps, return_embeddings=False, update_memory=False)
                crop_seg_logit = crop_outputs['segmentation_logits']
                
                preds += F.pad(
                    crop_seg_logit,
                    (int(x1), int(preds.shape[3] - x2), int(y1), int(preds.shape[2] - y2)),
                )
                count_mat[:, :, y1:y2, x1:x2] += 1
        
        assert (count_mat == 0).sum() == 0
        preds = preds / count_mat
        return preds


def parse_args() -> Namespace:
    parser = ArgumentParser(description="Memory-Enhanced GeoSegformer Testing")
    # åŸºæœ¬åƒæ•¸
    parser.add_argument("img_dir", type=str, help="æ¸¬è©¦å½±åƒç›®éŒ„")
    parser.add_argument("ann_dir", type=str, help="æ¸¬è©¦æ¨™è¨»ç›®éŒ„")
    parser.add_argument("category_csv", type=str, help="é¡åˆ¥å®šç¾©CSVæ–‡ä»¶")
    parser.add_argument("checkpoint", type=str, help="æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘")
    parser.add_argument("test_gps_csv", type=str, help="æ¸¬è©¦é›†GPS CSVæ–‡ä»¶")
    
    # GPSæ­£è¦åŒ–åƒæ•¸ï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    parser.add_argument("--train-gps-csv", type=str, required=True, 
                       help="è¨“ç·´é›†GPS CSVï¼ˆç”¨æ–¼è¨ˆç®—æ­£è¦åŒ–åƒæ•¸ï¼‰")
    parser.add_argument("--val-gps-csv", type=str, required=True,
                       help="é©—è­‰é›†GPS CSVï¼ˆç”¨æ–¼è¨ˆç®—æ­£è¦åŒ–åƒæ•¸ï¼‰") 
    parser.add_argument("--gps-norm-method", type=str, default="minmax",
                       choices=["minmax", "zscore"],
                       help="GPSæ­£è¦åŒ–æ–¹æ³•ï¼ˆå¿…é ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰")
    
    # æ¨¡å‹åƒæ•¸ï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    parser.add_argument("--model-size", type=str, default="b0", choices=["b0", "b1", "b2"])
    parser.add_argument("--feature-dim", type=int, default=512)
    parser.add_argument("--fusion-method", type=str, default="attention", 
                       choices=["add", "concat", "attention"])
    parser.add_argument("--memory-size", type=int, default=20)
    parser.add_argument("--spatial-radius", type=float, default=0.00005)
    
    # æ¸¬è©¦åƒæ•¸
    parser.add_argument("--save-dir", type=str, default=None, help="çµæœä¿å­˜ç›®éŒ„")
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--num-workers", type=int, default=0)
    parser.add_argument("--max-len", type=int, default=None)
    
    # æ¨ç†åƒæ•¸
    parser.add_argument("--use-slide-inference", action="store_true", default=True,
                       help="ä½¿ç”¨æ»‘çª—æ¨ç†")
    
    return parser.parse_args()


def main(args: Namespace):
    print("ğŸ§ª è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer æ¸¬è©¦é–‹å§‹")
    print("=" * 60)
    
    # åœ–åƒå°ºå¯¸è¨­å®š
    image_size = 1080, 1920  # H x W
    crop_size = 512, 512
    stride = 384, 384
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥é¡åˆ¥å®šç¾©
    categories = Category.load(args.category_csv)
    num_categories = Category.get_num_categories(categories)
    print(f"ğŸ“Š é¡åˆ¥æ•¸é‡: {num_categories}")
    
    # è¨­ç½®GPSæ­£è¦åŒ–ï¼ˆèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
    print(f"ğŸŒ è¨­ç½®GPSæ­£è¦åŒ– (æ–¹æ³•: {args.gps_norm_method})")
    gps_normalizer = setup_gps_normalization(
        args.train_gps_csv, 
        args.val_gps_csv,
        method=args.gps_norm_method
    )
    
    # å‰µå»ºæ¨¡å‹
    print(f"ğŸš€ å‰µå»ºè¨˜æ†¶å¢å¼·ç‰ˆæ¨¡å‹...")
    model = create_memory_enhanced_geo_segformer(
        num_classes=num_categories,
        model_size=args.model_size,
        feature_dim=args.feature_dim,
        fusion_method=args.fusion_method,
        memory_size=args.memory_size,
        spatial_radius=args.spatial_radius,
        memory_save_path=None  # æ¸¬è©¦æ™‚ä¸ä¿å­˜è¨˜æ†¶åº«
    ).to(device)
    
    # è¼‰å…¥æª¢æŸ¥é»
    print(f"ğŸ“‚ è¼‰å…¥æª¢æŸ¥é»: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint["model"])
    model.eval()
    
    # é¡¯ç¤ºæ¨¡å‹åƒæ•¸çµ±è¨ˆ
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œåƒæ•¸é‡: {total_params:.2f}M")
    
    # å¦‚æœæª¢æŸ¥é»ä¸­æœ‰è¨˜æ†¶åº«çµ±è¨ˆï¼Œé¡¯ç¤ºè¨“ç·´æ™‚çš„çµ±è¨ˆ
    if "memory_stats" in checkpoint:
        memory_stats = checkpoint["memory_stats"]
        print(f"ğŸ“ˆ è¨“ç·´æ™‚è¨˜æ†¶åº«çµ±è¨ˆ:")
        print(f"  ç¸½ä½ç½®æ•¸: {memory_stats.get('total_locations', 'N/A')}")
        print(f"  ç¸½è¨˜æ†¶æ•¸: {memory_stats.get('total_memories', 'N/A')}")
        print(f"  å‘½ä¸­ç‡: {memory_stats.get('hit_rate', 'N/A'):.4f}")
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    if args.save_dir and not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
        print(f"ğŸ“ å‰µå»ºä¿å­˜ç›®éŒ„: {args.save_dir}")
    
    # æ•¸æ“šè®Šæ›ï¼ˆèˆ‡è¨“ç·´æ™‚çš„é©—è­‰é›†è®Šæ›ä¸€è‡´ï¼‰
    transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,  # â­ é‡è¦ï¼šå¿…é ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´
        transform.Resize(image_size),
        transform.Normalize(),
    ]
    
    # å‰µå»ºæ¨ç†å™¨
    if args.use_slide_inference:
        inferencer = GeoSlideInferencer(crop_size, stride, num_categories)
        print(f"ğŸ” ä½¿ç”¨æ»‘çª—æ¨ç† (crop: {crop_size}, stride: {stride})")
    else:
        # å¦‚æœä¸ä½¿ç”¨æ»‘çª—æ¨ç†ï¼Œç›´æ¥èª¿ç”¨æ¨¡å‹
        inferencer = None
        print(f"ğŸ” ä½¿ç”¨ç›´æ¥æ¨ç†")
    
    # å¯è¦–åŒ–å·¥å…·
    if args.save_dir:
        visualizer = IdMapVisualizer(categories)
        img_saver = ImgSaver(args.save_dir, visualizer)
    
    # æå¤±å‡½æ•¸å’Œè©•ä¼°æŒ‡æ¨™
    criterion = torch.nn.CrossEntropyLoss().to(device)
    metrics = Metrics(num_categories, nan_to_num=0)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
    print(f"ğŸ“ è¼‰å…¥æ¸¬è©¦æ•¸æ“šé›†...")
    test_dataset = MemoryEnhancedGeoSegDataset(
        transforms=transforms,
        img_dir=args.img_dir,
        ann_dir=args.ann_dir,
        gps_csv=args.test_gps_csv,
        max_len=args.max_len,
    )
    
    test_dataloader = test_dataset.get_loader(
        batch_size=args.batch_size, 
        pin_memory=False, 
        num_workers=args.num_workers
    )
    
    print(f"ğŸ“Š æ¸¬è©¦æ•¸æ“š: {len(test_dataset)} å¼µå½±åƒ")
    
    # é–‹å§‹æ¸¬è©¦
    print(f"\nğŸ§ª é–‹å§‹æ¸¬è©¦...")
    with Progress() as prog:
        with torch.no_grad():
            task = prog.add_task("Testing", total=len(test_dataloader))
            avg_loss = 0
            memory_weight_sum = 0
            
            for batch_idx, data in enumerate(test_dataloader):
                img = data["img"].to(device)
                ann = data["ann"].to(device)[:, 0, :, :]
                gps = data["gps"].to(device)
                
                # æ¨ç†
                if inferencer is not None:
                    # æ»‘çª—æ¨ç†
                    pred = inferencer.inference(model, img, gps)
                else:
                    # ç›´æ¥æ¨ç†
                    outputs = model(img, gps, return_embeddings=False, update_memory=False)
                    pred = outputs['segmentation_logits']
                    memory_weight_sum += outputs.get('memory_weight', 0)
                
                # è¨ˆç®—æå¤±
                loss = criterion(pred, ann)
                avg_loss += loss.item()
                
                # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
                metrics.compute_and_accum(pred.argmax(1), ann)
                
                # ä¿å­˜çµæœ
                if args.save_dir:
                    filenames = data.get("filename", [f"test_{batch_idx}"])
                    for i, (filename, p) in enumerate(zip(filenames, pred)):
                        save_name = f"{filename}_pred.png"
                        img_saver.save_pred(p[None, :], save_name)
                
                prog.update(task, advance=1)
            
            # ç²å–æœ€çµ‚çµæœ
            result = metrics.get_and_reset()
            avg_loss /= len(test_dataloader)
            avg_memory_weight = memory_weight_sum / len(test_dataloader) if inferencer is None else 0
            
            prog.remove_task(task)
    
    # å‰µå»ºçµæœè¡¨æ ¼
    print(f"\nğŸ“Š æ¸¬è©¦çµæœ:")
    table = Table()
    table.add_column("Category")
    table.add_column("Acc")
    table.add_column("IoU")
    table.add_column("Dice")
    table.add_column("Fscore")
    table.add_column("Precision")
    table.add_column("Recall")
    
    for cat, acc, iou, dice, fs, pre, rec in zip(
        categories,
        result["Acc"],
        result["IoU"],
        result["Dice"],
        result["Fscore"],
        result["Precision"],
        result["Recall"],
    ):
        table.add_row(
            cat.name,
            "{:.5f}".format(acc),
            "{:.5f}".format(iou),
            "{:.5f}".format(dice),
            "{:.5f}".format(fs),
            "{:.5f}".format(pre),
            "{:.5f}".format(rec),
        )
    
    # æ·»åŠ å¹³å‡å€¼
    table.add_row(
        "Avg.",
        "{:.5f}".format(result["Acc"].mean()),
        "{:.5f}".format(result["IoU"].mean()),
        "{:.5f}".format(result["Dice"].mean()),
        "{:.5f}".format(result["Fscore"].mean()),
        "{:.5f}".format(result["Precision"].mean()),
        "{:.5f}".format(result["Recall"].mean()),
    )
    
    print(table)
    print(f"\nğŸ“ˆ ç¸½é«”æŒ‡æ¨™:")
    print(f"  å¹³å‡æå¤±: {avg_loss:.5f}")
    print(f"  å¹³å‡IoU: {result['IoU'].mean():.5f}")
    print(f"  å¹³å‡Dice: {result['Dice'].mean():.5f}")
    if avg_memory_weight > 0:
        print(f"  å¹³å‡è¨˜æ†¶æ¬Šé‡: {avg_memory_weight:.4f}")
    
    # ç²å–æ¸¬è©¦æ™‚è¨˜æ†¶åº«çµ±è¨ˆ
    test_memory_stats = model.get_memory_stats()
    print(f"\nğŸ§  æ¸¬è©¦æ™‚è¨˜æ†¶åº«çµ±è¨ˆ:")
    print(f"  ç¸½ä½ç½®æ•¸: {test_memory_stats['total_locations']}")
    print(f"  ç¸½è¨˜æ†¶æ•¸: {test_memory_stats['total_memories']}")
    print(f"  å‘½ä¸­ç‡: {test_memory_stats['hit_rate']:.4f}")
    print(f"  ç¸½æŸ¥è©¢æ•¸: {test_memory_stats['total_queries']}")
    
    print(f"\nğŸ‰ è¨˜æ†¶å¢å¼·ç‰ˆ GeoSegformer æ¸¬è©¦å®Œæˆï¼")
    if args.save_dir:
        print(f"ğŸ“ çµæœå·²ä¿å­˜åˆ°: {args.save_dir}")


if __name__ == "__main__":
    args = parse_args()
    main(args)