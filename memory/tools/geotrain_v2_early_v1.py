import os
import json
import csv
import copy
import numpy as np
import pandas as pd
from argparse import ArgumentParser, Namespace
from pathlib import Path
from collections import OrderedDict
from typing import Dict, List, Tuple, Any

import torch
import torch.nn.functional as F
from rich import print
from rich.progress import Progress
from rich.table import Table

import engine.transform as transform
from engine.category import Category
from engine.dataloading import ImgAnnDataset
from engine.logger import Logger
from engine.metric import Metrics
from engine.visualizer import IdMapVisualizer, ImgSaver
from engine.geo_v2 import MemoryEnhancedGeoSegformer, create_memory_enhanced_geo_segformer


def set_seed(seed=42):
    """Ë®≠ÂÆöÊâÄÊúâÈö®Ê©üÁ®ÆÂ≠ê"""
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class EarlyStopping:
    """Early stopping utility class."""
    
    def __init__(self, patience=10, min_delta=0, monitor='mIoU', mode='max'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.mode = mode
        self.wait = 0
        self.best_score = None
        self.early_stop = False
        
        if mode == 'max':
            self.monitor_op = lambda current, best: current > best + min_delta
            self.best_score = float('-inf')
        else:
            self.monitor_op = lambda current, best: current < best - min_delta
            self.best_score = float('inf')
    
    def __call__(self, current_score):
        if self.monitor_op(current_score, self.best_score):
            self.best_score = current_score
            self.wait = 0
            return True  # Improvement detected
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.early_stop = True
            return False  # No improvement


def save_best_metrics_to_csv(categories, result, epoch, score, logdir):
    """Save the best metrics to CSV file."""
    csv_file = os.path.join(logdir, "best_metrics_per_category.csv")
    
    # Prepare data for CSV
    data = []
    for i, cat in enumerate(categories):
        row = {
            'Category_ID': cat.id,
            'Category_Name': cat.name,
            'Category_Abbr': cat.abbr,
            'Accuracy': float(result["Acc"][i]),
            'IoU': float(result["IoU"][i]),
            'Dice': float(result["Dice"][i]),
            'Fscore': float(result["Fscore"][i]),
            'Precision': float(result["Precision"][i]),
            'Recall': float(result["Recall"][i]),
            'Epoch': epoch,
            'Best_Score': float(score)
        }
        data.append(row)
    
    # Add average row
    avg_row = {
        'Category_ID': 'Average',
        'Category_Name': 'Average',
        'Category_Abbr': 'Avg',
        'Accuracy': float(result["Acc"].mean()),
        'IoU': float(result["IoU"].mean()),
        'Dice': float(result["Dice"].mean()),
        'Fscore': float(result["Fscore"].mean()),
        'Precision': float(result["Precision"].mean()),
        'Recall': float(result["Recall"].mean()),
        'Epoch': epoch,
        'Best_Score': float(score)
    }
    data.append(avg_row)
    
    # Write to CSV
    fieldnames = ['Category_ID', 'Category_Name', 'Category_Abbr', 'Accuracy', 
                  'IoU', 'Dice', 'Fscore', 'Precision', 'Recall', 'Epoch', 'Best_Score']
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
    
    print(f"‚úÖ Best metrics saved to: {csv_file}")


def save_args_to_file(args: Namespace, logdir: str):
    """Save training arguments to a text file."""
    args_dict = vars(args)
    args_file = os.path.join(logdir, "training_args.txt")
    
    with open(args_file, 'w') as f:
        f.write("Training Arguments\n")
        f.write("=" * 50 + "\n\n")
        
        for key, value in args_dict.items():
            f.write(f"{key}: {value}\n")
        
        f.write(f"\nTraining started at: {__import__('datetime').datetime.now()}\n")


def create_training_summary_file(args, logdir: str):
    """üÜï ÂâµÂª∫Ë®ìÁ∑¥ÈÖçÁΩÆÊëòË¶ÅÊñá‰ª∂"""
    summary_file = os.path.join(logdir, "training_summary.json")
    
    summary_data = {
        'model_config': {
            'model_size': args.model_size,
            'feature_dim': args.feature_dim,
            'fusion_method': args.fusion_method,
            'memory_size': args.memory_size,
            'spatial_radius': args.spatial_radius,
            'memory_enabled': args.memory_size > 0,  # üÜï Ë®òÈåÑË®òÊÜ∂Â∫´ÁãÄÊÖã
        },
        'training_config': {
            'batch_size': args.batch_size,
            'max_epochs': args.max_epochs,
            'lr_backbone': args.lr_backbone,
            'lr_head': args.lr_head,
            'lr_gps': args.lr_gps,
            'seg_weight': args.seg_weight,
            'contrastive_weight': args.contrastive_weight,
        },
        'data_paths': {
            'train_img_dir': args.train_img_dir,
            'train_ann_dir': args.train_ann_dir,
            'val_img_dir': args.val_img_dir,
            'val_ann_dir': args.val_ann_dir,
            'train_gps_csv': args.train_gps_csv,
            'val_gps_csv': args.val_gps_csv,
            'category_csv': args.category_csv,
        },
        'memory_config': {
            'memory_warmup_epochs': args.memory_warmup_epochs,
            'save_memory_stats': args.save_memory_stats,
        },
        'training_info': {
            'seed': getattr(args, 'seed', 42),
            'monitor': getattr(args, 'monitor', 'mIoU'),
            'early_stop': getattr(args, 'early_stop', False),
            'patience': getattr(args, 'patience', 10),
        }
    }
    
    with open(summary_file, 'w') as f:
        json.dump(summary_data, f, indent=2)
    
    print(f"üìÑ Training summary saved to: {summary_file}")
    return summary_file


def save_checkpoint_geo(model, optimizer, warmup_scheduler, poly_scheduler, epoch, 
                       best_score, filepath, is_best=False, keep_only_best=False, 
                       memory_stats=None, is_warmup=False, args=None):
    """üÜï Â¢ûÂº∑ÁâàÊ™¢Êü•Èªû‰øùÂ≠òÔºåÂåÖÂê´Ê®°ÂûãÈÖçÁΩÆ‰ø°ÊÅØ"""
    checkpoint = {
        'epoch': epoch,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'best_score': best_score,
        'memory_stats': memory_stats or {}
    }
    
    # üÜï ‰øùÂ≠òÊ®°ÂûãÈÖçÁΩÆÂèÉÊï∏
    if args:
        checkpoint['args'] = args
        checkpoint['model_config'] = {
            'model_size': getattr(args, 'model_size', 'b0'),
            'feature_dim': getattr(args, 'feature_dim', 512),
            'fusion_method': getattr(args, 'fusion_method', 'attention'),
            'memory_size': getattr(args, 'memory_size', 20),
            'spatial_radius': getattr(args, 'spatial_radius', 0.00005),
            'num_classes': model.num_classes if hasattr(model, 'num_classes') else None,
            'memory_enabled': args.memory_size > 0,  # üÜï Ë®òÈåÑË®òÊÜ∂Â∫´ÁãÄÊÖã
        }
    
    # Ê†πÊìöÊòØÂê¶Âú®warmupÈöéÊÆµÈÅ∏ÊìáË™øÂ∫¶Âô®
    if is_warmup:
        checkpoint['warmup_scheduler'] = warmup_scheduler.state_dict() if warmup_scheduler else None
    else:
        checkpoint['poly_scheduler'] = poly_scheduler.state_dict() if poly_scheduler else None
    
    if keep_only_best:
        # Âè™Âú®ÊòØÊúÄ‰Ω≥Ê®°ÂûãÊôÇÊâç‰øùÂ≠ò
        if is_best:
            best_filepath = filepath.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_filepath)
            print(f"üíæ Saved best model: {best_filepath}")
    else:
        # Ê≠£Â∏∏‰øùÂ≠òÈÇèËºØ
        torch.save(checkpoint, filepath)
        
        if is_best:
            best_filepath = filepath.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_filepath)


def ensure_memory_bank_availability(model, args, epoch):
    """üÜï Á¢∫‰øùË®òÊÜ∂Â∫´Âú®ÈóúÈçµÊôÇÂàªË¢´‰øùÂ≠ò"""
    
    # üÜï Ê™¢Êü•Ë®òÊÜ∂Â∫´ÊòØÂê¶ÂïüÁî®
    if args.memory_size == 0:
        return  # Ë®òÊÜ∂Â∫´Á¶ÅÁî®ÊôÇË∑≥ÈÅé
    
    # Âú®Ë®òÊÜ∂È†êÁÜ±ÈöéÊÆµÁµêÊùüÊôÇ‰øùÂ≠ò
    if epoch == args.memory_warmup_epochs:
        print(f"üß† Memory warmup completed at epoch {epoch}, saving memory bank...")
        model.save_memory_bank()
    
    # ÊØè50ÂÄãepoch‰øùÂ≠ò‰∏ÄÊ¨°
    if epoch % 50 == 0:
        print(f"üîÑ Periodic memory bank save at epoch {epoch}...")
        model.save_memory_bank()


class ContrastiveLoss(torch.nn.Module):
    """
    Â∞çÊØîÂ≠∏ÁøíÊêçÂ§±ÔºåÁî®ÊñºÂ∞çÈΩäÂΩ±ÂÉèÂíå GPS ÁâπÂæµ
    """
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
        
    def forward(self, image_embeds: torch.Tensor, location_embeds: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_embeds: Image embeddings, shape (batch_size, embed_dim)
            location_embeds: Location embeddings, shape (batch_size, embed_dim)
        Returns:
            Contrastive loss value
        """
        if image_embeds is None or location_embeds is None:
            return torch.tensor(0.0, device=image_embeds.device if image_embeds is not None else 'cpu')
        
        batch_size = image_embeds.shape[0]
        
        if image_embeds.shape != location_embeds.shape:
            return torch.tensor(0.0, device=image_embeds.device)
        
        if torch.isnan(image_embeds).any() or torch.isnan(location_embeds).any():
            return torch.tensor(0.0, device=image_embeds.device)
        
        # Ê≠£Ë¶èÂåñÂµåÂÖ•
        image_embeds = F.normalize(image_embeds, dim=-1)
        location_embeds = F.normalize(location_embeds, dim=-1)
        
        # Ë®àÁÆóÁõ∏‰ººÂ∫¶Áü©Èô£
        similarity = torch.matmul(image_embeds, location_embeds.T) / self.temperature
        
        # ÂâµÂª∫Ê®ôÁ±§ÔºàÂ∞çËßíÁ∑öÁÇ∫Ê≠£Ê®£Êú¨Ôºâ
        labels = torch.arange(batch_size, device=image_embeds.device)
        
        # ÈõôÂêëÂ∞çÊØîÊêçÂ§±
        loss_i2l = F.cross_entropy(similarity, labels)
        loss_l2i = F.cross_entropy(similarity.T, labels)
        
        final_loss = (loss_i2l + loss_l2i) / 2
        
        if torch.isnan(final_loss):
            return torch.tensor(0.0, device=image_embeds.device)
        
        return final_loss


class MemoryAwareContrastiveLoss(torch.nn.Module):
    """
    Ë®òÊÜ∂ÊÑüÁü•ÁöÑÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±
    ËÄÉÊÖÆÁõ∏‰ºº‰ΩçÁΩÆÁöÑÊ®£Êú¨‰∏çÊáâË©≤Ë¢´Âº∑Âà∂ÂàÜÈõ¢
    """
    def __init__(self, temperature: float = 0.07, spatial_threshold: float = 0.0001):
        super().__init__()
        self.temperature = temperature
        self.spatial_threshold = spatial_threshold
        
    def forward(self, image_embeds: torch.Tensor, location_embeds: torch.Tensor, gps_coords: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_embeds: Image embeddings, shape (batch_size, embed_dim)
            location_embeds: Location embeddings, shape (batch_size, embed_dim)
            gps_coords: GPS coordinates, shape (batch_size, 2)
        Returns:
            Memory-aware contrastive loss value
        """
        if image_embeds is None or location_embeds is None:
            return torch.tensor(0.0, device=image_embeds.device if image_embeds is not None else 'cpu')
        
        batch_size = image_embeds.shape[0]
        
        # Ë®àÁÆóGPSË∑ùÈõ¢Áü©Èô£
        gps_distances = torch.cdist(gps_coords, gps_coords)
        
        # Ê≠£Ë¶èÂåñÂµåÂÖ•
        image_embeds = F.normalize(image_embeds, dim=-1)
        location_embeds = F.normalize(location_embeds, dim=-1)
        
        # Ë®àÁÆóÁõ∏‰ººÂ∫¶Áü©Èô£
        similarity = torch.matmul(image_embeds, location_embeds.T) / self.temperature
        
        total_loss = 0
        valid_samples = 0
        
        for i in range(batch_size):
            # ÊâæÂà∞Ë∑ùÈõ¢ËºÉÈÅ†ÁöÑË≤†Ê®£Êú¨ÔºàÈÅøÂÖçÁõ∏Ëøë‰ΩçÁΩÆË¢´Âº∑Âà∂ÂàÜÈõ¢Ôºâ
            far_mask = gps_distances[i] > self.spatial_threshold
            neg_indices = torch.where(far_mask)[0]
            
            if len(neg_indices) > 0:
                # Ê≠£Ê®£Êú¨ÔºàËá™Â∑±Ôºâ
                pos_sim = similarity[i, i]
                
                # Ë≤†Ê®£Êú¨ÔºàË∑ùÈõ¢ËºÉÈÅ†ÁöÑ‰ΩçÁΩÆÔºâ
                neg_sims = similarity[i, neg_indices]
                
                # Â∞çÊØîÊêçÂ§±
                logits = torch.cat([pos_sim.unsqueeze(0), neg_sims])
                labels = torch.zeros(1, dtype=torch.long, device=image_embeds.device)
                loss = F.cross_entropy(logits.unsqueeze(0), labels)
                
                total_loss += loss
                valid_samples += 1
        
        return total_loss / max(valid_samples, 1)


class MemoryEnhancedGeoSegDataset(ImgAnnDataset):
    """
    Ë®òÊÜ∂Â¢ûÂº∑ÁâàÊï∏ÊìöÈõÜ
    """
    def __init__(
        self,
        transforms: list,
        img_dir: str,
        ann_dir: str,
        gps_csv: str,
        max_len: int = None,
    ):
        super().__init__(transforms, img_dir, ann_dir, max_len)
        
        # ËºâÂÖ• GPS Êï∏Êìö
        self.gps_data = pd.read_csv(gps_csv)
        
        # ÂâµÂª∫Ê™îÂêçÂà∞ GPS ÁöÑÊò†Â∞Ñ
        self.filename_to_gps = {}
        for _, row in self.gps_data.iterrows():
            filename = os.path.splitext(row['filename'])[0]
            self.filename_to_gps[filename] = [row['lat'], row['long']]
        
        print(f"‚úÖ Loaded GPS data for {len(self.filename_to_gps)} images")
        
        # ÂàÜÊûêGPSÊï∏ÊìöÂàÜ‰Ωà
        self.analyze_gps_distribution()
    
    def analyze_gps_distribution(self):
        """ÂàÜÊûêGPSÊï∏ÊìöÂàÜ‰Ωà"""
        lats = [coords[0] for coords in self.filename_to_gps.values()]
        lons = [coords[1] for coords in self.filename_to_gps.values()]
        
        print(f"üìä GPSÊï∏ÊìöÂàÜÊûê:")
        print(f"  Á∑ØÂ∫¶ÁØÑÂúç: [{min(lats):.6f}, {max(lats):.6f}] (ÁØÑÂúç: {max(lats)-min(lats):.6f})")
        print(f"  Á∂ìÂ∫¶ÁØÑÂúç: [{min(lons):.6f}, {max(lons):.6f}] (ÁØÑÂúç: {max(lons)-min(lons):.6f})")
        
        # Ë®àÁÆóÈáçË§á‰ΩçÁΩÆ
        unique_positions = set()
        duplicate_count = 0
        for coords in self.filename_to_gps.values():
            coord_str = f"{coords[0]:.7f},{coords[1]:.7f}"
            if coord_str in unique_positions:
                duplicate_count += 1
            else:
                unique_positions.add(coord_str)
        
        duplicate_rate = duplicate_count / len(self.filename_to_gps) * 100
        print(f"  ÂîØ‰∏Ä‰ΩçÁΩÆÊï∏: {len(unique_positions)}")
        print(f"  ÈáçË§á‰ΩçÁΩÆÁéá: {duplicate_rate:.2f}%")
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        # Áç≤ÂèñÂéüÂßãÊï∏Êìö
        data = super().__getitem__(idx)
        
        # ÂæûË∑ØÂæë‰∏≠ÊèêÂèñÊ™îÂêç
        img_path = self.img_ann_paths[idx][0]
        filename = os.path.splitext(os.path.basename(img_path))[0]
        
        # Ê∑ªÂä† GPS Êï∏ÊìöÂíåÊ™îÂêç
        if filename in self.filename_to_gps:
            gps_coords = self.filename_to_gps[filename]
            data['gps'] = torch.tensor(gps_coords, dtype=torch.float32)
        else:
            print(f"‚ö†Ô∏è Warning: No GPS data found for {filename}")
            data['gps'] = torch.zeros(2, dtype=torch.float32)
        
        # Ê∑ªÂä†Ê™îÂêçÁî®ÊñºËøΩËπ§
        data['filename'] = filename
        
        return data


def setup_gps_normalization(train_gps_csv: str, val_gps_csv: str, method: str = "minmax"):
    """
    Ë®≠ÁΩÆGPSÊ≠£Ë¶èÂåñ
    """
    # Âêà‰ΩµË®ìÁ∑¥ÂíåÈ©óË≠âÈõÜË®àÁÆóÂÖ®Â±ÄÁµ±Ë®à
    train_gps = pd.read_csv(train_gps_csv)
    val_gps = pd.read_csv(val_gps_csv)
    all_gps = pd.concat([train_gps, val_gps], ignore_index=True)
    
    if method == "minmax":
        lat_min = all_gps['lat'].min()
        lat_max = all_gps['lat'].max()
        lon_min = all_gps['long'].min()
        lon_max = all_gps['long'].max()
        
        # Ê∑ªÂä†Â∞èÈáèpaddingÈÅøÂÖçÈÇäÁïåÂïèÈ°å
        lat_range = lat_max - lat_min
        lon_range = lon_max - lon_min
        padding = 0.01  # 1%ÁöÑpadding
        
        lat_min -= lat_range * padding
        lat_max += lat_range * padding
        lon_min -= lon_range * padding
        lon_max += lon_range * padding
        
        return transform.GPSNormalize(
            lat_range=(lat_min, lat_max),
            lon_range=(lon_min, lon_max)
        )
    else:
        raise ValueError(f"Method {method} not implemented yet")


def parse_args() -> Namespace:
    parser = ArgumentParser(description="Enhanced Memory-Enhanced GeoSegformer Training")
    
    # Êï∏ÊìöË∑ØÂæë
    parser.add_argument("train_img_dir", type=str)
    parser.add_argument("train_ann_dir", type=str)
    parser.add_argument("val_img_dir", type=str)  
    parser.add_argument("val_ann_dir", type=str)
    parser.add_argument("category_csv", type=str)
    parser.add_argument("train_gps_csv", type=str)
    parser.add_argument("val_gps_csv", type=str)
    parser.add_argument("max_epochs", type=int)
    parser.add_argument("logdir", type=str)
    
    # Ê®°ÂûãÂèÉÊï∏
    parser.add_argument("--model-size", type=str, default="b0", choices=["b0", "b1", "b2"])
    parser.add_argument("--feature-dim", type=int, default=512)
    parser.add_argument("--fusion-method", type=str, default="attention", 
                       choices=["add", "concat", "attention"])
    
    # üÜï Ë®òÊÜ∂Â∫´ÂèÉÊï∏ - ÊîØÊåÅË®≠ÁÇ∫0Á¶ÅÁî®
    parser.add_argument("--memory-size", type=int, default=20, help="Memory size per location (set to 0 to disable memory bank)")
    parser.add_argument("--spatial-radius", type=float, default=0.00005, help="Spatial radius for memory")
    parser.add_argument("--gps-norm-method", type=str, default="minmax", 
                       choices=["minmax", "zscore"], 
                       help="GPS normalization method")
    
    # ÊêçÂ§±Ê¨äÈáç
    parser.add_argument("--seg-weight", type=float, default=1.0)
    parser.add_argument("--contrastive-weight", type=float, default=0.05, help="Contrastive loss weight")
    parser.add_argument("--temperature", type=float, default=0.07)
    parser.add_argument("--spatial-threshold", type=float, default=0.15, help="GPS distance threshold")
    
    # Ë®ìÁ∑¥ÂèÉÊï∏
    parser.add_argument("--save-interval", type=int, default=1)
    parser.add_argument("--checkpoint-interval", type=int, default=10)
    parser.add_argument("--val-interval", type=int, default=1)
    parser.add_argument("--batch-size", type=int, default=2)
    parser.add_argument("--num-workers", type=int, default=0)
    parser.add_argument("--train-max-len", type=int, default=None)
    parser.add_argument("--val-max-len", type=int, default=None)
    parser.add_argument("--pin-memory", action="store_true", default=False)
    parser.add_argument("--resume", type=int, default=0)
    
    # üÜï Ë®òÊÜ∂Â∫´Áõ∏Èóú
    parser.add_argument("--memory-warmup-epochs", type=int, default=3, help="Epochs to warm up memory bank")
    parser.add_argument("--save-memory-stats", action="store_true", default=True, help="Save memory bank statistics")
    
    # üÜï Êó©ÂÅúÁõ∏ÈóúÂèÉÊï∏
    parser.add_argument("--early-stop", action="store_true", default=False, 
                       help="Enable early stopping")
    parser.add_argument("--patience", type=int, default=10, 
                       help="Number of epochs to wait for improvement")
    parser.add_argument("--min-delta", type=float, default=0.001, 
                       help="Minimum change to qualify as improvement")
    parser.add_argument("--monitor", type=str, default="mIoU", 
                       choices=["mIoU", "loss"], help="Metric to monitor for early stopping")
    
    # üÜï Ê™îÊ°à‰øùÂ≠òÈÅ∏È†Ö
    parser.add_argument("--keep-only-best", action="store_true", default=False,
                       help="Only save the best model checkpoint to save disk space")
    parser.add_argument("--save-last-checkpoint", action="store_true", default=False,
                       help="Also save the last checkpoint for resuming training")

    # üÜï Èö®Ê©üÁ®ÆÂ≠êÁõ∏ÈóúÂèÉÊï∏
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    
    # Â≠∏ÁøíÁéáÂèÉÊï∏
    parser.add_argument("--lr-backbone", type=float, default=6e-5, help="Learning rate for backbone")
    parser.add_argument("--lr-head", type=float, default=6e-4, help="Learning rate for decode head")
    parser.add_argument("--lr-gps", type=float, default=3e-4, help="Learning rate for GPS encoder")
    parser.add_argument("--lr-memory", type=float, default=6e-4, help="Learning rate for memory components")
    parser.add_argument("--lr-fusion", type=float, default=6e-4, help="Learning rate for fusion components")
    
    return parser.parse_args()


def main_training_logic(args: Namespace):
    """‰∏ªË¶ÅÁöÑË®ìÁ∑¥ÈÇèËºØ - ÊîØÊåÅË®òÊÜ∂Â∫´Ê∂àËûç"""
    
    image_size = 720, 1280
    crop_size = 320, 320
    stride = 240, 240
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ËºâÂÖ•È°ûÂà•ÂÆöÁæ©
    categories = Category.load(args.category_csv)
    num_categories = Category.get_num_categories(categories)
    
    # üÜï Ê™¢Êü•Ë®òÊÜ∂Â∫´Ë®≠ÁΩÆ
    memory_enabled = args.memory_size > 0
    
    if memory_enabled:
        print(f"üß† Ë®òÊÜ∂Â∫´Â∑≤ÂïüÁî®:")
        print(f"  Ë®òÊÜ∂Â∫´Â§ßÂ∞è: {args.memory_size}")
        print(f"  Á©∫ÈñìÂçäÂæë: {args.spatial_radius}")
        memory_save_path = os.path.join(args.logdir, "memory_stats.json") if args.save_memory_stats else None
    else:
        print(f"üö´ Ë®òÊÜ∂Â∫´Â∑≤Á¶ÅÁî® - ÂÉÖË®ìÁ∑¥GPSÁµÑ‰ª∂")
        memory_save_path = None
        # Áï∂Ë®òÊÜ∂Â∫´Á¶ÅÁî®ÊôÇÔºåÊüê‰∫õÂèÉÊï∏‰∏çÈáçË¶Å
        args.spatial_radius = 0.0
        args.memory_warmup_epochs = 0
        args.save_memory_stats = False
    
    # üîç ÂàÜÊûêGPSÊï∏ÊìöÔºàÂ¶ÇÊûúË®òÊÜ∂Â∫´ÂïüÁî®Ôºâ
    if memory_enabled:
        print("üîç Ë®ìÁ∑¥ÂâçGPSÊï∏ÊìöÂàÜÊûê:")
        print("=" * 50)
        from engine.geo_v2 import debug_memory_system
        debug_memory_system(args.train_gps_csv, args.spatial_radius)
        print("=" * 50)
        print()
    
    print(f"üöÄ Ë®òÊÜ∂Â¢ûÂº∑Áâà GeoSegformer Ë®ìÁ∑¥ÈÖçÁΩÆ:")
    print(f"  GPSÊ≠£Ë¶èÂåñÊñπÊ≥ï: {getattr(args, 'gps_norm_method', 'minmax')}")
    print(f"  Ê®°ÂûãÂ§ßÂ∞è: {args.model_size}")
    print(f"  ÁâπÂæµÁ∂≠Â∫¶: {args.feature_dim}")
    print(f"  Ë®òÊÜ∂Â∫´ÁãÄÊÖã: {'ÂïüÁî®' if memory_enabled else 'Á¶ÅÁî®'}")
    print(f"  ÊâπÊ¨°Â§ßÂ∞è: {args.batch_size}")
    print(f"  ÂàÜÂâ≤Ê¨äÈáç: {args.seg_weight}")
    print(f"  Â∞çÊØîÂ≠∏ÁøíÊ¨äÈáç: {args.contrastive_weight}")
    
    # Ë®≠ÁΩÆGPSÊ≠£Ë¶èÂåñ
    gps_normalizer = setup_gps_normalization(
        args.train_gps_csv, 
        args.val_gps_csv,
        method=getattr(args, 'gps_norm_method', 'minmax')
    )
    
    # ÂâµÂª∫Ë®òÊÜ∂Â¢ûÂº∑ÁâàÊ®°Âûã
    model = create_memory_enhanced_geo_segformer(
        num_classes=num_categories,
        model_size=args.model_size,
        feature_dim=args.feature_dim,
        fusion_method=args.fusion_method,
        memory_size=args.memory_size,  # üÜï ÂèØ‰ª•ÊòØ0
        spatial_radius=args.spatial_radius,
        memory_save_path=memory_save_path
    ).to(device)
    
    print(f"‚úÖ ÂâµÂª∫Ë®òÊÜ∂Â¢ûÂº∑ÁâàÊ®°ÂûãÔºåÂèÉÊï∏Èáè: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # Êï∏ÊìöËÆäÊèõ
    train_transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,
        transform.RandomResizeCrop(image_size, (0.5, 2), crop_size),
        transform.ColorJitter(0.3, 0.3, 0.3),
        transform.Normalize(),
    ]
    
    val_transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,
        transform.Resize(image_size),
        transform.Normalize(),
    ]
    
    # ÂâµÂª∫Êï∏ÊìöÈõÜ
    train_dataset = MemoryEnhancedGeoSegDataset(
        transforms=train_transforms,
        img_dir=args.train_img_dir,
        ann_dir=args.train_ann_dir,
        gps_csv=args.train_gps_csv,
        max_len=args.train_max_len,
    )
    
    val_dataset = MemoryEnhancedGeoSegDataset(
        transforms=val_transforms,
        img_dir=args.val_img_dir,
        ann_dir=args.val_ann_dir,
        gps_csv=args.val_gps_csv,
        max_len=args.val_max_len,
    )
    
    # Êï∏ÊìöËºâÂÖ•Âô®
    train_dataloader = train_dataset.get_loader(
        batch_size=args.batch_size,
        pin_memory=args.pin_memory,
        num_workers=args.num_workers,
        shuffle=True,
        drop_last=True,
    )
    
    val_dataloader = val_dataset.get_loader(
        batch_size=1,
        pin_memory=args.pin_memory,
        num_workers=args.num_workers,
    )
    
    # ÊêçÂ§±ÂáΩÊï∏
    seg_criterion = torch.nn.CrossEntropyLoss().to(device)
    
    # üÜï Ê†πÊìöË®òÊÜ∂Â∫´ÁãÄÊÖãÈÅ∏ÊìáÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±
    if memory_enabled:
        contrastive_criterion = MemoryAwareContrastiveLoss(
            temperature=args.temperature,
            spatial_threshold=args.spatial_threshold
        ).to(device)
        print(f"‚úÖ ‰ΩøÁî®Ë®òÊÜ∂ÊÑüÁü•Â∞çÊØîÂ≠∏ÁøíÊêçÂ§±")
    else:
        contrastive_criterion = ContrastiveLoss(
            temperature=args.temperature
        ).to(device)
        print(f"‚úÖ ‰ΩøÁî®Á∞°ÂñÆÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±")
    
    # Ë©ï‰º∞ÊåáÊ®ô
    metrics = Metrics(num_categories, nan_to_num=0)
    
    # üÜï Ê†πÊìöÊ®°ÂûãÁµÑ‰ª∂Ë®≠ÁΩÆÂÑ™ÂåñÂô®
    optimizer_params = [
        {"params": model.image_encoder.parameters(), "lr": args.lr_backbone},
        {"params": model.location_encoder.parameters(), "lr": args.lr_gps},
        {"params": model.cross_modal_fusion.parameters(), "lr": args.lr_fusion},
        {"params": model.segmentation_head.parameters(), "lr": args.lr_head},
        {"params": model.contrastive_proj.parameters(), "lr": args.lr_fusion},
    ]
    
    # üîß ‰øÆÂæ©ÔºöÊ™¢Êü•Ë®òÊÜ∂Â∫´ÁµÑ‰ª∂ÊòØÂê¶Â≠òÂú®
    if memory_enabled:
        if hasattr(model, 'memory_fusion') and model.memory_fusion is not None:
            optimizer_params.append({"params": model.memory_fusion.parameters(), "lr": args.lr_memory})
            print(f"‚úÖ Ê∑ªÂä†memory_fusionÂèÉÊï∏Âà∞ÂÑ™ÂåñÂô®")
        
        if hasattr(model, 'memory_attention') and model.memory_attention is not None:
            optimizer_params.append({"params": model.memory_attention.parameters(), "lr": args.lr_memory})
            print(f"‚úÖ Ê∑ªÂä†memory_attentionÂèÉÊï∏Âà∞ÂÑ™ÂåñÂô®")
        
        print(f"‚úÖ ÂÑ™ÂåñÂô®ÂåÖÂê´Ë®òÊÜ∂Â∫´ÂèÉÊï∏")
    else:
        print(f"üö´ Ë®òÊÜ∂Â∫´Á¶ÅÁî®ÔºåÂÑ™ÂåñÂô®‰∏çÂåÖÂê´Ë®òÊÜ∂Â∫´ÂèÉÊï∏")
    
    optimizer = torch.optim.AdamW(optimizer_params)
    
    # Â≠∏ÁøíÁéáË™øÂ∫¶Âô®
    warmup_epochs = args.memory_warmup_epochs if memory_enabled else 3
    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
        optimizer, 1e-4, 1, len(train_dataloader) * warmup_epochs
    )
    poly_scheduler = torch.optim.lr_scheduler.PolynomialLR(
        optimizer, args.max_epochs, 1
    )
    
    # üÜï ÂàùÂßãÂåñÊó©ÂÅú
    early_stopping = None
    if args.early_stop:
        mode = 'max' if args.monitor == 'mIoU' else 'min'
        early_stopping = EarlyStopping(
            patience=args.patience, 
            min_delta=args.min_delta, 
            monitor=args.monitor,
            mode=mode
        )
    
    # üÜï ÂàùÂßãÂåñÊúÄ‰Ω≥ÂàÜÊï∏ËøΩËπ§
    best_score = float('-inf') if args.monitor == 'mIoU' else float('inf')
    
    # Ê™¢Êü•ÈªûÊÅ¢Âæ©
    if args.resume:
        checkpoint = torch.load(
            os.path.join(args.logdir, f"checkpoint_{args.resume}.pth")
        )
        model.load_state_dict(checkpoint["model"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        best_score = checkpoint.get('best_score', best_score)
        start_epoch = args.resume + 1
        print(f"‚úÖ Âæûepoch {args.resume}ÊÅ¢Âæ©Ë®ìÁ∑¥")
    else:
        start_epoch = 1
    
    # ÂâµÂª∫Êó•Ë™åÁõÆÈåÑ
    if not args.resume and os.path.exists(args.logdir):
        raise FileExistsError(
            f"{args.logdir} already exists. Please specify a different logdir or resume a checkpoint."
        )
    
    os.makedirs(args.logdir, exist_ok=True)
    
    # üÜï ‰øùÂ≠òË®ìÁ∑¥ÂèÉÊï∏ÂíåÈÖçÁΩÆÊëòË¶Å
    save_args_to_file(args, args.logdir)
    create_training_summary_file(args, args.logdir)
    
    logger = Logger(args.logdir)
    img_saver = ImgSaver(args.logdir, IdMapVisualizer(categories))
    
    # üÜï Ë®òÈåÑË®ìÁ∑¥ÈÖçÁΩÆ
    logger.info("Training", f"Starting training with memory bank: {memory_enabled}")
    logger.info("Training", f"Early stopping: {args.early_stop}")
    if args.early_stop:
        logger.info("Training", f"Monitoring: {args.monitor}, Patience: {args.patience}")
    if args.keep_only_best:
        logger.info("Training", "üíæ Keep only best model enabled - saving disk space")
    else:
        logger.info("Training", f"üìÅ Regular checkpoint saving every {args.checkpoint_interval} epochs")
    
    # Ë®ìÁ∑¥Âæ™Áí∞
    with Progress() as prog:
        whole_task = prog.add_task("Memory-Enhanced Training", total=args.max_epochs)
        
        for e in range(start_epoch, args.max_epochs + 1):
            train_task = prog.add_task(f"Train - {e}", total=len(train_dataloader))
            
            # Ë®ìÁ∑¥ÈöéÊÆµ
            model.train()
            train_seg_loss = 0
            train_contrastive_loss = 0
            train_total_loss = 0
            train_memory_weight = 0
            
            is_warmup = e <= warmup_epochs
            
            for batch_idx, data in enumerate(train_dataloader):
                img = data["img"].to(device)
                ann = data["ann"].to(device)[:, 0, :, :]
                gps = data["gps"].to(device)
                
                optimizer.zero_grad()
                
                # ÂâçÂêëÂÇ≥Êí≠
                outputs = model(img, gps, return_embeddings=True, update_memory=memory_enabled)
                
                # ÂàÜÂâ≤ÊêçÂ§±
                seg_loss = seg_criterion(outputs['segmentation_logits'], ann)
                
                # üÜï Ê¢ù‰ª∂ÊÄßÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±
                if memory_enabled:
                    # Ë®òÊÜ∂ÊÑüÁü•Â∞çÊØîÂ≠∏ÁøíÊêçÂ§±
                    contrastive_loss = contrastive_criterion(
                        outputs['image_embeddings'], 
                        outputs['location_embeddings'],
                        gps
                    )
                else:
                    # Á∞°ÂñÆÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±
                    contrastive_loss = contrastive_criterion(
                        outputs['image_embeddings'], 
                        outputs['location_embeddings']
                    )
                
                # Á∏ΩÊêçÂ§±
                contrastive_weight = args.contrastive_weight * (0.1 if is_warmup else 1.0)
                total_loss = (args.seg_weight * seg_loss + 
                             contrastive_weight * contrastive_loss)
                
                # ÂèçÂêëÂÇ≥Êí≠
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                if is_warmup:
                    warmup_scheduler.step()
                
                # Ë®òÈåÑÊêçÂ§±
                train_seg_loss += seg_loss.item()
                train_contrastive_loss += contrastive_loss if isinstance(contrastive_loss, float) else contrastive_loss.item()
                train_total_loss += total_loss.item()
                train_memory_weight += outputs.get('memory_weight', 0)
                
                # ÂÆöÊúüËº∏Âá∫Ë®òÊÜ∂Â∫´Áµ±Ë®à
                if batch_idx % 50 == 0 and memory_enabled:
                    memory_stats = model.get_memory_stats()
                    logger.info("Memory", 
                               f"Locations: {memory_stats['total_locations']}, "
                               f"Memories: {memory_stats['total_memories']}, "
                               f"Hit Rate: {memory_stats['hit_rate']:.3f}")
                
                prog.update(train_task, advance=1)
            
            # Ë®àÁÆóÂπ≥ÂùáÂÄº
            train_seg_loss /= len(train_dataloader)
            train_contrastive_loss /= len(train_dataloader)
            train_total_loss /= len(train_dataloader)
            train_memory_weight /= len(train_dataloader)
            
            # Ë®òÈåÑË®ìÁ∑¥ÁµêÊûú
            logger.info("TrainLoop", f"Total Loss: {train_total_loss:.5f}")
            logger.info("TrainLoop", f"Seg Loss: {train_seg_loss:.5f}")
            logger.info("TrainLoop", f"Contrastive Loss: {train_contrastive_loss:.5f}")
            if memory_enabled:
                logger.info("TrainLoop", f"Memory Weight: {train_memory_weight:.4f}")
            
            logger.tb_log("TrainLoop/TotalLoss", train_total_loss, e)
            logger.tb_log("TrainLoop/SegLoss", train_seg_loss, e)
            logger.tb_log("TrainLoop/ContrastiveLoss", train_contrastive_loss, e)
            if memory_enabled:
                logger.tb_log("TrainLoop/MemoryWeight", train_memory_weight, e)
            
            # ‰øùÂ≠òË®ìÁ∑¥Ê®£Êú¨
            if e % args.save_interval == 0:
                img_saver.save_img(img, f"train_{e}_img.png")
                img_saver.save_ann(ann, f"train_{e}_ann.png")
                img_saver.save_pred(outputs['segmentation_logits'], f"train_{e}_pred.png")
            
            prog.remove_task(train_task)
            
            # üÜï Ë®òÊÜ∂Â∫´ÂÆöÊúü‰øùÂ≠òÔºàÂÉÖÁï∂Ë®òÊÜ∂Â∫´ÂïüÁî®ÊôÇÔºâ
            if memory_enabled:
                ensure_memory_bank_availability(model, args, e)
            
            # È©óË≠âÈöéÊÆµ
            if e % args.val_interval == 0:
                with torch.no_grad():
                    val_task = prog.add_task(f"Val - {e}", total=len(val_dataloader))
                    model.eval()
                    
                    val_seg_loss = 0
                    val_contrastive_loss = 0
                    val_total_loss = 0
                    val_memory_weight = 0
                    
                    for data in val_dataloader:
                        img = data["img"].to(device)
                        ann = data["ann"].to(device)[:, 0, :, :]
                        gps = data["gps"].to(device)
                        
                        # Êé®ÁêÜÔºà‰∏çÊõ¥Êñ∞Ë®òÊÜ∂Â∫´Ôºâ
                        outputs = model(img, gps, return_embeddings=True, update_memory=False)
                        pred = outputs['segmentation_logits']
                        
                        # Ë®àÁÆóÊêçÂ§±
                        seg_loss = seg_criterion(pred, ann)
                        
                        # üÜï Ê¢ù‰ª∂ÊÄßÂ∞çÊØîÂ≠∏ÁøíÊêçÂ§±
                        if memory_enabled:
                            contrastive_loss = contrastive_criterion(
                                outputs['image_embeddings'], 
                                outputs['location_embeddings'],
                                gps
                            )
                        else:
                            contrastive_loss = contrastive_criterion(
                                outputs['image_embeddings'], 
                                outputs['location_embeddings']
                            )
                        
                        total_loss = (args.seg_weight * seg_loss + 
                                     args.contrastive_weight * contrastive_loss)
                        
                        val_seg_loss += seg_loss.item()
                        val_contrastive_loss += contrastive_loss if isinstance(contrastive_loss, float) else contrastive_loss.item()
                        val_total_loss += total_loss.item()
                        val_memory_weight += outputs.get('memory_weight', 0)
                        
                        # Ë®àÁÆóË©ï‰º∞ÊåáÊ®ô
                        metrics.compute_and_accum(pred.argmax(1), ann)
                        
                        prog.update(val_task, advance=1)
                    
                    # Âπ≥ÂùáÊêçÂ§±
                    val_seg_loss /= len(val_dataloader)
                    val_contrastive_loss /= len(val_dataloader)
                    val_total_loss /= len(val_dataloader)
                    val_memory_weight /= len(val_dataloader)
                    
                    # ‰øùÂ≠òÈ©óË≠âÊ®£Êú¨
                    img_saver.save_img(img, f"val_{e}_img.png")
                    img_saver.save_ann(ann, f"val_{e}_ann.png")
                    img_saver.save_pred(pred, f"val_{e}_pred.png")
                    
                    # Áç≤ÂèñË©ï‰º∞ÁµêÊûú
                    result = metrics.get_and_reset()
                    current_miou = result["IoU"].mean()
                    
                    # üÜï ÂâµÂª∫ÁµêÊûúË°®Ê†º
                    table = Table()
                    table.add_column("Category")
                    table.add_column("Acc")
                    table.add_column("IoU")
                    table.add_column("Dice")
                    table.add_column("Fscore")
                    table.add_column("Precision")
                    table.add_column("Recall")
                    
                    for cat, acc, iou, dice, fs, pre, rec in zip(
                        categories,
                        result["Acc"],
                        result["IoU"],
                        result["Dice"],
                        result["Fscore"],
                        result["Precision"],
                        result["Recall"],
                    ):
                        table.add_row(
                            cat.name,
                            "{:.5f}".format(acc),
                            "{:.5f}".format(iou),
                            "{:.5f}".format(dice),
                            "{:.5f}".format(fs),
                            "{:.5f}".format(pre),
                            "{:.5f}".format(rec),
                        )
                    
                    table.add_row(
                        "Avg.",
                        "{:.5f}".format(result["Acc"].mean()),
                        "{:.5f}".format(result["IoU"].mean()),
                        "{:.5f}".format(result["Dice"].mean()),
                        "{:.5f}".format(result["Fscore"].mean()),
                        "{:.5f}".format(result["Precision"].mean()),
                        "{:.5f}".format(result["Recall"].mean()),
                    )
                    
                    prog.remove_task(val_task)
                    print(table)
                    
                    # Ë®òÈåÑÈ©óË≠âÁµêÊûú
                    logger.info("ValLoop", f"Total Loss: {val_total_loss:.5f}")
                    logger.info("ValLoop", f"Seg Loss: {val_seg_loss:.5f}")
                    logger.info("ValLoop", f"Contrastive Loss: {val_contrastive_loss:.5f}")
                    if memory_enabled:
                        logger.info("ValLoop", f"Memory Weight: {val_memory_weight:.4f}")
                    logger.info("ValLoop", f"mIoU: {result['IoU'].mean():.5f}")
                    
                    logger.tb_log("ValLoop/TotalLoss", val_total_loss, e)
                    logger.tb_log("ValLoop/SegLoss", val_seg_loss, e)
                    logger.tb_log("ValLoop/ContrastiveLoss", val_contrastive_loss, e)
                    if memory_enabled:
                        logger.tb_log("ValLoop/MemoryWeight", val_memory_weight, e)
                    logger.tb_log("ValLoop/mIoU", result["IoU"].mean(), e)
                    
                    # Ë®òÊÜ∂Â∫´Áµ±Ë®à
                    if memory_enabled:
                        memory_stats = model.get_memory_stats()
                        logger.info("Memory", f"Final Stats - Locations: {memory_stats['total_locations']}, "
                                             f"Memories: {memory_stats['total_memories']}, "
                                             f"Hit Rate: {memory_stats['hit_rate']:.4f}")
                        
                        logger.tb_log("Memory/TotalLocations", memory_stats['total_locations'], e)
                        logger.tb_log("Memory/TotalMemories", memory_stats['total_memories'], e)
                        logger.tb_log("Memory/HitRate", memory_stats['hit_rate'], e)
                    
                    # üÜï Á¢∫ÂÆöÁï∂ÂâçÂàÜÊï∏Áî®ÊñºÊó©ÂÅú
                    current_score = current_miou if args.monitor == 'mIoU' else val_total_loss
                    
                    # üÜï Ê™¢Êü•ÊòØÂê¶ÊòØÊúÄ‰Ω≥Ê®°Âûã
                    is_best = False
                    if args.monitor == 'mIoU':
                        is_best = current_score > best_score
                    else:
                        is_best = current_score < best_score
                    
                    if is_best:
                        best_score = current_score
                        logger.info("Training", f"New best {args.monitor}: {best_score:.5f}")
                        save_best_metrics_to_csv(categories, result, e, best_score, args.logdir)
                    
                    # üÜï ‰øùÂ≠òÊ™¢Êü•ÈªûÔºàÂåÖÂê´Ê®°ÂûãÈÖçÁΩÆÔºâ
                    checkpoint_path = os.path.join(args.logdir, f"checkpoint_{e}.pth")
                    memory_stats = model.get_memory_stats() if memory_enabled else {}
                    save_checkpoint_geo(model, optimizer, warmup_scheduler, poly_scheduler, 
                                      e, best_score, checkpoint_path, is_best, args.keep_only_best,
                                      memory_stats, is_warmup, args)
                    
                    # üÜï Êó©ÂÅúÊ™¢Êü•
                    if early_stopping is not None:
                        improved = early_stopping(current_score)
                        if not improved:
                            logger.info("Training", f"No improvement for {early_stopping.wait}/{args.patience} epochs")
                        
                        if early_stopping.early_stop:
                            logger.info("Training", f"Early stopping triggered at epoch {e}")
                            logger.info("Training", f"Best {args.monitor}: {best_score:.5f}")
                            break
            
            # Â≠∏ÁøíÁéáË™øÂ∫¶
            if not is_warmup:
                poly_scheduler.step()
            
            # ‰øùÂ≠òÊ™¢Êü•Èªû
            if e % args.checkpoint_interval == 0:
                checkpoint_path = os.path.join(args.logdir, f"checkpoint_{e}.pth")
                if not args.keep_only_best:
                    memory_stats = model.get_memory_stats() if memory_enabled else {}
                    save_checkpoint_geo(model, optimizer, warmup_scheduler, poly_scheduler, 
                                      e, best_score, checkpoint_path, memory_stats=memory_stats,
                                      is_warmup=is_warmup, args=args)
            
            # üÜï ‰øùÂ≠òÊúÄÂæåÊ™¢Êü•Èªû
            if args.save_last_checkpoint or not args.keep_only_best:
                last_checkpoint_path = os.path.join(args.logdir, "checkpoint_last.pth")
                memory_stats = model.get_memory_stats() if memory_enabled else {}
                save_checkpoint_geo(model, optimizer, warmup_scheduler, poly_scheduler, 
                                  e, best_score, last_checkpoint_path, memory_stats=memory_stats,
                                  is_warmup=is_warmup, args=args)
            
            prog.update(whole_task, advance=1)
        
        prog.remove_task(whole_task)
    
    # üÜï Ë®ìÁ∑¥ÂÆåÊàêÂæå‰øùÂ≠òÊúÄÁµÇË®òÊÜ∂Â∫´ÔºàÂÉÖÁï∂Ë®òÊÜ∂Â∫´ÂïüÁî®ÊôÇÔºâ
    if memory_enabled and args.save_memory_stats:
        print(f"üíæ Saving final memory bank...")
        model.save_memory_bank()
        
    # ÊúÄÁµÇÁµ±Ë®à
    final_memory_stats = model.get_memory_stats()
    logger.info("Training", "Training completed!")
    logger.info("Training", f"Final best {args.monitor}: {best_score:.5f}")
    
    print(f"\nüéâ Ë®òÊÜ∂Â¢ûÂº∑Áâà GeoSegformer Ë®ìÁ∑¥ÂÆåÊàêÔºÅ")
    if memory_enabled:
        print(f"üìä ÊúÄÁµÇË®òÊÜ∂Â∫´Áµ±Ë®à:")
        print(f"  Á∏Ω‰ΩçÁΩÆÊï∏: {final_memory_stats['total_locations']}")
        print(f"  Á∏ΩË®òÊÜ∂Êï∏: {final_memory_stats['total_memories']}")
        print(f"  ÂëΩ‰∏≠Áéá: {final_memory_stats['hit_rate']:.4f}")
        print(f"  Âπ≥ÂùáÊØè‰ΩçÁΩÆË®òÊÜ∂Êï∏: {final_memory_stats['avg_memories_per_location']:.2f}")
    else:
        print(f"üö´ Ë®òÊÜ∂Â∫´Â∑≤Á¶ÅÁî® - GPS-only Ê®°ÂûãË®ìÁ∑¥ÂÆåÊàê")
    
    return best_score


def run_single_seed_experiment(args: Namespace, seed: int):
    """Âü∑Ë°åÂñÆ‰∏ÄÁ®ÆÂ≠êÁöÑÂØ¶È©ó"""
    set_seed(seed)
    
    original_logdir = args.logdir
    seed_logdir = f"{original_logdir}_seed_{seed}"
    args.logdir = seed_logdir
    
    memory_status = "with_memory" if args.memory_size > 0 else "gps_only"
    print(f"\nüé≤ Running GeoSegformer experiment ({memory_status}) with seed {seed}")
    print(f"üìÅ Results will be saved to: {seed_logdir}")
    
    best_score = main_training_logic(args)
    
    args.logdir = original_logdir
    
    return {
        'seed': seed,
        'best_score': best_score,
        'logdir': seed_logdir,
        'memory_enabled': args.memory_size > 0
    }


def main(args: Namespace):
    """‰∏ªÂáΩÊï∏ÔºöÂü∑Ë°åÂñÆÁ®ÆÂ≠êÂØ¶È©ó"""
    result = run_single_seed_experiment(args, args.seed)
    
    memory_status = "with memory bank" if result['memory_enabled'] else "GPS-only (no memory)"
    print(f"‚úÖ GeoSegformer training completed ({memory_status}) with seed {args.seed}")
    print(f"Final best {args.monitor}: {result['best_score']:.4f}")


if __name__ == "__main__":
    args = parse_args()
    main(args)