import os
import pandas as pd
from argparse import ArgumentParser, Namespace
from pathlib import Path
from typing import Dict, Any
import numpy as np

import torch
from rich import print
from rich.progress import Progress
from rich.table import Table

import engine.transform as transform
from engine.category import Category
from engine.dataloading import ImgAnnDataset
from engine.metric import Metrics
from engine.geo_v2_memory import create_memory_enhanced_geo_segformer
from engine.visualizer import IdMapVisualizer, ImgSaver


class MemoryEnhancedGeoSegDataset(ImgAnnDataset):
    """
    è¨˜æ†¶å¢å¼·ç‰ˆæ¸¬è©¦æ•¸æ“šé›†
    """
    def __init__(
        self,
        transforms: list,
        img_dir: str,
        ann_dir: str,
        gps_csv: str,
        max_len: int = None,
    ):
        super().__init__(transforms, img_dir, ann_dir, max_len)
        
        # è¼‰å…¥ GPS æ•¸æ“š
        self.gps_data = pd.read_csv(gps_csv)
        
        # å‰µå»ºæª”ååˆ° GPS çš„æ˜ å°„
        self.filename_to_gps = {}
        for _, row in self.gps_data.iterrows():
            filename = os.path.splitext(row['filename'])[0]
            self.filename_to_gps[filename] = [row['lat'], row['long']]
        
        print(f"âœ… Loaded GPS data for {len(self.filename_to_gps)} images")
        
        # åˆ†æGPSæ•¸æ“šåˆ†ä½ˆ
        self.analyze_gps_distribution()
    
    def analyze_gps_distribution(self):
        """åˆ†æGPSæ•¸æ“šåˆ†ä½ˆ"""
        lats = [coords[0] for coords in self.filename_to_gps.values()]
        lons = [coords[1] for coords in self.filename_to_gps.values()]
        
        print(f"ğŸ“Š GPSæ•¸æ“šåˆ†æ:")
        print(f"  ç·¯åº¦ç¯„åœ: [{min(lats):.6f}, {max(lats):.6f}] (ç¯„åœ: {max(lats)-min(lats):.6f})")
        print(f"  ç¶“åº¦ç¯„åœ: [{min(lons):.6f}, {max(lons):.6f}] (ç¯„åœ: {max(lons)-min(lons):.6f})")
        
        # è¨ˆç®—é‡è¤‡ä½ç½®
        unique_positions = set()
        duplicate_count = 0
        for coords in self.filename_to_gps.values():
            coord_str = f"{coords[0]:.7f},{coords[1]:.7f}"
            if coord_str in unique_positions:
                duplicate_count += 1
            else:
                unique_positions.add(coord_str)
        
        duplicate_rate = duplicate_count / len(self.filename_to_gps) * 100
        print(f"  å”¯ä¸€ä½ç½®æ•¸: {len(unique_positions)}")
        print(f"  é‡è¤‡ä½ç½®ç‡: {duplicate_rate:.2f}%")
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        # ç²å–åŸå§‹æ•¸æ“š
        data = super().__getitem__(idx)
        
        # å¾è·¯å¾‘ä¸­æå–æª”å
        img_path = self.img_ann_paths[idx][0]
        filename = os.path.splitext(os.path.basename(img_path))[0]
        
        # æ·»åŠ  GPS æ•¸æ“šå’Œæª”å
        if filename in self.filename_to_gps:
            gps_coords = self.filename_to_gps[filename]
            data['gps'] = torch.tensor(gps_coords, dtype=torch.float32)
        else:
            print(f"âš ï¸ Warning: No GPS data found for {filename}")
            data['gps'] = torch.zeros(2, dtype=torch.float32)
        
        # æ·»åŠ æª”åç”¨æ–¼è¿½è¹¤
        data['filename'] = filename
        
        return data


def manual_gps_normalization_exact_training_params(train_gps_csv: str, val_gps_csv: str, method: str = "minmax"):
    """
    ğŸ†• æ‰‹å‹•è¨­ç½®èˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒçš„GPSæ­£è¦åŒ–åƒæ•¸
    ç¢ºä¿èˆ‡ geotrain_v2_early_v1.py ä¸­çš„ setup_gps_normalization é‚è¼¯å®Œå…¨ä¸€è‡´
    """
    print(f"ğŸ—ºï¸  æ‰‹å‹•è¨­ç½®GPSæ­£è¦åŒ–åƒæ•¸ (method: {method})")
    print("   âš ï¸  ç¢ºä¿èˆ‡è¨“ç·´æ™‚åƒæ•¸å®Œå…¨ä¸€è‡´!")
    
    # ğŸ”‘ é—œéµï¼šä½¿ç”¨èˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒçš„æ•¸æ“šè¼‰å…¥å’Œè™•ç†é‚è¼¯
    train_gps = pd.read_csv(train_gps_csv)
    val_gps = pd.read_csv(val_gps_csv)
    all_gps = pd.concat([train_gps, val_gps], ignore_index=True)
    
    if method == "minmax":
        # ğŸ”‘ é—œéµï¼šèˆ‡è¨“ç·´æ™‚å®Œå…¨ç›¸åŒçš„è¨ˆç®—æ–¹å¼
        lat_min = all_gps['lat'].min()
        lat_max = all_gps['lat'].max()
        lon_min = all_gps['long'].min()
        lon_max = all_gps['long'].max()
        
        # ğŸ”‘ é—œéµï¼šä½¿ç”¨å®Œå…¨ç›¸åŒçš„paddingè¨ˆç®—
        lat_range = lat_max - lat_min
        lon_range = lon_max - lon_min
        padding = 0.01  # å¿…é ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´
        
        lat_min -= lat_range * padding
        lat_max += lat_range * padding
        lon_min -= lon_range * padding
        lon_max += lon_range * padding
        
        print(f"ğŸ“‹ è¨ˆç®—çš„GPSæ­£è¦åŒ–åƒæ•¸:")
        print(f"   åŸå§‹ç¯„åœ - ç·¯åº¦: [{all_gps['lat'].min():.8f}, {all_gps['lat'].max():.8f}]")
        print(f"   åŸå§‹ç¯„åœ - ç¶“åº¦: [{all_gps['long'].min():.8f}, {all_gps['long'].max():.8f}]")
        print(f"   Padding: {padding}")
        print(f"   æœ€çµ‚ç¯„åœ - ç·¯åº¦: [{lat_min:.8f}, {lat_max:.8f}]")
        print(f"   æœ€çµ‚ç¯„åœ - ç¶“åº¦: [{lon_min:.8f}, {lon_max:.8f}]")
        
        return transform.GPSNormalize(
            lat_range=(lat_min, lat_max),
            lon_range=(lon_min, lon_max)
        )
    else:
        raise ValueError(f"Method {method} not implemented in manual mode")


def analyze_gps_quantization_matching(
    train_gps_csv: str, 
    test_gps_csv: str, 
    spatial_radius: float,
    gps_normalizer,
    model_memory_bank=None
):
    """
    ğŸ†• åˆ†æGPSé‡åŒ–åŒ¹é…æƒ…æ³ï¼ˆåŒ…å«æ­£è¦åŒ–ï¼‰
    """
    
    def gps_to_key_with_normalization(lat, lon, normalizer, radius):
        """æ¨¡æ“¬å®Œæ•´çš„GPSè™•ç†æµç¨‹ï¼šæ­£è¦åŒ– + é‡åŒ–"""
        # 1. æ­£è¦åŒ–
        original = torch.tensor([lat, lon], dtype=torch.float32)
        data = {'gps': original}
        normalized_data = normalizer(data)
        normalized_gps = normalized_data['gps']
        
        # 2. é‡åŒ–
        lat_grid = round(normalized_gps[0].item() / radius) * radius
        lon_grid = round(normalized_gps[1].item() / radius) * radius
        return f"{lat_grid:.7f},{lon_grid:.7f}", normalized_gps.numpy()
    
    # è¼‰å…¥GPSæ•¸æ“š
    train_gps = pd.read_csv(train_gps_csv)
    test_gps = pd.read_csv(test_gps_csv)
    
    print(f"ğŸ“Š GPSé‡åŒ–åŒ¹é…åˆ†æ (åŒ…å«æ­£è¦åŒ–)")
    print(f"   spatial_radius: {spatial_radius:.7f}")
    print("=" * 60)
    
    # 1. å»ºç«‹è¨“ç·´é›†çš„é‡åŒ–éµé›†åˆ
    train_keys = set()
    train_key_counts = {}
    train_normalized_coords = []
    
    for _, row in train_gps.iterrows():
        key, normalized = gps_to_key_with_normalization(
            row['lat'], row['long'], gps_normalizer, spatial_radius
        )
        train_keys.add(key)
        train_key_counts[key] = train_key_counts.get(key, 0) + 1
        train_normalized_coords.append(normalized)
    
    print(f"ğŸ“ è¨“ç·´é›†çµ±è¨ˆ:")
    print(f"   ç¸½GPSè¨˜éŒ„: {len(train_gps)}")
    print(f"   å”¯ä¸€é‡åŒ–ä½ç½®: {len(train_keys)}")
    print(f"   å¹³å‡æ¯ä½ç½®æ¨£æœ¬æ•¸: {len(train_gps) / len(train_keys):.2f}")
    
    # 2. åˆ†ææ¸¬è©¦é›†çš„é‡åŒ–åŒ¹é…
    test_matches = 0
    test_keys = set()
    test_key_counts = {}
    test_normalized_coords = []
    match_details = []
    
    for idx, row in test_gps.iterrows():
        key, normalized = gps_to_key_with_normalization(
            row['lat'], row['long'], gps_normalizer, spatial_radius
        )
        test_keys.add(key)
        test_key_counts[key] = test_key_counts.get(key, 0) + 1
        test_normalized_coords.append(normalized)
        
        is_match = key in train_keys
        if is_match:
            test_matches += 1
            
        match_details.append({
            'idx': idx,
            'filename': row['filename'],
            'original_lat': row['lat'],
            'original_long': row['long'],
            'normalized_lat': normalized[0],
            'normalized_long': normalized[1],
            'quantized_key': key,
            'matches_train': is_match,
            'train_count': train_key_counts.get(key, 0)
        })
    
    print(f"\nğŸ¯ æ¸¬è©¦é›†åŒ¹é…çµ±è¨ˆ:")
    print(f"   ç¸½GPSè¨˜éŒ„: {len(test_gps)}")
    print(f"   å”¯ä¸€é‡åŒ–ä½ç½®: {len(test_keys)}")
    print(f"   åŒ¹é…è¨“ç·´é›†çš„è¨˜éŒ„: {test_matches}")
    print(f"   åŒ¹é…ç‡: {test_matches / len(test_gps) * 100:.2f}%")
    
    # 3. æ­£è¦åŒ–ç¯„åœåˆ†æ
    train_normalized_coords = np.array(train_normalized_coords)
    test_normalized_coords = np.array(test_normalized_coords)
    
    print(f"\nğŸ—ºï¸  æ­£è¦åŒ–å¾Œçš„GPSç¯„åœ:")
    print(f"   è¨“ç·´é›† - ç·¯åº¦: [{train_normalized_coords[:, 0].min():.6f}, {train_normalized_coords[:, 0].max():.6f}]")
    print(f"   è¨“ç·´é›† - ç¶“åº¦: [{train_normalized_coords[:, 1].min():.6f}, {train_normalized_coords[:, 1].max():.6f}]")
    print(f"   æ¸¬è©¦é›† - ç·¯åº¦: [{test_normalized_coords[:, 0].min():.6f}, {test_normalized_coords[:, 0].max():.6f}]")
    print(f"   æ¸¬è©¦é›† - ç¶“åº¦: [{test_normalized_coords[:, 1].min():.6f}, {test_normalized_coords[:, 1].max():.6f}]")
    
    # 4. é‡ç–Šåˆ†æ
    overlapping_keys = train_keys.intersection(test_keys)
    train_only_keys = train_keys - test_keys
    test_only_keys = test_keys - train_keys
    
    print(f"\nğŸ”„ ä½ç½®é‡ç–Šåˆ†æ:")
    print(f"   é‡ç–Šé‡åŒ–ä½ç½®: {len(overlapping_keys)}")
    print(f"   åƒ…è¨“ç·´é›†ä½ç½®: {len(train_only_keys)}")
    print(f"   åƒ…æ¸¬è©¦é›†ä½ç½®: {len(test_only_keys)}")
    print(f"   ä½ç½®é‡ç–Šç‡: {len(overlapping_keys) / len(train_keys.union(test_keys)) * 100:.2f}%")
    
    # 5. å¦‚æœæä¾›äº†æ¨¡å‹è¨˜æ†¶åº«ï¼Œæª¢æŸ¥å¯¦éš›è¨˜æ†¶åº«å…§å®¹
    if model_memory_bank is not None:
        memory_stats = model_memory_bank.get_memory_stats()
        memory_keys = set(model_memory_bank.memory_bank.keys())
        
        print(f"\nğŸ§  å¯¦éš›è¨˜æ†¶åº«çµ±è¨ˆ:")
        print(f"   è¨˜æ†¶åº«ä½ç½®æ•¸: {memory_stats['total_locations']}")
        print(f"   è¨˜æ†¶åº«ç¸½è¨˜æ†¶æ•¸: {memory_stats['total_memories']}")
        
        # æª¢æŸ¥è¨˜æ†¶åº«éµèˆ‡è¨“ç·´é›†çš„åŒ¹é…
        memory_train_overlap = memory_keys.intersection(train_keys)
        memory_test_overlap = memory_keys.intersection(test_keys)
        
        print(f"   è¨˜æ†¶åº«èˆ‡è¨“ç·´é›†é‡ç–Š: {len(memory_train_overlap)}/{len(memory_keys)} ({len(memory_train_overlap)/max(len(memory_keys), 1)*100:.1f}%)")
        print(f"   è¨˜æ†¶åº«èˆ‡æ¸¬è©¦é›†é‡ç–Š: {len(memory_test_overlap)}/{len(memory_keys)} ({len(memory_test_overlap)/max(len(memory_keys), 1)*100:.1f}%)")
        
        # åˆ†ææ¸¬è©¦é›†èƒ½å¾è¨˜æ†¶åº«ç²å¾—å¤šå°‘æœ‰æ•ˆè¨˜æ†¶
        effective_test_matches = 0
        for key in test_keys:
            if key in memory_keys:
                effective_test_matches += test_key_counts[key]
        
        print(f"   æ¸¬è©¦è¨˜éŒ„çš„æœ‰æ•ˆè¨˜æ†¶è¦†è“‹: {effective_test_matches}/{len(test_gps)} ({effective_test_matches/len(test_gps)*100:.1f}%)")
    
    return {
        'match_rate': test_matches / len(test_gps),
        'overlap_rate': len(overlapping_keys) / len(train_keys.union(test_keys)),
        'train_keys': train_keys,
        'test_keys': test_keys,
        'overlapping_keys': overlapping_keys,
        'match_details': pd.DataFrame(match_details),
        'train_key_counts': train_key_counts,
        'test_key_counts': test_key_counts
    }


def test_single_gps_quantization_with_normalization(model, test_gps_tensor, gps_normalizer, device):
    """ğŸ†• æ¸¬è©¦å–®å€‹GPSçš„å®Œæ•´è™•ç†æµç¨‹ï¼šæ­£è¦åŒ– + é‡åŒ– + è¨˜æ†¶æª¢ç´¢"""
    
    print(f"\nğŸ”¬ å–®GPSå®Œæ•´è™•ç†æµç¨‹æ¸¬è©¦:")
    print("-" * 50)
    
    model.eval()
    with torch.no_grad():
        for i, original_gps in enumerate(test_gps_tensor[:5]):  # æ¸¬è©¦å‰5å€‹
            
            print(f"GPS {i+1}: åŸå§‹åº§æ¨™ [{original_gps[0]:.6f}, {original_gps[1]:.6f}]")
            
            # 1. æ‰‹å‹•æ­£è¦åŒ–
            data = {'gps': original_gps.clone()}
            normalized_data = gps_normalizer(data)
            normalized_gps = normalized_data['gps']
            
            print(f"   æ­¥é©Ÿ1 - æ­£è¦åŒ–: [{normalized_gps[0]:.6f}, {normalized_gps[1]:.6f}]")
            
            # 2. æ‰‹å‹•é‡åŒ–
            spatial_radius = model.memory_bank.spatial_radius
            expected_key = model.memory_bank.gps_to_key(normalized_gps)
            
            print(f"   æ­¥é©Ÿ2 - é‡åŒ–éµ: {expected_key}")
            
            # 3. æª¢æŸ¥è¨˜æ†¶åº«
            has_memory = expected_key in model.memory_bank.memory_bank
            memory_count = len(model.memory_bank.memory_bank.get(expected_key, {}).get('features', []))
            
            print(f"   æ­¥é©Ÿ3 - è¨˜æ†¶åº«æª¢æŸ¥:")
            print(f"           æœ‰æ­¤éµ: {has_memory}")
            print(f"           è¨˜æ†¶æ•¸é‡: {memory_count}")
            
            # 4. å®Œæ•´çš„æ¨¡å‹æ¨ç†æ¸¬è©¦
            gps_batch = normalized_gps.unsqueeze(0).to(device)
            memory_features = model.memory_bank.retrieve_memory(gps_batch)
            memory_norm = torch.norm(memory_features).item()
            
            print(f"   æ­¥é©Ÿ4 - è¨˜æ†¶æª¢ç´¢:")
            print(f"           æª¢ç´¢ç‰¹å¾µç¯„æ•¸: {memory_norm:.6f}")
            print(f"           æœ‰æ•ˆè¨˜æ†¶: {'âœ…' if memory_norm > 1e-6 else 'âŒ'}")
            
            # 5. æ¨¡å‹å®Œæ•´å‰å‘å‚³æ’­æ¸¬è©¦
            dummy_image = torch.randn(1, 3, 224, 224).to(device)  # å‡çš„åœ–åƒ
            try:
                outputs = model(dummy_image, gps_batch, return_embeddings=False, update_memory=False)
                memory_weight = outputs.get('memory_weight', 0)
                print(f"   æ­¥é©Ÿ5 - å®Œæ•´æ¨ç†:")
                print(f"           è¨˜æ†¶æ¬Šé‡: {memory_weight:.4f}")
                print(f"           æ¨ç†æˆåŠŸ: âœ…")
            except Exception as e:
                print(f"   æ­¥é©Ÿ5 - å®Œæ•´æ¨ç†:")
                print(f"           æ¨ç†å¤±æ•—: âŒ {str(e)}")
            
            print()


def load_model_from_checkpoint(checkpoint_path: str, num_categories: int, device: str):
    """ğŸ†• å¢å¼·ç‰ˆæ¨¡å‹è¼‰å…¥ï¼Œæ”¯æŒå¾æª¢æŸ¥é»æ¨æ–·æ¨¡å‹åƒæ•¸"""
    print(f"ğŸ“‚ Loading checkpoint from: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    except Exception as e:
        print(f"âš ï¸  Standard loading failed: {e}")
        print("ğŸ”„ Trying alternative loading method...")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
            print("âœ… Loaded weights only (without training args)")
            return load_model_with_default_params(checkpoint, num_categories, device)
        except Exception as e2:
            print(f"âŒ Alternative loading also failed: {e2}")
            raise e2
    
    # ğŸ†• å˜—è©¦å¾æª¢æŸ¥é»ä¸­æå–æ¨¡å‹åƒæ•¸
    model_args = extract_model_args_from_checkpoint(checkpoint, checkpoint_path)
    
    if model_args:
        print(f"âœ… Extracted model configuration:")
        print(f"   Model: {model_args['model_size']}, Feature dim: {model_args['feature_dim']}")
        print(f"   Memory: {model_args['memory_size']}, Spatial radius: {model_args['spatial_radius']}")
        
        model = create_memory_enhanced_geo_segformer(
            num_classes=num_categories,
            model_size=model_args['model_size'],
            feature_dim=model_args['feature_dim'],
            fusion_method=model_args['fusion_method'],
            memory_size=model_args['memory_size'],
            spatial_radius=model_args['spatial_radius'],
            memory_save_path=None
        ).to(device)
        
        model.load_state_dict(checkpoint['model'])
        model.eval()
        
        epoch = checkpoint.get('epoch', 'unknown')
        best_score = checkpoint.get('best_score', 'unknown')
        print(f"âœ… Model loaded from epoch {epoch}, best score: {best_score}")
        
        return model, model_args
    
    else:
        print("âš ï¸  Could not extract model args, using defaults")
        model = load_model_with_default_params(checkpoint, num_categories, device)
        return model, None


def extract_model_args_from_checkpoint(checkpoint, checkpoint_path: str):
    """ğŸ†• å¾æª¢æŸ¥é»ä¸­æå–æ¨¡å‹åƒæ•¸"""
    
    # æ–¹æ³•1: æª¢æŸ¥æ˜¯å¦æœ‰é¡¯å¼ä¿å­˜çš„args
    if 'args' in checkpoint:
        args = checkpoint['args']
        if hasattr(args, 'model_size'):
            return {
                'model_size': getattr(args, 'model_size', 'b0'),
                'feature_dim': getattr(args, 'feature_dim', 512),
                'fusion_method': getattr(args, 'fusion_method', 'attention'),
                'memory_size': getattr(args, 'memory_size', 20),
                'spatial_radius': getattr(args, 'spatial_radius', 0.00005),
            }
    
    # æ–¹æ³•2: æª¢æŸ¥model_config
    if 'model_config' in checkpoint:
        config = checkpoint['model_config']
        return {
            'model_size': config.get('model_size', 'b0'),
            'feature_dim': config.get('feature_dim', 512),
            'fusion_method': config.get('fusion_method', 'attention'),
            'memory_size': config.get('memory_size', 20),
            'spatial_radius': config.get('spatial_radius', 0.00005),
        }
    
    # æ–¹æ³•3: å¾æ¨¡å‹æ¬Šé‡æ¨æ–·åƒæ•¸
    if 'model' in checkpoint:
        model_state = checkpoint['model']
        extracted_args = infer_model_args_from_weights(model_state)
        if extracted_args:
            return extracted_args
    
    # æ–¹æ³•4: å¾æª”æ¡ˆè·¯å¾‘æ¨æ–·åƒæ•¸
    path_args = infer_model_args_from_path(checkpoint_path)
    if path_args:
        return path_args
    
    return None


def infer_model_args_from_weights(model_state_dict):
    """ğŸ†• å¾æ¨¡å‹æ¬Šé‡æ¨æ–·åƒæ•¸"""
    try:
        # æ¨æ–·feature_dim
        feature_dim = None
        for key, tensor in model_state_dict.items():
            if 'location_encoder.mlp.4.weight' in key:
                feature_dim = tensor.shape[0]
                break
            elif 'segmentation_head.0.weight' in key:
                feature_dim = tensor.shape[1]
                break
        
        # æ¨æ–·model_size
        model_size = "b0"  # é»˜èª
        for key, tensor in model_state_dict.items():
            if 'image_encoder.feature_fusion.0.weight' in key:
                input_channels = tensor.shape[1]
                if input_channels == 512:
                    model_size = "b0"
                elif input_channels == 1024:
                    model_size = "b1"
                break
        
        if feature_dim:
            return {
                'model_size': model_size,
                'feature_dim': feature_dim,
                'fusion_method': 'attention',
                'memory_size': 20,
                'spatial_radius': 0.00005,
            }
    
    except Exception as e:
        print(f"âš ï¸  Failed to infer from weights: {e}")
    
    return None


def infer_model_args_from_path(checkpoint_path: str):
    """ğŸ†• å¾æª”æ¡ˆè·¯å¾‘æ¨æ–·åƒæ•¸"""
    try:
        path_str = str(checkpoint_path).lower()
        
        # æ¨æ–·model_size
        if 'b1' in path_str:
            model_size = 'b1'
        elif 'b2' in path_str:
            model_size = 'b2'
        else:
            model_size = 'b0'
        
        # æ¨æ–·feature_dim
        feature_dim = 512  # é»˜èª
        if 'dim256' in path_str or 'feature256' in path_str:
            feature_dim = 256
        elif 'dim1024' in path_str or 'feature1024' in path_str:
            feature_dim = 1024
        
        return {
            'model_size': model_size,
            'feature_dim': feature_dim,
            'fusion_method': 'attention',
            'memory_size': 20,
            'spatial_radius': 0.00005,
        }
    
    except Exception as e:
        print(f"âš ï¸  Failed to infer from path: {e}")
    
    return None


def load_model_with_default_params(checkpoint, num_categories: int, device: str):
    """ä½¿ç”¨é»˜èªåƒæ•¸è¼‰å…¥æ¨¡å‹"""
    print("ğŸ”§ Using default parameters for model creation:")
    model_size = "b0"
    feature_dim = 512
    fusion_method = "attention"
    memory_size = 20
    spatial_radius = 0.00005
    
    print(f"   Model: {model_size}, Feature dim: {feature_dim}")
    print(f"   Memory: {memory_size}, Spatial radius: {spatial_radius}")
    
    model = create_memory_enhanced_geo_segformer(
        num_classes=num_categories,
        model_size=model_size,
        feature_dim=feature_dim,
        fusion_method=fusion_method,
        memory_size=memory_size,
        spatial_radius=spatial_radius,
        memory_save_path=None
    ).to(device)
    
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    
    epoch = checkpoint.get('epoch', 'unknown') if isinstance(checkpoint, dict) else 'unknown'
    best_score = checkpoint.get('best_score', 'unknown') if isinstance(checkpoint, dict) else 'unknown'
    print(f"âœ… Model loaded from epoch {epoch}, best score: {best_score}")
    
    return model


def find_memory_bank_file(checkpoint_path: str, logdir: str = None):
    """ğŸ†• æ™ºèƒ½å°‹æ‰¾è¨˜æ†¶åº«æ–‡ä»¶"""
    possible_paths = []
    
    # 1. å¾æª¢æŸ¥é»è·¯å¾‘æ¨æ–·è¨˜æ†¶åº«è·¯å¾‘
    checkpoint_dir = os.path.dirname(checkpoint_path)
    possible_paths.extend([
        os.path.join(checkpoint_dir, "memory_stats.pth"),
        os.path.join(checkpoint_dir, "memory_stats.json"),
        os.path.join(checkpoint_dir, "multilayer_memory_stats.pth"),
        os.path.join(checkpoint_dir, "multilayer_memory_stats.json"),
    ])
    
    # 2. å¦‚æœæä¾›äº†logdir
    if logdir:
        possible_paths.extend([
            os.path.join(logdir, "memory_stats.pth"),
            os.path.join(logdir, "memory_stats.json"),
            os.path.join(logdir, "multilayer_memory_stats.pth"),
            os.path.join(logdir, "multilayer_memory_stats.json"),
        ])
    
    # 3. æª¢æŸ¥æ˜¯å¦å­˜åœ¨
    for path in possible_paths:
        if os.path.exists(path):
            print(f"ğŸ” Found memory bank file: {path}")
            return path
    
    print("âŒ No memory bank file found in expected locations:")
    for path in possible_paths:
        print(f"   - {path}")
    
    return None


def run_inference_with_memory(model, dataloader, device, args, memory_enabled=True):
    """ğŸ†• åŸ·è¡Œå¸¶è¨˜æ†¶æˆ–ä¸å¸¶è¨˜æ†¶çš„æ¨ç†"""
    criterion = torch.nn.CrossEntropyLoss().to(device)
    metrics = Metrics(len(Category.load(args.category_csv, show=False)), nan_to_num=0)
    
    total_loss = 0
    total_memory_weight = 0
    memory_hit_count = 0
    
    # ğŸ†• GPSé‡åŒ–çµ±è¨ˆ
    quantization_hits = 0
    effective_memory_count = 0
    total_samples_processed = 0
    
    # ğŸ†• è¨˜æ†¶åº«ç‹€æ…‹ç®¡ç†
    original_memory_enabled = memory_enabled
    if not memory_enabled and hasattr(model, 'memory_bank'):
        print("ğŸ”„ Temporarily disabling memory bank for comparison...")
        original_memory_bank = model.memory_bank.memory_bank
        from collections import defaultdict
        model.memory_bank.memory_bank = defaultdict(lambda: {'features': [], 'count': 0, 'last_updated': 0})
    
    progress_desc = "Testing (with memory)" if memory_enabled else "Testing (without memory)"
    
    with Progress() as prog:
        with torch.no_grad():
            task = prog.add_task(progress_desc, total=len(dataloader))
            
            for batch_idx, data in enumerate(dataloader):
                img = data["img"].to(device)
                ann = data["ann"].to(device)[:, 0, :, :]
                gps = data["gps"].to(device)
                
                # æ¨ç†ï¼ˆä¸æ›´æ–°è¨˜æ†¶åº«ï¼‰
                outputs = model(img, gps, return_embeddings=False, update_memory=False)
                pred = outputs['segmentation_logits']
                
                # è¨ˆç®—æå¤±
                loss = criterion(pred, ann)
                total_loss += loss.item()
                
                # è¨˜éŒ„è¨˜æ†¶çµ±è¨ˆ
                memory_weight = outputs.get('memory_weight', 0)
                total_memory_weight += memory_weight
                if memory_weight > 0.1:  # æœ‰æ•ˆè¨˜æ†¶é–¾å€¼
                    memory_hit_count += 1
                
                # ğŸ†• GPSé‡åŒ–çµ±è¨ˆï¼ˆåƒ…åœ¨memory_enabledæ™‚çµ±è¨ˆï¼‰
                if memory_enabled and hasattr(model, 'memory_bank'):
                    for i in range(gps.shape[0]):
                        gps_coord = gps[i]
                        quantized_key = model.memory_bank.gps_to_key(gps_coord)
                        
                        # æª¢æŸ¥æ˜¯å¦æœ‰è¨˜æ†¶
                        has_memory = quantized_key in model.memory_bank.memory_bank
                        if has_memory:
                            quantization_hits += 1
                            memory_count = len(model.memory_bank.memory_bank[quantized_key]['features'])
                            if memory_count > 0:
                                effective_memory_count += 1
                        
                        total_samples_processed += 1
                
                # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
                metrics.compute_and_accum(pred.argmax(1), ann)
                
                # ä¿å­˜é æ¸¬çµæœ
                if args.save_dir:
                    save_suffix = "_with_mem" if memory_enabled else "_no_mem"
                    for fn, p in zip(data["img_path"], pred):
                        filename = Path(fn).stem + save_suffix + ".png"
                        img_saver = ImgSaver(args.save_dir, IdMapVisualizer(Category.load(args.category_csv, show=False)))
                        img_saver.save_pred(p[None, :], filename)
                
                # é¡¯ç¤ºè¨˜æ†¶çµ±è¨ˆ
                if args.show_memory_stats and batch_idx % 100 == 0 and memory_enabled:
                    if hasattr(model, 'get_memory_stats'):
                        memory_stats = model.get_memory_stats()
                        hit_rate = quantization_hits / max(total_samples_processed, 1)
                        effective_rate = effective_memory_count / max(total_samples_processed, 1)
                        print(f"ğŸ§  Batch {batch_idx}: Locations: {memory_stats['total_locations']}, "
                              f"Memory hit rate: {memory_stats['hit_rate']:.3f}, "
                              f"GPS quantization hit: {hit_rate:.3f}, "
                              f"Effective memory: {effective_rate:.3f}, "
                              f"Current weight: {memory_weight:.3f}")
                
                prog.update(task, advance=1)
            
            # ç²å–æœ€çµ‚çµæœ
            result = metrics.get_and_reset()
            avg_loss = total_loss / len(dataloader)
            avg_memory_weight = total_memory_weight / len(dataloader)
            memory_usage_rate = memory_hit_count / len(dataloader)
            
            # ğŸ†• GPSé‡åŒ–çµ±è¨ˆ
            gps_quantization_hit_rate = quantization_hits / max(total_samples_processed, 1) if memory_enabled else 0
            effective_memory_rate = effective_memory_count / max(total_samples_processed, 1) if memory_enabled else 0
            
            prog.remove_task(task)
    
    # ğŸ†• æ¢å¾©è¨˜æ†¶åº«ï¼ˆå¦‚æœä¹‹å‰ç¦ç”¨äº†ï¼‰
    if not original_memory_enabled and hasattr(model, 'memory_bank'):
        print("ğŸ”„ Restoring original memory bank...")
        model.memory_bank.memory_bank = original_memory_bank
    
    return {
        'result': result,
        'avg_loss': avg_loss,
        'avg_memory_weight': avg_memory_weight,
        'memory_usage_rate': memory_usage_rate,
        'memory_enabled': memory_enabled,
        'gps_quantization_hit_rate': gps_quantization_hit_rate,
        'effective_memory_rate': effective_memory_rate,
        'total_samples': total_samples_processed
    }


def print_comparison_results(with_mem_results, without_mem_results):
    """ğŸ†• æ‰“å°è¨˜æ†¶å°æ¯”çµæœ"""
    print("\n" + "="*80)
    print("ğŸ†š Memory Bank Impact Analysis")
    print("="*80)
    
    # åŸºæœ¬æŒ‡æ¨™å°æ¯”
    with_miou = with_mem_results['result']['IoU'].mean()
    without_miou = without_mem_results['result']['IoU'].mean()
    miou_improvement = with_miou - without_miou
    
    with_acc = with_mem_results['result']['Acc'].mean()
    without_acc = without_mem_results['result']['Acc'].mean()
    acc_improvement = with_acc - without_acc
    
    with_loss = with_mem_results['avg_loss']
    without_loss = without_mem_results['avg_loss']
    loss_improvement = without_loss - with_loss  # æå¤±è¶Šå°è¶Šå¥½
    
    print(f"ğŸ“Š Overall Performance Comparison:")
    print(f"   Mean IoU:      {without_miou:.5f} â†’ {with_miou:.5f} ({miou_improvement:+.5f})")
    print(f"   Mean Accuracy: {without_acc:.5f} â†’ {with_acc:.5f} ({acc_improvement:+.5f})")
    print(f"   Average Loss:  {without_loss:.5f} â†’ {with_loss:.5f} ({loss_improvement:+.5f})")
    
    # è¨˜æ†¶ä½¿ç”¨çµ±è¨ˆ
    print(f"\nğŸ§  Memory Usage Statistics:")
    print(f"   Average Memory Weight: {with_mem_results['avg_memory_weight']:.4f}")
    print(f"   Memory Usage Rate:     {with_mem_results['memory_usage_rate']:.2%}")
    print(f"   GPS Quantization Hit Rate: {with_mem_results['gps_quantization_hit_rate']:.2%}")
    print(f"   Effective Memory Rate:     {with_mem_results['effective_memory_rate']:.2%}")
    
    # æ”¹é€²åˆ†æ
    print(f"\nğŸ“ˆ Improvement Analysis:")
    if miou_improvement > 0.001:
        print(f"   âœ… Memory bank provides {miou_improvement:.4f} mIoU improvement")
    elif miou_improvement > -0.001:
        print(f"   â¡ï¸  Memory bank has minimal impact ({miou_improvement:+.4f} mIoU)")
    else:
        print(f"   âŒ Memory bank may be hurting performance ({miou_improvement:+.4f} mIoU)")
    
    # GPSé‡åŒ–è¨ºæ–·
    print(f"\nğŸ¯ GPS Quantization Diagnosis:")
    hit_rate = with_mem_results['gps_quantization_hit_rate']
    if hit_rate < 0.1:
        print(f"   âŒ Very low GPS quantization hit rate ({hit_rate:.1%})")
        print(f"      â†’ GPS normalization may be inconsistent with training")
    elif hit_rate < 0.3:
        print(f"   âš ï¸  Low GPS quantization hit rate ({hit_rate:.1%})")
        print(f"      â†’ Some GPS coordinates don't match training quantization")
    else:
        print(f"   âœ… Good GPS quantization hit rate ({hit_rate:.1%})")
        print(f"      â†’ GPS normalization appears consistent with training")


def save_comparison_results_csv(with_mem_results, without_mem_results, categories, save_dir):
    """ä¿å­˜è¨˜æ†¶å°æ¯”çµæœåˆ°CSV"""
    
    print("ğŸ†š Exporting memory comparison results to CSV...")
    
    comparison_data = []
    
    # å„é¡åˆ¥å°æ¯”
    for i, cat in enumerate(categories):
        with_iou = with_mem_results['result']['IoU'][i]
        without_iou = without_mem_results['result']['IoU'][i]
        with_acc = with_mem_results['result']['Acc'][i]
        without_acc = without_mem_results['result']['Acc'][i]
        
        row = {
            'Category': cat.name,
            'Category_ID': cat.id,
            'IoU_Without_Memory': float(without_iou),
            'IoU_With_Memory': float(with_iou),
            'IoU_Improvement': float(with_iou - without_iou),
            'IoU_Improvement_Percent': float((with_iou - without_iou) / max(without_iou, 1e-8) * 100),
            'Acc_Without_Memory': float(without_acc),
            'Acc_With_Memory': float(with_acc),
            'Acc_Improvement': float(with_acc - without_acc),
            'Acc_Improvement_Percent': float((with_acc - without_acc) / max(without_acc, 1e-8) * 100)
        }
        comparison_data.append(row)
    
    # æ·»åŠ å¹³å‡å°æ¯”
    with_avg_iou = with_mem_results['result']['IoU'].mean()
    without_avg_iou = without_mem_results['result']['IoU'].mean()
    with_avg_acc = with_mem_results['result']['Acc'].mean()
    without_avg_acc = without_mem_results['result']['Acc'].mean()
    
    avg_row = {
        'Category': 'Average',
        'Category_ID': 'AVG',
        'IoU_Without_Memory': float(without_avg_iou),
        'IoU_With_Memory': float(with_avg_iou),
        'IoU_Improvement': float(with_avg_iou - without_avg_iou),
        'IoU_Improvement_Percent': float((with_avg_iou - without_avg_iou) / without_avg_iou * 100),
        'Acc_Without_Memory': float(without_avg_acc),
        'Acc_With_Memory': float(with_avg_acc),
        'Acc_Improvement': float(with_avg_acc - without_avg_acc),
        'Acc_Improvement_Percent': float((with_avg_acc - without_avg_acc) / without_avg_acc * 100)
    }
    comparison_data.append(avg_row)
    
    # ä¿å­˜å°æ¯”çµæœ
    comparison_df = pd.DataFrame(comparison_data)
    comparison_csv = os.path.join(save_dir, "memory_comparison_results.csv")
    comparison_df.to_csv(comparison_csv, index=False, float_format='%.5f')
    print(f"   âœ… Memory comparison: {comparison_csv}")
    
    return comparison_csv


def parse_args() -> Namespace:
    parser = ArgumentParser(description="ğŸ§  Enhanced GeoSegformer Model Testing with Memory Bank")
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument("img_dir", type=str, help="Test images directory")
    parser.add_argument("ann_dir", type=str, help="Test annotations directory")
    parser.add_argument("category_csv", type=str, help="Category CSV file")
    parser.add_argument("checkpoint", type=str, help="Model checkpoint file")
    parser.add_argument("test_gps_csv", type=str, help="Test GPS CSV file")
    parser.add_argument("train_gps_csv", type=str, help="Training GPS CSV file (for normalization)")
    parser.add_argument("val_gps_csv", type=str, help="Validation GPS CSV file (for normalization)")
    
    # ğŸ†• è¨˜æ†¶åº«ç›¸é—œåƒæ•¸
    parser.add_argument("--memory-bank-path", type=str, default=None,
                       help="Path to saved memory bank (auto-search if not provided)")
    parser.add_argument("--force-no-memory", action="store_true",
                       help="Force testing without memory bank (for comparison)")
    parser.add_argument("--logdir", type=str, default=None,
                       help="Training log directory (for auto-finding memory bank)")
    
    # æ¸¬è©¦åƒæ•¸
    parser.add_argument("--save-dir", type=str, default=None, 
                       help="Directory to save prediction results")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size for testing")
    parser.add_argument("--num-workers", type=int, default=0, help="Number of data loading workers")
    parser.add_argument("--max-len", type=int, default=None, help="Maximum number of test samples")
    
    # GPSæ­£è¦åŒ–
    parser.add_argument("--gps-norm-method", type=str, default="minmax", 
                       choices=["minmax", "zscore"], help="GPS normalization method")
    
    # èª¿è©¦é¸é …
    parser.add_argument("--show-memory-stats", action="store_true",
                       help="Show memory bank statistics during testing")
    parser.add_argument("--compare-with-without-memory", action="store_true",
                       help="Run comparison between with/without memory")
    parser.add_argument("--verbose", action="store_true",
                       help="Show detailed progress and statistics")
    parser.add_argument("--verify-gps-quantization", action="store_true", default=True,
                       help="Verify GPS quantization matching before testing")
    
    return parser.parse_args()


def main(args: Namespace):
    print("ğŸ§  Enhanced GeoSegformer Model Testing with Memory Bank")
    print("=" * 70)
    
    # åŸºæœ¬è¨­ç½®
    image_size = 720, 1280
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # è¼‰å…¥é¡åˆ¥
    categories = Category.load(args.category_csv)
    num_categories = Category.get_num_categories(categories)
    print(f"ğŸ“‹ Categories: {num_categories} classes")
    
    # ğŸ†• è¼‰å…¥æ¨¡å‹ï¼ˆæ”¯æŒåƒæ•¸æ¨æ–·ï¼‰
    model_info = load_model_from_checkpoint(args.checkpoint, num_categories, device)
    if isinstance(model_info, tuple):
        model, model_args = model_info
    else:
        model = model_info
        model_args = None
    
    # ğŸ†• å°‹æ‰¾å’Œè¼‰å…¥è¨˜æ†¶åº«
    memory_loaded = False
    if not args.force_no_memory:
        memory_bank_path = args.memory_bank_path
        
        if not memory_bank_path:
            # è‡ªå‹•å°‹æ‰¾è¨˜æ†¶åº«æ–‡ä»¶
            memory_bank_path = find_memory_bank_file(args.checkpoint, args.logdir)
        
        if memory_bank_path:
            print(f"ğŸ”„ Loading memory bank from: {memory_bank_path}")
            memory_loaded = model.load_memory_bank(memory_bank_path)
            
            if memory_loaded:
                initial_stats = model.get_memory_stats()
                print(f"ğŸ§  Memory bank loaded successfully!")
                print(f"   ğŸ“ {initial_stats['total_locations']} GPS locations")
                print(f"   ğŸ§  {initial_stats['total_memories']} stored memories")
        else:
            print("âš ï¸  No memory bank found, testing without historical memory")
    
    if args.force_no_memory:
        print("ğŸš« Force testing without memory bank")
    
    # ğŸ†• è¨­ç½®GPSæ­£è¦åŒ–ï¼ˆæ‰‹å‹•æ–¹å¼ï¼Œç¢ºä¿èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    print("\nğŸ—ºï¸  Setting up GPS normalization (manual mode)...")
    gps_normalizer = manual_gps_normalization_exact_training_params(
        args.train_gps_csv,
        args.val_gps_csv,
        method=args.gps_norm_method
    )
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    if args.save_dir and not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
        print(f"ğŸ“ Created save directory: {args.save_dir}")
    
    # æ•¸æ“šè®Šæ›
    transforms = [
        transform.LoadImg(),
        transform.LoadAnn(),
        gps_normalizer,
        transform.Resize(image_size),
        transform.Normalize(),
    ]
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
    test_dataset = MemoryEnhancedGeoSegDataset(
        transforms=transforms,
        img_dir=args.img_dir,
        ann_dir=args.ann_dir,
        gps_csv=args.test_gps_csv,
        max_len=args.max_len,
    )
    
    dataloader = test_dataset.get_loader(
        batch_size=args.batch_size,
        pin_memory=False,
        num_workers=args.num_workers
    )
    
    # ğŸ†• GPSé‡åŒ–åŒ¹é…æª¢æŸ¥
    if args.verify_gps_quantization and memory_loaded:
        print("\nğŸ” GPSé‡åŒ–åŒ¹é…æª¢æŸ¥:")
        print("=" * 50)
        
        # åˆ†æGPSé‡åŒ–åŒ¹é…
        quantization_results = analyze_gps_quantization_matching(
            args.train_gps_csv,
            args.test_gps_csv,
            model.memory_bank.spatial_radius,
            gps_normalizer,
            model.memory_bank
        )
        
        # æ¸¬è©¦å–®å€‹GPSçš„é‡åŒ–
        if len(test_dataset) > 0:
            sample_gps = torch.stack([test_dataset[i]['gps'] for i in range(min(5, len(test_dataset)))])
            test_single_gps_quantization_with_normalization(model, sample_gps, gps_normalizer, device)
        
        # æ ¹æ“šåŒ¹é…ç‡çµ¦å‡ºå»ºè­°
        match_rate = quantization_results['match_rate']
        if match_rate < 0.1:
            print(f"âŒ GPSåŒ¹é…ç‡éä½ ({match_rate*100:.1f}%)ï¼Œè¨˜æ†¶åº«å¯èƒ½ç„¡æ³•æœ‰æ•ˆå·¥ä½œï¼")
            print(f"å»ºè­°æª¢æŸ¥:")
            print(f"  1. spatial_radiusæ˜¯å¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´")
            print(f"  2. GPSæ­£è¦åŒ–åƒæ•¸æ˜¯å¦æ­£ç¢º")
            print(f"  3. è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“šçš„GPSåˆ†ä½ˆå·®ç•°")
        elif match_rate > 0.5:
            print(f"âœ… GPSåŒ¹é…ç‡è‰¯å¥½ ({match_rate*100:.1f}%)ï¼Œè¨˜æ†¶åº«æ‡‰è©²èƒ½æœ‰æ•ˆå·¥ä½œ")
        else:
            print(f"ğŸ’¡ GPSåŒ¹é…ç‡ä¸­ç­‰ ({match_rate*100:.1f}%)ï¼Œè¨˜æ†¶åº«æœƒéƒ¨åˆ†æœ‰æ•ˆ")
    
    print(f"\nğŸš€ Starting evaluation on {len(dataloader)} samples...")
    
    # ğŸ†• åŸ·è¡Œæ¸¬è©¦
    if args.compare_with_without_memory and memory_loaded:
        print("\nğŸ†š Running comparison: with vs without memory bank")
        
        # æ¸¬è©¦å¸¶è¨˜æ†¶åº«
        print("\n1ï¸âƒ£ Testing WITH memory bank...")
        with_mem_results = run_inference_with_memory(model, dataloader, device, args, memory_enabled=True)
        
        # æ¸¬è©¦ä¸å¸¶è¨˜æ†¶åº«
        print("\n2ï¸âƒ£ Testing WITHOUT memory bank...")
        without_mem_results = run_inference_with_memory(model, dataloader, device, args, memory_enabled=False)
        
        # æ‰“å°å°æ¯”çµæœ
        print_comparison_results(with_mem_results, without_mem_results)
        
        # ä½¿ç”¨å¸¶è¨˜æ†¶çš„çµæœä½œç‚ºä¸»è¦çµæœ
        main_results = with_mem_results
        
        # ğŸ†• ä¿å­˜å°æ¯”çµæœåˆ°CSV
        if args.save_dir:
            save_comparison_results_csv(with_mem_results, without_mem_results, categories, args.save_dir)
        
    else:
        # æ¨™æº–æ¸¬è©¦æµç¨‹
        memory_enabled = memory_loaded and not args.force_no_memory
        main_results = run_inference_with_memory(model, dataloader, device, args, memory_enabled=memory_enabled)
    
    # é¡¯ç¤ºä¸»è¦çµæœ
    result = main_results['result']
    avg_loss = main_results['avg_loss']
    
    print("\nğŸ“Š Final Test Results:")
    print("=" * 80)
    
    # å‰µå»ºçµæœè¡¨æ ¼
    table = Table(title="ğŸ¯ GeoSegformer Test Performance")
    table.add_column("Category")
    table.add_column("Acc")
    table.add_column("IoU")
    table.add_column("Dice")
    table.add_column("Fscore")
    table.add_column("Precision")
    table.add_column("Recall")
    
    for cat, acc, iou, dice, fs, pre, rec in zip(
        categories,
        result["Acc"],
        result["IoU"],
        result["Dice"],
        result["Fscore"],
        result["Precision"],
        result["Recall"],
    ):
        table.add_row(
            cat.name,
            "{:.5f}".format(acc),
            "{:.5f}".format(iou),
            "{:.5f}".format(dice),
            "{:.5f}".format(fs),
            "{:.5f}".format(pre),
            "{:.5f}".format(rec),
        )
    
    table.add_row(
        "Average",
        "{:.5f}".format(result["Acc"].mean()),
        "{:.5f}".format(result["IoU"].mean()),
        "{:.5f}".format(result["Dice"].mean()),
        "{:.5f}".format(result["Fscore"].mean()),
        "{:.5f}".format(result["Precision"].mean()),
        "{:.5f}".format(result["Recall"].mean()),
    )
    
    print(table)
    
    print(f"\nğŸ“ˆ Overall Performance:")
    print(f"   Average Loss: {avg_loss:.5f}")
    print(f"   Mean IoU: {result['IoU'].mean():.5f}")
    print(f"   Mean Accuracy: {result['Acc'].mean():.5f}")
    
    if memory_loaded:
        print(f"   Average Memory Weight: {main_results['avg_memory_weight']:.4f}")
        print(f"   Memory Usage Rate: {main_results['memory_usage_rate']:.2%}")
        print(f"   GPS Quantization Hit Rate: {main_results['gps_quantization_hit_rate']:.2%}")
        print(f"   Effective Memory Rate: {main_results['effective_memory_rate']:.2%}")
    
    # æœ€çµ‚è¨˜æ†¶åº«çµ±è¨ˆ
    if args.show_memory_stats and memory_loaded:
        final_memory_stats = model.get_memory_stats()
        print(f"\nğŸ§  Final Memory Bank Statistics:")
        print(f"   Total Locations: {final_memory_stats['total_locations']}")
        print(f"   Total Memories: {final_memory_stats['total_memories']}")
        print(f"   Hit Rate: {final_memory_stats['hit_rate']:.4f}")
        print(f"   Avg Memories per Location: {final_memory_stats['avg_memories_per_location']:.2f}")
    
    # ä¿å­˜çµæœ
    if args.save_dir:
        results_file = os.path.join(args.save_dir, "test_results.txt")
        with open(results_file, 'w') as f:
            f.write("Enhanced GeoSegformer Test Results\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Checkpoint: {args.checkpoint}\n")
            f.write(f"Memory Bank: {memory_loaded}\n")
            f.write(f"Test samples: {len(dataloader)}\n")
            f.write(f"Average Loss: {avg_loss:.5f}\n")
            f.write(f"Mean IoU: {result['IoU'].mean():.5f}\n")
            f.write(f"Mean Accuracy: {result['Acc'].mean():.5f}\n")
            
            if memory_loaded:
                f.write(f"Average Memory Weight: {main_results['avg_memory_weight']:.4f}\n")
                f.write(f"Memory Usage Rate: {main_results['memory_usage_rate']:.2%}\n")
                f.write(f"GPS Quantization Hit Rate: {main_results['gps_quantization_hit_rate']:.2%}\n")
                f.write(f"Effective Memory Rate: {main_results['effective_memory_rate']:.2%}\n")
            
            f.write(f"\nPer-category results:\n")
            for i, cat in enumerate(categories):
                f.write(f"{cat.name}: IoU={result['IoU'][i]:.5f}, Acc={result['Acc'][i]:.5f}\n")
            
            if memory_loaded and args.show_memory_stats:
                final_memory_stats = model.get_memory_stats()
                f.write(f"\nMemory Bank Statistics:\n")
                f.write(f"Total Locations: {final_memory_stats['total_locations']}\n")
                f.write(f"Total Memories: {final_memory_stats['total_memories']}\n")
                f.write(f"Hit Rate: {final_memory_stats['hit_rate']:.4f}\n")
        
        print(f"ğŸ’¾ Results saved to: {results_file}")
    
    print(f"\nâœ… Testing completed successfully!")
    
    if memory_loaded:
        print(f"ğŸ§  Memory-enhanced inference completed")
        print(f"   Memory usage rate: {main_results['memory_usage_rate']:.1%}")
        print(f"   GPS quantization hit rate: {main_results['gps_quantization_hit_rate']:.1%}")
        if main_results['gps_quantization_hit_rate'] < 0.1:
            print(f"   âš ï¸  Low GPS quantization hit rate suggests GPS normalization issues")
        elif main_results['gps_quantization_hit_rate'] > 0.5:
            print(f"   âœ… Good GPS quantization hit rate indicates proper memory bank utilization")
    else:
        print(f"ğŸ”„ Standard inference completed without memory bank")


if __name__ == "__main__":
    args = parse_args()
    main(args)